{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG"
   ]
  },
  {
   "attachments": {
    "RAG_multimodal.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG_multimodal.png](attachment:RAG_multimodal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from chromadb import Client\n",
    "from langchain.schema import SystemMessage, HumanMessage  # Add this import\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CLIP model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client and create a collection\n",
    "client = Client()\n",
    "text_collection = client.get_or_create_collection(name=\"text_embeddings_collection\")\n",
    "image_collection = client.get_or_create_collection(name=\"image_embeddings_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text embeddings using CLIP and ensure they are converted to lists\n",
    "def generate_text_embedding(text):\n",
    "    chunks = [text[i:i + 512] for i in range(0, len(text), 512)]  # Split text into manageable chunks\n",
    "    embeddings = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        inputs = processor(text=[chunk], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model.get_text_features(**inputs).cpu().numpy().flatten()\n",
    "            embeddings.append(embedding.tolist())  # Convert NumPy array to Python list\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store embeddings in Chroma DB\n",
    "def add_text_embeddings_to_chroma(embeddings, pdf_filename, text_content):\n",
    "    documents = [{\"embedding\": embedding, \"metadata\": {\"type\": \"text\", \"content\": text_content[:100], \"source\": pdf_filename}} for embedding in embeddings]\n",
    "    \n",
    "    # Add documents (embeddings + metadata) to Chroma DB\n",
    "    text_collection.add(\n",
    "        embeddings=[doc['embedding'] for doc in documents],\n",
    "        metadatas=[doc['metadata'] for doc in documents],\n",
    "        documents=[text_content for _ in documents],\n",
    "        ids=[f\"{pdf_filename}_{i}\" for i in range(len(documents))]  # Unique IDs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process all PDFs and store embeddings\n",
    "def process_and_store_text_embeddings(pdf_dir):\n",
    "    for pdf_filename in os.listdir(pdf_dir):\n",
    "        if pdf_filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
    "            print(f\"Processing {pdf_filename}...\")\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            text_embeddings = generate_text_embedding(text)  # Generate embeddings\n",
    "            add_text_embeddings_to_chroma(text_embeddings, pdf_filename, text)  # Store embeddings in Chroma\n",
    "            print(f\"Successfully stored text data of {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2305.07622v3.pdf...\n",
      "Successfully stored text data of 2305.07622v3.pdf\n",
      "Processing 2305.19860v4.pdf...\n",
      "Successfully stored text data of 2305.19860v4.pdf\n",
      "Processing 2307.10700.pdf...\n",
      "Successfully stored text data of 2307.10700.pdf\n",
      "Processing 2307.15780v3.pdf...\n",
      "Successfully stored text data of 2307.15780v3.pdf\n",
      "Processing 2312.10997.pdf...\n",
      "Successfully stored text data of 2312.10997.pdf\n",
      "Processing 2402.01680.pdf...\n",
      "Successfully stored text data of 2402.01680.pdf\n",
      "Processing 2402.06196.pdf...\n",
      "Successfully stored text data of 2402.06196.pdf\n",
      "Processing 2404.02929.pdf...\n",
      "Successfully stored text data of 2404.02929.pdf\n",
      "Processing 2408.13296.pdf...\n",
      "Successfully stored text data of 2408.13296.pdf\n",
      "Processing 2409.01007.pdf...\n",
      "Successfully stored text data of 2409.01007.pdf\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PDFs\n",
    "pdf_dir = \"../pdfs\"\n",
    "process_and_store_text_embeddings(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents stored in Chroma DB: 2485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_total_documents():\n",
    "    total_documents = text_collection.count()\n",
    "    print(f\"Total number of documents stored in Chroma DB: {total_documents}\")\n",
    "    return total_documents\n",
    "\n",
    "check_total_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Content: 1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\n",
      "Wangc, and Haofen Wang a,c\n",
      "a...\n",
      "Metadata: {'content': '1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu ', 'source': '2312.10997.pdf', 'type': 'text'}\n",
      "--------------------------------------------------\n",
      "Document Content: A Survey on Large Language Models for Recommendation\n",
      "Likang Wu1,2∗, Zhi Zheng1,2∗, Zhaopeng Qiu2∗, Hao Wang1† , Hongchao Gu1 , Tingjia\n",
      "Shen1 , Chuan Qin2 , Chen Zhu2 , Hengshu Zhu2† , Qi Liu1 , Hui Xi...\n",
      "Metadata: {'content': 'A Survey on Large Language Models for Recommendation\\nLikang Wu1,2∗, Zhi Zheng1,2∗, Zhaopeng Qiu2∗, H', 'source': '2305.19860v4.pdf', 'type': 'text'}\n",
      "--------------------------------------------------\n",
      "Document Content: A Survey on Large Language Models for Recommendation\n",
      "Likang Wu1,2∗, Zhi Zheng1,2∗, Zhaopeng Qiu2∗, Hao Wang1† , Hongchao Gu1 , Tingjia\n",
      "Shen1 , Chuan Qin2 , Chen Zhu2 , Hengshu Zhu2† , Qi Liu1 , Hui Xi...\n",
      "Metadata: {'content': 'A Survey on Large Language Models for Recommendation\\nLikang Wu1,2∗, Zhi Zheng1,2∗, Zhaopeng Qiu2∗, H', 'source': '2305.19860v4.pdf', 'type': 'text'}\n",
      "--------------------------------------------------\n",
      "Document Content: LLM-Rec: Personalized Recommendation via\n",
      "Prompting Large Language Models\n",
      "Hanjia Lyu1, Song Jiang2, Hanqing Zeng3, Yinglong Xia3, Qifan Wang3,\n",
      "Si Zhang3, Ren Chen3, Christopher Leung3, Jiajie Tang3, Ji...\n",
      "Metadata: {'content': 'LLM-Rec: Personalized Recommendation via\\nPrompting Large Language Models\\nHanjia Lyu1, Song Jiang2, H', 'source': '2307.15780v3.pdf', 'type': 'text'}\n",
      "--------------------------------------------------\n",
      "Document Content: A Survey on Large Language Models for Recommendation\n",
      "Likang Wu1,2∗, Zhi Zheng1,2∗, Zhaopeng Qiu2∗, Hao Wang1† , Hongchao Gu1 , Tingjia\n",
      "Shen1 , Chuan Qin2 , Chen Zhu2 , Hengshu Zhu2† , Qi Liu1 , Hui Xi...\n",
      "Metadata: {'content': 'A Survey on Large Language Models for Recommendation\\nLikang Wu1,2∗, Zhi Zheng1,2∗, Zhaopeng Qiu2∗, H', 'source': '2305.19860v4.pdf', 'type': 'text'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to retrieve a random subset of documents from a Chroma DB text collection\n",
    "def retrieve_random_documents_from_chroma(collection, num_docs=5):\n",
    "    # First, retrieve all documents (or limit the number)\n",
    "    result = collection.get(include=[\"documents\", \"metadatas\"], limit=1000)  # Adjust the limit as needed\n",
    "    \n",
    "    # Combine documents and their metadata\n",
    "    documents = result['documents']\n",
    "    metadatas = result['metadatas']\n",
    "    \n",
    "    # If there are fewer documents than requested, return all\n",
    "    if len(documents) <= num_docs:\n",
    "        return list(zip(documents, metadatas))\n",
    "    \n",
    "    # Otherwise, randomly sample the requested number of documents\n",
    "    random_indices = random.sample(range(len(documents)), num_docs)\n",
    "    random_docs = [(documents[i], metadatas[i]) for i in random_indices]\n",
    "    \n",
    "    return random_docs\n",
    "\n",
    "# Example usage\n",
    "random_docs = retrieve_random_documents_from_chroma(text_collection, num_docs=5)\n",
    "\n",
    "# Display the retrieved random documents and their metadata\n",
    "for doc, meta in random_docs:\n",
    "    print(f\"Document Content: {doc[:200]}...\")  # Print the first 200 characters of the document\n",
    "    print(f\"Metadata: {meta}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for query text using the same embedding model used for documents\n",
    "def generate_query_embedding(query_text):\n",
    "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.get_text_features(**inputs).cpu().numpy().flatten().tolist()  # Ensure it's a list\n",
    "    return query_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Chroma DB and retrieve relevant documents\n",
    "def retrieve_relevant_documents(query_text, k=5):\n",
    "    query_embedding = generate_query_embedding(query_text)\n",
    "    \n",
    "    # Query the Chroma DB to get relevant documents\n",
    "    result = collection.query(\n",
    "        query_embeddings=[query_embedding],  # Embed the query\n",
    "        n_results=k,  # Number of results to return\n",
    "        include=[\"documents\", \"metadatas\"]  # Retrieve documents and metadata\n",
    "    )\n",
    "    \n",
    "    # Process the returned results (handling lists)\n",
    "    documents = result[\"documents\"]  # List of documents\n",
    "    metadatas = result[\"metadatas\"]  # List of corresponding metadata\n",
    "    \n",
    "    # Combine documents into a context for GPT-4\n",
    "    combined_documents = \"\\n\\n\".join([f\"Source: {meta['source']}\\nContent: {doc}\" for doc, meta in zip(documents, metadatas)])\n",
    "    \n",
    "    return combined_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a response using GPT-4 with the retrieved documents as context\n",
    "def generate_rag_response(query_text, k=5):\n",
    "    # Generate embeddings for the query and retrieve relevant documents from Chroma DB\n",
    "    query_embedding = generate_query_embedding(query_text)\n",
    "    \n",
    "    # Query Chroma DB\n",
    "    result = text_collection.query(\n",
    "        query_embeddings=[query_embedding], \n",
    "        n_results=k, \n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # Process the returned results properly\n",
    "    documents = result['documents']  # List of document contents\n",
    "    metadatas = result['metadatas']  # List of metadata dictionaries\n",
    "    \n",
    "    # Combine the documents with their metadata (source, content)\n",
    "    combined_documents = \"\"\n",
    "    for i, (doc, meta) in enumerate(zip(documents, metadatas)):\n",
    "        # Directly access the dictionary's keys without using .get() on lists\n",
    "        source = meta['source'] if 'source' in meta else 'Unknown source'\n",
    "        content = meta['content'] if 'content' in meta else 'No content available'\n",
    "        \n",
    "        combined_documents += f\"Document {i + 1}:\\n\"\n",
    "        combined_documents += f\"Source: {source}\\nContent: {content}\\n\\n\"\n",
    "    \n",
    "    # Prepare the prompt for GPT-4 using the retrieved documents as context\n",
    "    prompt = f\"Context related to the query:\\n\\n{combined_documents}\\n\\nNow answer the following question:\\n{query_text}\"\n",
    "    \n",
    "    # Call GPT-4 with the context and the original query\n",
    "    response = llm(messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "    \n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajayk\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that the provided context does not include any information specifically about LLM-Rec. However, I can provide a detailed explanation based on general knowledge.\n",
      "\n",
      "**LLM-Rec**, which stands for **Large Language Model-based Recommendation**, refers to the application of large language models (LLMs) such as GPT-3, GPT-4, or other similar models in the field of recommendation systems. These models leverage vast amounts of data and sophisticated neural network architectures to understand and generate human-like text. Here’s a detailed explanation:\n",
      "\n",
      "### 1. **Introduction to Recommender Systems:**\n",
      "Recommender systems are algorithms designed to suggest items to users based on various factors like user preferences, past behavior, and item characteristics. Common applications include product recommendations on e-commerce sites, movie or music suggestions on streaming services, and social media content recommendations.\n",
      "\n",
      "### 2. **Traditional Approaches:**\n",
      "- **Collaborative Filtering:** This method predicts a user's interests by collecting preferences from many users. It can be user-based (finding similar users) or item-based (finding similar items).\n",
      "- **Content-Based Filtering:** This approach recommends items similar to those a user has liked in the past, based on item features.\n",
      "- **Hybrid Methods:** Combining collaborative and content-based filtering to improve recommendation accuracy.\n",
      "\n",
      "### 3. **Large Language Models (LLMs):**\n",
      "LLMs are trained on diverse and extensive textual data to understand and generate human-like text. They can capture complex patterns and relationships in data, making them powerful tools for various natural language processing (NLP) tasks.\n",
      "\n",
      "### 4. **Application of LLMs in Recommendations (LLM-Rec):**\n",
      "- **Contextual Understanding:** LLMs can understand the context and nuances in user queries or interactions, enabling more accurate and personalized recommendations.\n",
      "- **Natural Language Processing:** These models can process and analyze user reviews, comments, and other text data to extract meaningful insights about user preferences and item characteristics.\n",
      "- **Conversation-Based Recommendations:** LLM-Rec can facilitate conversational recommender systems, where users interact with the system in a natural, dialogue-based manner to receive suggestions.\n",
      "- **Cold-Start Problem:** LLMs can help address the cold-start problem (where new users or items have little to no interaction data) by leveraging contextual information and external data sources.\n",
      "\n",
      "### 5. **Advantages of LLM-Rec:**\n",
      "- **Personalization:** Enhanced ability to tailor recommendations based on a deep understanding of user preferences and contextual information.\n",
      "- **Scalability:** Capability to handle large-scale data and provide recommendations across diverse domains.\n",
      "- **Flexibility:** Adaptability to various recommendation scenarios, including dynamic and real-time recommendation needs.\n",
      "\n",
      "### 6. **Challenges and Considerations:**\n",
      "- **Computational Resources:** LLMs require significant computational power and resources for training and inference.\n",
      "- **Data Privacy:** Ensuring user data privacy and complying with regulations is critical when using extensive data for training LLMs.\n",
      "- **Bias and Fairness:** Addressing inherent biases in LLMs to ensure fair and unbiased recommendations.\n",
      "\n",
      "### 7. **Future Directions:**\n",
      "- **Integration with Other AI Technologies:** Combining LLMs with other AI techniques like reinforcement learning for more adaptive and interactive recommendation systems.\n",
      "- **Improving Efficiency:** Developing more efficient training and inference methods to reduce the computational burden of LLMs.\n",
      "- **Enhanced User Interaction:** Focusing on improving the user experience through more intuitive and interactive recommendation interfaces.\n",
      "\n",
      "In summary, LLM-Rec leverages the capabilities of large language models to enhance the accuracy, personalization, and contextual understanding of recommendation systems, offering significant improvements over traditional methods but also bringing new challenges and considerations.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query_text = \"Explain in detail about LLM-Rec?\"\n",
    "rag_response = generate_rag_response(query_text, k=5)\n",
    "print(rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2305.07622v3.pdf\n",
      "Extracted 2 images from ../pdfs\\2305.07622v3.pdf\n",
      "Stored 2 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2305.07622v3.pdf\n",
      "\n",
      "Processing: 2305.19860v4.pdf\n",
      "Extracted 8 images from ../pdfs\\2305.19860v4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 8 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2305.19860v4.pdf\n",
      "\n",
      "Processing: 2307.10700.pdf\n",
      "Extracted 0 images from ../pdfs\\2307.10700.pdf\n",
      "No images found in 2307.10700.pdf\n",
      "Processing: 2307.15780v3.pdf\n",
      "Extracted 2 images from ../pdfs\\2307.15780v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 2 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2307.15780v3.pdf\n",
      "\n",
      "Processing: 2312.10997.pdf\n",
      "Extracted 6 images from ../pdfs\\2312.10997.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Insert of existing embedding ID: image_2\n",
      "Insert of existing embedding ID: image_3\n",
      "Insert of existing embedding ID: image_4\n",
      "Insert of existing embedding ID: image_5\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_2\n",
      "Add of existing embedding ID: image_3\n",
      "Add of existing embedding ID: image_4\n",
      "Add of existing embedding ID: image_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 6 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2312.10997.pdf\n",
      "\n",
      "Processing: 2402.01680.pdf\n",
      "Extracted 3 images from ../pdfs\\2402.01680.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Insert of existing embedding ID: image_2\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 3 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2402.01680.pdf\n",
      "\n",
      "Processing: 2402.06196.pdf\n",
      "Extracted 50 images from ../pdfs\\2402.06196.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Insert of existing embedding ID: image_2\n",
      "Insert of existing embedding ID: image_3\n",
      "Insert of existing embedding ID: image_4\n",
      "Insert of existing embedding ID: image_5\n",
      "Insert of existing embedding ID: image_6\n",
      "Insert of existing embedding ID: image_7\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_2\n",
      "Add of existing embedding ID: image_3\n",
      "Add of existing embedding ID: image_4\n",
      "Add of existing embedding ID: image_5\n",
      "Add of existing embedding ID: image_6\n",
      "Add of existing embedding ID: image_7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 50 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2402.06196.pdf\n",
      "\n",
      "Processing: 2404.02929.pdf\n",
      "Extracted 9 images from ../pdfs\\2404.02929.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Insert of existing embedding ID: image_2\n",
      "Insert of existing embedding ID: image_3\n",
      "Insert of existing embedding ID: image_4\n",
      "Insert of existing embedding ID: image_5\n",
      "Insert of existing embedding ID: image_6\n",
      "Insert of existing embedding ID: image_7\n",
      "Insert of existing embedding ID: image_8\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_2\n",
      "Add of existing embedding ID: image_3\n",
      "Add of existing embedding ID: image_4\n",
      "Add of existing embedding ID: image_5\n",
      "Add of existing embedding ID: image_6\n",
      "Add of existing embedding ID: image_7\n",
      "Add of existing embedding ID: image_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 9 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2404.02929.pdf\n",
      "\n",
      "Processing: 2408.13296.pdf\n",
      "Converted CMYK to RGB for image on page 17 of ../pdfs\\2408.13296.pdf\n",
      "Converted CMYK to RGB for image on page 48 of ../pdfs\\2408.13296.pdf\n",
      "Converted CMYK to RGB for image on page 60 of ../pdfs\\2408.13296.pdf\n",
      "Converted CMYK to RGB for image on page 68 of ../pdfs\\2408.13296.pdf\n",
      "Converted CMYK to RGB for image on page 70 of ../pdfs\\2408.13296.pdf\n",
      "Extracted 30 images from ../pdfs\\2408.13296.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Insert of existing embedding ID: image_2\n",
      "Insert of existing embedding ID: image_3\n",
      "Insert of existing embedding ID: image_4\n",
      "Insert of existing embedding ID: image_5\n",
      "Insert of existing embedding ID: image_6\n",
      "Insert of existing embedding ID: image_7\n",
      "Insert of existing embedding ID: image_8\n",
      "Insert of existing embedding ID: image_9\n",
      "Insert of existing embedding ID: image_10\n",
      "Insert of existing embedding ID: image_11\n",
      "Insert of existing embedding ID: image_12\n",
      "Insert of existing embedding ID: image_13\n",
      "Insert of existing embedding ID: image_14\n",
      "Insert of existing embedding ID: image_15\n",
      "Insert of existing embedding ID: image_16\n",
      "Insert of existing embedding ID: image_17\n",
      "Insert of existing embedding ID: image_18\n",
      "Insert of existing embedding ID: image_19\n",
      "Insert of existing embedding ID: image_20\n",
      "Insert of existing embedding ID: image_21\n",
      "Insert of existing embedding ID: image_22\n",
      "Insert of existing embedding ID: image_23\n",
      "Insert of existing embedding ID: image_24\n",
      "Insert of existing embedding ID: image_25\n",
      "Insert of existing embedding ID: image_26\n",
      "Insert of existing embedding ID: image_27\n",
      "Insert of existing embedding ID: image_28\n",
      "Insert of existing embedding ID: image_29\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_2\n",
      "Add of existing embedding ID: image_3\n",
      "Add of existing embedding ID: image_4\n",
      "Add of existing embedding ID: image_5\n",
      "Add of existing embedding ID: image_6\n",
      "Add of existing embedding ID: image_7\n",
      "Add of existing embedding ID: image_8\n",
      "Add of existing embedding ID: image_9\n",
      "Add of existing embedding ID: image_10\n",
      "Add of existing embedding ID: image_11\n",
      "Add of existing embedding ID: image_12\n",
      "Add of existing embedding ID: image_13\n",
      "Add of existing embedding ID: image_14\n",
      "Add of existing embedding ID: image_15\n",
      "Add of existing embedding ID: image_16\n",
      "Add of existing embedding ID: image_17\n",
      "Add of existing embedding ID: image_18\n",
      "Add of existing embedding ID: image_19\n",
      "Add of existing embedding ID: image_20\n",
      "Add of existing embedding ID: image_21\n",
      "Add of existing embedding ID: image_22\n",
      "Add of existing embedding ID: image_23\n",
      "Add of existing embedding ID: image_24\n",
      "Add of existing embedding ID: image_25\n",
      "Add of existing embedding ID: image_26\n",
      "Add of existing embedding ID: image_27\n",
      "Add of existing embedding ID: image_28\n",
      "Add of existing embedding ID: image_29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 30 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2408.13296.pdf\n",
      "\n",
      "Processing: 2409.01007.pdf\n",
      "Extracted 9 images from ../pdfs\\2409.01007.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: image_0\n",
      "Insert of existing embedding ID: image_1\n",
      "Insert of existing embedding ID: image_2\n",
      "Insert of existing embedding ID: image_3\n",
      "Insert of existing embedding ID: image_4\n",
      "Insert of existing embedding ID: image_5\n",
      "Insert of existing embedding ID: image_6\n",
      "Insert of existing embedding ID: image_7\n",
      "Insert of existing embedding ID: image_8\n",
      "Add of existing embedding ID: image_0\n",
      "Add of existing embedding ID: image_1\n",
      "Add of existing embedding ID: image_2\n",
      "Add of existing embedding ID: image_3\n",
      "Add of existing embedding ID: image_4\n",
      "Add of existing embedding ID: image_5\n",
      "Add of existing embedding ID: image_6\n",
      "Add of existing embedding ID: image_7\n",
      "Add of existing embedding ID: image_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 9 image embeddings in Chroma DB.\n",
      "Processed and stored image embeddings for 2409.01007.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract images from a PDF\n",
    "def extract_images_from_pdf(pdf_path):\n",
    "    \"\"\"Extract images from a PDF.\"\"\"\n",
    "    images = []\n",
    "    \n",
    "    # Open the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        # Extract images from the page\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            img_ext = base_image[\"ext\"]\n",
    "            \n",
    "            # Convert image bytes to PIL Image\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "            \n",
    "            # Check if the image is CMYK and convert to RGB\n",
    "            if image.mode == \"CMYK\":\n",
    "                image = image.convert(\"RGB\")\n",
    "                print(f\"Converted CMYK to RGB for image on page {page_num + 1} of {pdf_path}\")\n",
    "            \n",
    "            images.append(image)\n",
    "    \n",
    "    print(f\"Extracted {len(images)} images from {pdf_path}\")\n",
    "    return images\n",
    "\n",
    "# Function to generate image embeddings using CLIP\n",
    "def generate_image_embedding(image):\n",
    "    # Resize image to the expected size for CLIP (e.g., 224x224 pixels)\n",
    "    image = image.resize((224, 224))  # CLIP expects this size for vision inputs\n",
    "    \n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.get_image_features(**inputs).cpu().numpy().flatten()\n",
    "    \n",
    "    return image_embedding.tolist()  # Convert NumPy array to list\n",
    "\n",
    "# Function to store image embeddings in Chroma DB\n",
    "def store_image_embeddings_in_chroma(embeddings, image_filenames, image_collection):\n",
    "    # Ensure embeddings are lists of lists for Chroma DB\n",
    "    if len(embeddings) == 0 or len(image_filenames) == 0:\n",
    "        print(\"No embeddings or filenames to store.\")\n",
    "        return\n",
    "    \n",
    "    documents = [{\"embedding\": embedding, \"metadata\": {\"type\": \"image\", \"filename\": filename}} \n",
    "                 for embedding, filename in zip(embeddings, image_filenames)]\n",
    "    \n",
    "    # Generate unique IDs for each image\n",
    "    ids = [f\"image_{i}\" for i in range(len(documents))]\n",
    "    \n",
    "    # Add documents (embeddings + metadata) to Chroma DB\n",
    "    image_collection.add(\n",
    "        embeddings=[doc['embedding'] for doc in documents],\n",
    "        metadatas=[doc['metadata'] for doc in documents],\n",
    "        ids=ids  # Ensure IDs are non-empty\n",
    "    )\n",
    "    \n",
    "    print(f\"Stored {len(embeddings)} image embeddings in Chroma DB.\")\n",
    "\n",
    "# Function to process each PDF file and store both text and image embeddings\n",
    "def process_pdfs_and_store_embeddings(pdf_dir, image_collection):\n",
    "    for pdf_filename in os.listdir(pdf_dir):\n",
    "        if pdf_filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
    "            print(f\"Processing: {pdf_filename}\")\n",
    "            \n",
    "            # Extract images from the PDF\n",
    "            images = extract_images_from_pdf(pdf_path)\n",
    "            \n",
    "            if not images:\n",
    "                print(f\"No images found in {pdf_filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Generate image embeddings\n",
    "            image_filenames = [f\"{pdf_filename}_image_{i}.png\" for i in range(len(images))]\n",
    "            image_embeddings = [generate_image_embedding(image) for image in images]\n",
    "            \n",
    "            # Store embeddings in Chroma DB\n",
    "            store_image_embeddings_in_chroma(image_embeddings, image_filenames, image_collection)\n",
    "            \n",
    "            print(f\"Processed and stored image embeddings for {pdf_filename}\\n\")\n",
    "\n",
    "# Example: Call the function to process all PDFs in the folder\n",
    "process_pdfs_and_store_embeddings(\"../pdfs\", image_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform User Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for the query using CLIP\n",
    "def generate_query_embedding(query_text):\n",
    "    inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.get_text_features(**inputs).cpu().numpy().flatten().tolist()  # Convert to list\n",
    "    return query_embedding\n",
    "\n",
    "query_text = \"AI-related images\"\n",
    "\n",
    "query_embedding = generate_query_embedding(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"../images\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant documents from ChromaDB\n",
    "def retrieve_from_chroma(query_text, k=5):\n",
    "    query_embedding = generate_query_embedding(query_text)\n",
    "\n",
    "    # Query text collection\n",
    "    text_results = text_collection.query(\n",
    "        query_embeddings=[query_embedding], \n",
    "        n_results=k, \n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Query image collection\n",
    "    image_results = image_collection.query(\n",
    "        query_embeddings=[query_embedding], \n",
    "        n_results=k, \n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    return text_results, image_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Text Results:\n",
      "Document 1:\n",
      "Source: 2402.06196.pdf\n",
      "Content: ['Large Language Models: A Survey\\nShervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\\nRichard Socher, Xavier Amatriain, Jianfeng Gao\\nAbstract—Large Language Models (LLMs) have drawn a\\nlot of attention due to their strong performance on a wide\\nrange of natural language tasks, since the release of ChatGPT\\nin November 2022. LLMs’ ability of general-purpose language\\nunderstanding and generation is acquired by training billions of\\nmodel’s parameters on massive amounts of text data, as predicted\\nby scaling laws [1], [2]. The research area of LLMs, while very\\nrecent, is evolving rapidly in many different ways. In this paper,\\nwe review some of the most prominent LLMs, including three\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\\ncharacteristics, contributions and limitations. We also give an\\noverview of techniques developed to build, and augment LLMs.\\nWe then survey popular datasets prepared for LLM training,\\nfine-tuning, and evaluation, review widely used LLM evaluation\\nmetrics, and compare the performance of several popular LLMs\\non a set of representative benchmarks. Finally, we conclude\\nthe paper by discussing open challenges and future research\\ndirections.\\nI.\\nINTRODUCTION\\nLanguage modeling is a long-standing research topic, dat-\\ning back to the 1950s with Shannon’s application of informa-\\ntion theory to human language, where he measured how well\\nsimple n-gram language models predict or compress natural\\nlanguage text [3]. Since then, statistical language modeling\\nbecame fundamental to many natural language understanding\\nand generation tasks, ranging from speech recognition, ma-\\nchine translation, to information retrieval [4], [5], [6].\\nThe recent advances on transformer-based large language\\nmodels (LLMs), pretrained on Web-scale text corpora, signif-\\nicantly extended the capabilities of language models (LLMs).\\nFor example, OpenAI’s ChatGPT and GPT-4 can be used not\\nonly for natural language processing, but also as general task\\nsolvers to power Microsoft’s Co-Pilot systems, for instance,\\ncan follow human instructions of complex new tasks per-\\nforming multi-step reasoning when needed. LLMs are thus\\nbecoming the basic building block for the development of\\ngeneral-purpose AI agents or artificial general intelligence\\n(AGI).\\nAs the field of LLMs is moving fast, with new findings,\\nmodels and techniques being published in a matter of months\\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\\ntioners often find it challenging to figure out the best recipes\\nto build LLM-powered AI systems for their tasks. This paper\\ngives a timely survey of the recent advances on LLMs. We\\nhope this survey will prove a valuable and accessible resource\\nfor students, researchers and developers.\\nLLMs are large-scale, pre-trained, statistical language mod-\\nels based on neural networks. The recent success of LLMs is\\nan accumulation of decades of research and development of\\nlanguage models, which can be categorized into four waves\\nthat have different starting points and velocity: statistical lan-\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n −1 words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing, where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan-\\nguage models with recurrent neural networks [23] or trans-\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge\\nlanguage\\nmodels\\n(LLMs)\\nmainly\\nrefer\\nto\\ntransformer-based neural language models\\n1 that contain\\ntens to hundreds of billions of parameters, which are pre-\\ntrained on massive text data, such as PaLM [31], LLaMA\\n[32], and GPT-4 [33], as summarized in Table III. Compared\\n1Recently, several very promising non-transformer LLMs have been pro-\\nposed, such as the LLMs based on structured state space models [29], [30].\\nSee Section VII for more details.\\narXiv:2402.06196v2  [cs.CL]  20 Feb 2024\\nEmerging\\nBasic\\nAugmented\\nLLM Capabilities\\nReasoning\\nCoding\\nComprehension\\nMultilingual\\nTool\\nutilization\\nWorld\\nknowledge\\nInstruction\\nfollowing\\nIn-context\\nlearning\\nInteracting\\nwith users\\nSelf-improvement\\nMulti choice QA\\nWikipedia QA\\nXNLI\\nCrosslingual QA\\nCrosslingual Tasks\\nTranslation\\nReading Comprehension\\nMulti choice QA\\nBoolean QA\\nSimplification\\nSummarization\\nFunction Calling\\nAPI calling\\nLogical\\nSymbolic\\nCommon Sense\\nArithmetic\\nTurn based\\nCompletion\\nTask definition\\nFew-shot\\nSymbolic\\nreference\\nPos/Neg example\\nStep by step\\nsolving\\nTool planning\\nTask\\ndecomposition\\nVirtual acting\\nPhysical acting\\nKnowledge base\\nutilization\\nAssignment\\nplanning\\nSelf-cirtisim\\nSelf-refinement\\nFig. 1: LLM Capabilities.\\nto PLMs, LLMs are not only much larger in model size, but\\nalso exhibit stronger language understanding and generation\\nabilities, and more importantly, emergent abilities that are\\nnot present in smaller-scale language models. As illustrated\\nin Fig. 1, these emergent abilities include (1) in-context\\nlearning, where LLMs learn a new task from a small set\\nof examples presented in the prompt at inference time, (2)\\ninstruction following, where LLMs, after instruction tuning,\\ncan follow the instructions for new types of tasks without\\nusing explicit examples, and (3) multi-step reasoning, where\\nLLMs can solve a complex task by breaking down that task\\ninto intermediate reasoning steps as demonstrated in the\\nchain-of-thought prompt [34]. LLMs can also be augmented\\nby using external knowledge and tools [35], [36] so that\\nthey can effectively interact with users and environment [37],\\nand continually improve itself using feedback data collected\\nthrough interactions (e.g. via reinforcement learning with\\nhuman feedback (RLHF)).\\nThrough advanced usage and augmentation techniques,\\nLLMs can be deployed as so-called AI agents: artificial entities\\nthat sense their environment, make decisions, and take actions.\\nPrevious research has focused on developing agents for specific\\ntasks and domains. The emergent abilities demonstrated by\\nLLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with\\ndynamic environment. Therefore, LLM-based agents often\\nneed to augment LLMs to e.g., obtain updated information\\nfrom external knowledge bases, verify whether a system action\\nproduces the expected result, and cope with when things do\\nnot go as expected, etc. We will discuss in detail LLM-based\\nagents in Section IV.\\nIn the rest of this paper, Section II presents an overview of\\nstate of the art of LLMs, focusing on three LLM families (GPT,\\nLLaMA and PaLM) and other representative models. Section\\nIII discusses how LLMs are built. Section IV discusses how\\nLLMs are used, and augmented for real-world applications\\nSections V and VI review popular datasets and benchmarks for\\nevaluating LLMs, and summarize the reported LLM evaluation\\nresults. Finally, Section VII concludes the paper by summa-\\nrizing the challenges and future research directions.\\nII.\\nLARGE LANGUAGE MODELS\\nIn this section we start with a review of early pre-trained\\nneural language models as they are the base of LLMs, and\\nthen focus our discussion on three families of LLMs: GPT,\\nLlaMA, and PaLM. Table I provides an overview of some of\\nthese models and their characteristics.\\nA. Early Pre-trained Neural Language Models\\nLanguage modeling using neural networks was pioneered\\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\\nneural language models (NLMs) that are comparable to n-gram\\nmodels. Then, [14] successfully applied NLMs to machine\\ntranslation. The release of RNNLM (an open source NLM\\ntoolkit) by Mikolov [41], [42] helped significantly popularize\\nNLMs. Afterwards, NLMs based on recurrent neural networks\\n(RNNs) and their variants, such as long short-term memory\\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\\nused for many natural language applications including machine\\ntranslation, text generation and text classification [43].\\nThen, the invention of the Transformer architecture [44]\\nmarks another milestone in the development of NLMs. By\\napplying self-attention to compute in parallel for every word\\nin a sentence or document an “attention score” to model the\\ninfluence each word has on another, Transformers allow for\\nmuch more parallelization than RNNs, which makes it possible\\nto efficiently pre-train very big language models on large\\namounts of data on GPUs. These pre-trained language models\\n(PLMs) can be fine-tuned for many downstream tasks.\\nPaper Strcuture\\nEarly Pre-trained\\nLanguage Models\\nII\\nLarge Language Models\\nA\\nIII\\nHOW LLMS ARE BUILT\\nA\\nData Cleaning\\nB\\nLarge Language\\nModel Families\\nB\\nOther Representative\\nLLMs\\nC\\nDominant LLM\\nArchitectures\\nTokenizations\\nC\\nPositional Encoding\\nD\\nModel Pre-training\\nE\\nFine-tuning and\\nInstruction Tuning\\nF\\nAlignment\\nG\\nDecoding Strategies\\nH\\nI\\nHOW LLMS ARE USED AND AUGMENTED\\nA\\nB\\nLLM limitations\\nCost-Effective Training/Inference,\\nAdaptation & Compression\\nI\\nUsing LLMs: Prompt Design\\nand Engineering\\nC\\nAugmenting LLMs through\\nexternal knowledge - RAG\\nD\\nUsing External Tools\\nE\\nLLM Agents\\nV\\n\\xa0POPULAR DATASETS FOR LLMS\\nA\\nDatasets for Basic Tasks: language\\nmodeling/understanding/generation\\nB\\n\\xa0Datasets for Emergent: ICL, reasoning,\\ninstruction following\\nC\\nDatasets for Augmented: using\\nexternal knowledge/tools\\nVI\\n\\xa0PROMINENT LLMS’ PERFORMANCE\\nON BENCHMARKS\\nA\\nB\\nVII\\nCHALLENGES AND FUTURE DIRECTIONS\\nA\\nSmaller and more efficient\\nLanguage Models\\nLLMs’ Performance on Different Tasks\\nPopular Metrics for Evaluating LLMs\\nB\\nNew Post-attention\\nArchitectural Paradigms\\nC\\nMulti-modal Models\\nD\\nImproved LLM Usage and\\nAugmentation techniques\\nD\\nSecurity and\\nEthical/Responsible AI\\nFig. 2: The paper structure.\\nWe group early popular Transformer-based PLMs, based on\\ntheir neural architectures, into three main categories: encoder-\\nonly, decoder-only, and encoder-decoder models. Comprehen-\\nsive surveys of early PLMs are provided in [43], [28].\\n1) Encoder-only PLMs: As the name suggests, the encoder-\\nonly models only consist of an encoder network. These models\\nare originally developed for language understanding tasks,\\nsuch as text classification, where the models need to predict a\\nclass label for an input text. Representative encoder-only mod-\\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\\nDeBERTa, XLM, XLNet, UNILM, as to be described below.\\nBERT (Birectional Encoder Representations from Trans-\\nformers) [24] is one of the most widely used encoder-only\\nlanguage models. BERT consists of three modules: (1) an\\nembedding module that converts input text into a sequence\\nof embedding vectors, (2) a stack of Transformer encoders\\nthat converts embedding vectors into contextual representation\\nvectors, and (3) a fully connected layer that converts the\\nrepresentation vectors (at the final layer) to one-hot vectors.\\nBERT is pre-trained uses two objectives: masked language\\nmodeling (MLM) and next sentence prediction. The pre-trained\\nBERT model can be fine-tuned by adding a classifier layer\\nfor many language understanding tasks, ranging from text\\nTABLE I: High-level Overview of Popular Language Models\\nType\\nModel Name\\n#Parameters\\nRelease\\nBase Models\\nOpen\\nSource\\n#Tokens\\nTraining dataset\\nBERT\\n110M, 340M\\n2018\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nRoBERTa\\n355M\\n2019\\n-\\n✓\\n2.2T\\nBooksCorpus,\\nEnglish\\nWikipedia,\\nCC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-Only\\nALBERT\\n12M,\\n18M,\\n60M,\\n235M\\n2019\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nDeBERTa\\n-\\n2020\\n-\\n✓\\n-\\nBooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet\\n110M, 340M\\n2019\\n-\\n✓\\n32.89B\\nBooksCorpus, English Wikipedia, Giga5, Com-\\nmon Crawl, ClueWeb 2012-B\\nDecoder-only\\nGPT-1\\n120M\\n2018\\n-\\n✓\\n1.3B\\nBooksCorpus\\nGPT-2\\n1.5B\\n2019\\n-\\n✓\\n10B\\nReddit outbound\\nT5 (Base)\\n223M\\n2019\\n-\\n✓\\n156B\\nCommon Crawl\\nEncoder-Decoder\\nMT5 (Base)\\n300M\\n2020\\n-\\n✓\\n-\\nNew Common Crawl-based dataset in 101 lan-\\nguages (m Common Crawl)\\nBART (Base)\\n139M\\n2019\\n-\\n✓\\n-\\nCorrupting text\\nGPT-3\\n125M,\\n350M,\\n760M, 1.3B, 2.7B,\\n6.7B, 13B, 175B\\n2020\\n×\\n300B\\nCommon Crawl (filtered), WebText2, Books1,\\nBooks2, Wikipedia\\nGPT Family\\nCODEX\\n12B\\n2021\\nGPT\\n✓\\n-\\nPublic GitHub software repositories\\nWebGPT\\n760M, 13B, 175B\\n2021\\nGPT-3\\n×\\n-\\nELI5\\nGPT-4\\n1.76T\\n2023\\n-\\n×\\n13T\\n-\\nLLaMA1\\n7B, 13B, 33B, 65B\\n2023\\n-\\n✓\\n1T, 1.4T\\nOnline sources\\nLLaMA2\\n7B, 13B, 34B, 70B\\n2023\\n-\\n✓\\n2T\\nOnline sources\\nAlpaca\\n7B\\n2023\\nLLaMA1\\n✓\\n-\\nGPT-3.5\\nVicuna-13B\\n13B\\n2023\\nLLaMA1\\n✓\\n-\\nGPT-3.5\\nLLaMA Family\\nKoala\\n13B\\n2023\\nLLaMA\\n✓\\n-\\nDialogue data\\nMistral-7B\\n7.3B\\n2023\\n✓\\n-\\n-\\nCode Llama\\n34\\n2023\\nLLaMA2\\n✓\\n500B\\nPublicly available code\\nLongLLaMA\\n3B, 7B\\n2023\\nOpenLLaMA\\n✓\\n1T\\n-\\nLLaMA-Pro-8B\\n8.3B\\n2024\\nLLaMA2-7B\\n✓\\n80B\\nCode and math corpora\\nTinyLlama-1.1B\\n1.1B\\n2024\\nLLaMA1.1B\\n✓\\n3T\\nSlimPajama, Starcoderdata\\nPaLM\\n8B, 62B, 540B\\n2022\\n-\\n×\\n780B\\nWeb documents, books, Wikipedia, conversations,\\nGitHub code\\nU-PaLM\\n8B, 62B, 540B\\n2022\\n-\\n×\\n1.3B\\nWeb documents, books, Wikipedia, conversations,\\nGitHub code\\nPaLM Family\\nPaLM-2\\n340B\\n2023\\n-\\n✓\\n3.6T\\nWeb documents, books, code, mathematics, con-\\nversational data\\nMed-PaLM\\n540B\\n2022\\nPaLM\\n×\\n780B\\nHealthSearchQA, MedicationQA, LiveQA\\nMed-PaLM 2\\n-\\n2023\\nPaLM 2\\n×\\n-\\nMedQA, MedMCQA, HealthSearchQA, LiveQA,\\nMedicationQA\\nFLAN\\n137B\\n2021\\nLaMDA-PT\\n✓\\n-\\nWeb documents, code, dialog data, Wikipedia\\nGopher\\n280B\\n2021\\n-\\n×\\n300B\\nMassiveText\\nERNIE 4.0\\n10B\\n2023\\n-\\n×\\n4TB\\nChinese text\\nRetro\\n7.5B\\n2021\\n-\\n×\\n600B\\nMassiveText\\nLaMDA\\n137B\\n2022\\n-\\n×\\n168B\\npublic dialog data and web documents\\nChinChilla\\n70B\\n2022\\n-\\n×\\n1.4T\\nMassiveText\\nGalactia-120B\\n120B\\n2022\\n-\\n450B\\nOther Popular LLMs\\nCodeGen\\n16.1B\\n2022\\n-\\n✓\\n-\\nTHE PILE, BIGQUERY, BIGPYTHON\\nBLOOM\\n176B\\n2022\\n-\\n✓\\n366B\\nROOTS\\nZephyr\\n7.24B\\n2023\\nMistral-7B\\n✓\\n800B\\nSynthetic data\\nGrok-0\\n33B\\n2023\\n-\\n×\\n-\\nOnline source\\nORCA-2\\n13B\\n2023\\nLLaMA2\\n-\\n2001B\\n-\\nStartCoder\\n15.5B\\n2023\\n-\\n✓\\n35B\\nGitHub\\nMPT\\n7B\\n2023\\n-\\n✓\\n1T\\nRedPajama, m Common Crawl, S2ORC, Common\\nCrawl\\nMixtral-8x7B\\n46.7B\\n2023\\n-\\n✓\\n-\\nInstruction dataset\\nFalcon 180B\\n180B\\n2023\\n-\\n✓\\n3.5T\\nRefinedWeb\\nGemini\\n1.8B, 3.25B\\n2023\\n✓\\n-\\nWeb documents, books, and code, image data,\\naudio data, video data\\nDeepSeek-Coder\\n1.3B, 6.7B, 33B\\n2024\\n-\\n✓\\n2T\\nGitHub’s Markdown and StackExchange\\nDocLLM\\n1B,7B\\n2024\\n-\\n×\\n2T\\nIIT-CDIP Test Collection 1.0, DocBank\\nclassification, question answering to language inference. A\\nhigh-level overview of BERT framework is shown in Fig 3. As\\nBERT significantly improved state of the art on a wide range\\nof language understanding tasks when it was published, the AI\\ncommunity was inspired to develop many similar encoder-only\\nlanguage models based on BERT.\\nRoBERTa [25] significantly improves the robustness of\\nBERT using a set of model design choices and training strate-\\ngies, such as modifying a few key hyperparameters, removing\\nthe next-sentence pre-training objective and training with much\\nlarger mini-batches and learning rates. ALBERT [45] uses two\\nparameter-reduction techniques to lower memory consumption\\nand increase the training speed of BERT: (1) splitting the\\nembedding matrix into two smaller matrices, and (2) using\\nrepeating layers split among groups. DeBERTa (Decoding-\\nenhanced BERT with disentangled attention) [26] improves the\\nBERT and RoBERTa models using two novel techniques. The\\nfirst is the disentangled attention mechanism, where each word\\nis represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words\\nFig. 3: Overall pre-training and fine-tuning procedures for\\nBERT. Courtesy of [24]\\nare computed using disentangled matrices on their contents and\\nrelative positions, respectively. Second, an enhanced mask de-\\ncoder is used to incorporate absolute positions in the decoding\\nlayer to predict the masked tokens in model pre-training. In\\naddition, a novel virtual adversarial training method is used for\\nfine-tuning to improve models’ generalization. ELECTRA [46]\\nuses a new pre-training task, known as replaced token detection\\n(RTD), which is empirically proven to be more sample-efficient\\nthan MLM. Instead of masking the input, RTD corrupts it by\\nreplacing some tokens with plausible alternatives sampled from\\na small generator network. Then, instead of training a model\\nthat predicts the original identities of the corrupted tokens, a\\ndiscriminative model is trained to predict whether a token in\\nthe corrupted input was replaced by a generated sample or not.\\nRTD is more sample-efficient than MLM because the former\\nis defined over all input tokens rather than just the small subset\\nbeing masked out, as illustrated in Fig 4.\\nFig. 4: A comparison between replaced token detection and\\nmasked language modeling. Courtesy of [46].\\nXLMs [47] extended BERT to cross-lingual language\\nmodels using two methods: (1) a unsupervised method that\\nonly relies on monolingual data, and (2) a supervised method\\nthat leverages parallel data with a new cross-lingual language\\nmodel objective, as illustrated in Fig 5. XLMs had obtained\\nstate-of-the-art results on cross-lingual classification, unsuper-\\nvised and supervised machine translation, at the time they were\\nproposed.\\nThere are also encoder-only language models that leverage\\nthe advantages of auto-regressive (decoder) models for model\\ntraining and inference. Two examples are XLNet and UNILM.\\nXLNet [48] is based on Transformer-XL, pre-trained using a\\ngeneralized autoregressive method that enables learning bidi-\\nrectional contexts by maximizing the expected likelihood over\\nFig. 5: Cross-lingual language model pretraining. The MLM\\nobjective is similar to BERT, but with continuous streams\\nof text as opposed to sentence pairs. The TLM objective\\nextends MLM to pairs of parallel sentences. To predict a\\nmasked English word, the model can attend to both the English\\nsentence and its French translation, and is encouraged to align\\nEnglish and French representations. Courtesy of [47].\\nall permutations of the factorization order. UNILM (UNIfied\\npre-trained Language Model) [49] is pre-trained using three\\ntypes of language modeling tasks: unidirectional, bidirectional,\\nand sequence-to-sequence prediction. This is achieved by\\nemploying a shared Transformer network and utilizing specific\\nself-attention masks to control what context the prediction is\\nconditioned on, as illustrated in Fig 6. The pre-trained model\\ncan be fine-tuned for both natural language understanding and\\ngeneration tasks.\\nFig. 6: Overview of unified LM pre-training. The model\\nparameters are shared across the LM objectives (i.e., bidirec-\\ntional LM, unidirectional LM, and sequence-to-sequence LM).\\nCourtesy of [49].\\n2) Decoder-only PLMs: Two of the most widely used\\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\\nOpenAI. These models lay the foundation to more powerful\\nLLMs subsequently, i.e., GPT-3 and GPT-4.\\nGPT-1 [50] demonstrates for the first time that good\\nperformance over a wide range of natural language tasks can be\\nobtained by Generative Pre-Training (GPT) of a decoder-only\\nTransformer model on a diverse corpus of unlabeled text in a\\nself-supervised learning fashion (i.e., next word/token predic-\\ntion), followed by discriminative fine-tuning on each specific\\ndownstream task (with much fewer samples), as illustrated in\\nFig 7. GPT-1 paves the way for subsequent GPT models, with\\neach version improving upon the architecture and achieving\\nbetter performance on various language tasks.\\nFig. 7: High-level overview of GPT pretraining, and fine-tuning\\nsteps. Courtesy of OpenAI.\\nGPT-2 [51] shows that language models are able to learn\\nto perform specific natural language tasks without any explicit\\nsupervision when trained on a large WebText dataset consisting\\nof millions of webpages. The GPT-2 model follows the model\\ndesigns of GPT-1 with a few modifications: Layer normal-\\nization is moved to the input of each sub-block, additional\\nlayer normalization is added after the final self-attention block,\\ninitialization is modified to account for the accumulation on\\nthe residual path and scaling the weights of residual layers,\\nvocabulary size is expanded to 50,25, and context size is\\nincreased from 512 to 1024 tokens.\\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\\nalmost all NLP tasks can be cast as a sequence-to-sequence\\ngeneration task. Thus, an encoder-decoder language model, by\\ndesign, is a unified model in that it can perform all natural\\nlanguage understanding and generation tasks. Representative\\nencoder-decoder PLMs we will review below are T5, mT5,\\nMASS, and BART.\\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\\nwhere transfer learning is effectively exploited for NLP via an\\nintroduction of a unified framework in which all NLP tasks are\\ncast as a text-to-text generation task. mT5 [53] is a multilingual\\nvariant of T5, which is pre-trained on a new Common Crawl-\\nbased dataset consisting of texts in 101 languages.\\nMASS (MAsked Sequence to Sequence pre-training) [54]\\nadopts the encoder-decoder framework to reconstruct a sen-\\ntence fragment given the remaining part of the sentence. The\\nencoder takes a sentence with randomly masked fragment\\n(several consecutive tokens) as input, and the decoder predicts\\nthe masked fragment. In this way, MASS jointly trains the\\nencoder and decoder for language embedding and generation,\\nrespectively.\\nBART [55] uses a standard sequence-to-sequence transla-\\ntion model architecture. It is pre-trained by corrupting text with\\nan arbitrary noising function, and then learning to reconstruct\\nthe original text.\\nB. Large Language Model Families\\nLarge\\nlanguage\\nmodels\\n(LLMs)\\nmainly\\nrefer\\nto\\ntransformer-based\\nPLMs\\nthat\\ncontain\\ntens\\nto\\nhundreds\\nof billions of parameters. Compared to PLMs reviewed above,\\nLLMs are not only much larger in model size, but also exhibit\\nstronger language understanding and generation and emergent\\nabilities that are not present in smaller-scale models. In what\\nfollows, we review three LLM families: GPT, LLaMA, and\\nPaLM, as illustrated in Fig 8.\\n1) The GPT Family: Generative Pre-trained Transform-\\ners (GPT) are a family of decoder-only Transformer-based\\nlanguage models, developed by OpenAI. This family con-\\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\\nCODEX, and WebGPT. Although early GPT models, such as\\nGPT-1 and GPT-2, are open-source, recent models, such as\\nGPT-3 and GPT-4, are close-source and can only be accessed\\nvia APIs. GPT-1 and GPT-2 models have been discussed in\\nthe early PLM subsection. We start with GPT-3 below.\\nGPT-3 [56] is a pre-trained autoregressive language model\\nwith 175 billion parameters. GPT-3 is widely considered as\\nthe first LLM in that it not only is much larger than previous\\nPLMs, but also for the first time demonstrates emergent\\nabilities that are not observed in previous smaller PLMs. GPT-\\n3 shows the emergent ability of in-context learning, which\\nmeans GPT-3 can be applied to any downstream tasks without\\nany gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the\\nmodel. GPT-3 achieved strong performance on many NLP\\ntasks, including translation, question-answering, and the cloze\\ntasks, as well as several ones that require on-the-fly reasoning\\nor domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\\nperformance of GPT-3 as a function of the number of examples\\nin in-context prompts.\\nCODEX [57], released by OpenAI in March 2023, is a\\ngeneral-purpose programming model that can parse natural\\nlanguage and generate code in response. CODEX is a de-\\nscendant of GPT-3, fine-tuned for programming applications\\non code corpora collected from GitHub. CODEX powers\\nMicrosoft’s GitHub Copilot.\\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\\nanswer open-ended questions using a text-based web browser,\\nfacilitating users to search and navigate the web. Specifically,\\nWebGPT is trained in three steps. The first is for WebGPT\\nto learn to mimic human browsing behaviors using human\\ndemonstration data. Then, a reward function is learned to\\npredict human preferences. Finally, WebGPT is refined to\\noptimize the reward function via reinforcement learning and\\nrejection sampling.\\nTo enable LLMs to follow expected human instructions,\\nInstructGPT [59] is proposed to align language models with\\nuser intent on a wide range of tasks by fine-tuning with\\nhuman feedback. Starting with a set of labeler-written prompts\\nand prompts submitted through the OpenAI API, a dataset\\nof labeler demonstrations of the desired model behavior is\\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\\ndataset of human-ranked model outputs is collected to further\\nfine-tune the model using reinforcement learning. The method\\nis known Reinforcement Learning from Human Feedback\\nGPT Family\\nPaLM Family\\n\\xa0 \\xa0LLaMA 1/2 Family\\nGPT\\nGPT1\\nGPT2\\nGPT3\\nGPT4\\nGPT3.5 Turbo\\ntext-davinci\\ncode-davinci\\nCODEX\\nInstructGPT\\nWebGPT\\nGPT4 Vision\\nGPT4 Turbo\\nGorilla\\nMistral\\nVigogne\\nStable Beluga2\\nKoala\\nCode LLaMA\\nVicuna\\nAlpaca\\nBaize\\nLong LLaMA\\nGiraffe\\nGuanaco\\nTulu\\nWizardLM\\nMed-PaLM\\nPaLM-E\\nMed-PaLM2\\nFLAN-PaLM\\nU-PaLM\\nPaLM2\\nPaLM\\nFig. 8: Popular LLM Families.\\nFig. 9: GPT-3 shows that larger models make increasingly\\nefficient use of in-context information. It shows in-context\\nlearning performance on a simple task requiring the model to\\nremove random symbols from a word, both with and without\\na natural language task description. Courtesy of [56].\\n(RLHF), as shown in 10. The resultant InstructGPT models\\nhave shown improvements in truthfulness and reductions in\\ntoxic output generation while having minimal performance\\nregressions on public NLP datasets.\\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\\nThe most important milestone of LLM development is the\\nlaunch of ChatGPT (Chat Generative Pre-trained Transformer)\\n[60] on November 30, 2022. ChatGPT is chatbot that enables\\nusers to steer a conversation to complete a wide range of\\ntasks such as question answering, information seeking, text\\nsummarization, and more. ChatGPT is powered by GPT-3.5\\n(and later by GPT-4), a sibling model to InstructGPT, which\\nis trained to follow an instruction in a prompt and provide a\\ndetailed response.\\nGPT-4 [33] is the latest and most powerful LLM in the\\nGPT family. Launched in March, 2023, GPT-4 is a multi-\\nmodal LLM in that it can take image and text as inputs and\\nproduce text outputs. While still less capable than humans\\nin some of the most challenging real-world scenarios, GPT-4\\nexhibits human-level performance on various professional and\\nacademic benchmarks, including passing a simulated bar exam\\nwith a score around the top 10% of test takers, as shown in\\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\\npredict next tokens on large text corpora, and then fine-tuned\\nwith RLHF to align model behaviors with human-desired ones.\\n2) The LLaMA Family: LLaMA is a collection of founda-\\ntion language models, released by Meta. Unlike GPT models,\\nLLaMA models are open-source, i.e., model weights are\\nreleased to the research community under a noncommercial\\nlicense. Thus, the LLaMA family grows rapidly as these\\nmodels are widely used by many research groups to develop\\nbetter open-source LLMs to compete the closed-source ones or\\nto develop task-specific LLMs for mission-critical applications.\\nThe first set of LLaMA models [32] was released in Febru-\\nary 2023, ranging from 7B to 65B parameters. These models\\nare pre-trained on trillions of tokens, collected from publicly\\navailable datasets. LLaMA uses the transformer architecture of\\nGPT-3, with a few minor architectural modifications, including\\n(1) using a SwiGLU activation function instead of ReLU,\\n(2) using rotary positional embeddings instead of absolute\\npositional embedding, and (3) using root-mean-squared layer-\\nnormalization instead of standard layer-normalization. The\\nopen-source LLaMA-13B model outperforms the proprietary\\nGPT-3 (175B) model on most benchmarks, making it a good\\nbaseline for LLM research.\\nFig. 11: GPT-4 performance on academic and professional\\nexams, compared with GPT 3.5. Courtesy of [33].\\nIn July 2023, Meta, in partnership with Microsoft, released\\nthe LLaMA-2 collection [61], which include both foundation\\nlanguage models and Chat models finetuned for dialog, known\\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\\nto outperform other open-source models on many public\\nbenchmarks. Fig 12 shows the training process of LLaMA-2\\nChat. The process begins with pre-training LLaMA-2 using\\npublicly available online data. Then, an initial version of\\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\\nquently, the model is iteratively refined using RLHF, rejection\\nsampling and proximal policy optimization. In the RLHF stage,\\nthe accumulation of human feedback for revising the reward\\nmodel is crucial to prevent the reward model from being\\nchanged too much, which could hurt the stability of LLaMA\\nmodel training.\\nFig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\\n52K instruction-following demonstrations generated in the\\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\\nis very cost-effective for training, especially for academic\\nresearch. On the self-instruct evaluation set, Alpaca performs\\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\\nThe Vicuna team has developed a 13B chat model, Vicuna-\\n13B, by fine-tuning LLaMA on user-shared conversations\\ncollected from ShareGPT. Preliminary evaluation using GPT-\\n4 as a evaluator shows that Vicuna-13B achieves more than\\n90% quality of OpenAI’s ChatGPT, and Google’s Bard while\\noutperforming other models like LLaMA and Stanford Alpaca\\nin more than 90% of cases. 13 shows the relative response\\nquality of Vicuna and a few other well-known models by\\nGPT-4. Another advantage of Vicuna-13B is its relative limited\\ncomputational demand for model training. The training cost of\\nVicuna-13B is merely $300.\\nFig. 13: Relative Response Quality of Vicuna and a few other\\nwell-known models by GPT-4. Courtesy of Vicuna Team.\\nLike Alpaca and Vicuna, the Guanaco models [63] are also\\nfinetuned LLaMA models using instruction-following data. But\\nthe finetuning is done very efficiently using QLoRA such\\nthat finetuning a 65B parameter model can be done on a\\nsingle 48GB GPU. QLoRA back-propagates gradients through\\na frozen, 4-bit quantized pre-trained language model into Low\\nRank Adapters (LoRA). The best Guanaco model outperforms\\nall previously released models on the Vicuna benchmark,\\nreaching 99.3% of the performance level of ChatGPT while\\nonly requiring 24 hours of fine-tuning on a single GPU.\\nKoala [64] is yet another instruction-following language\\nmodel built on LLaMA, but with a specific focus on interaction\\ndata that include user inputs and responses generated by highly\\ncapable closed-source chat models such as ChatGPT. The\\nKoala-13B model performs competitively with state-of-the-art\\nchat models according to human evaluation based on real-\\nworld user prompts.\\nMistral-7B [65] is a 7B-parameter language model engi-\\nneered for superior performance and efficiency. Mistral-7B\\noutperforms the best open-source 13B model (LLaMA-2-13B)\\nacross all evaluated benchmarks, and the best open-source\\n34B model (LLaMA-34B) in reasoning, mathematics, and code\\ngeneration. This model leverages grouped-query attention for\\nfaster inference, coupled with sliding window attention to\\neffectively handle sequences of arbitrary length with a reduced\\ninference cost.\\nThe LLaMA family is growing rapidly, as more instruction-\\nfollowing models have been built on LLaMA or LLaMA-\\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],\\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\\nBeluga2 [72], just to name a few.\\n3) The PaLM Family: The PaLM (Pathways Language\\nModel) family are developed by Google. The first PaLM\\nmodel [31] was announced in April 2022 and remained private\\nuntil March 2023. It is a 540B parameter transformer-based\\nLLM. The model is pre-trained on a high-quality text corpus\\nconsisting of 780 billion tokens that comprise a wide range\\nof natural language tasks and use cases. PaLM is pre-trained\\non 6144 TPU v4 chips using the Pathways system, which\\nenables highly efficient training across multiple TPU Pods.\\nPaLM demonstrates continued benefits of scaling by achiev-\\ning state-of-the-art few-shot learning results on hundreds of\\nlanguage understanding and generation benchmarks. PaLM-\\n540B outperforms not only state-of-the-art fine-tuned models\\non a suite of multi-step reasoning tasks, but also on par with\\nhumans on the recently released BIG-bench benchmark.\\nThe U-PaLM models of 8B, 62B, and 540B scales are\\ncontinually trained on PaLM with UL2R, a method of continue\\ntraining LLMs on a few steps with UL2’s mixture-of-denoiser\\nobjective [73]. An approximately 2x computational savings\\nrate is reported.\\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\\nCompared to other instruction finetuning work mentioned\\nabove, Flan-PaLM’s finetuning is performed using a much\\nlarger number of tasks, larger model sizes, and chain-of-\\nthought data. As a result, Flan-PaLM substantially outperforms\\nprevious instruction-following models. For instance, Flan-\\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\\noutperforms PaLM-540B by a large margin (+9.4% on av-\\nerage). The finetuning data comprises 473 datasets, 146 task\\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\\ntask categories. Courtesy of [74].\\nPaLM-2 [75] is a more compute-efficient LLM with bet-\\nter multilingual and reasoning capabilities, compared to its\\npredecessor PaLM. PaLM-2 is trained using a mixture of\\nobjectives. Through extensive evaluations on English, multi-\\nlingual, and reasoning tasks, PaLM-2 significantly improves\\nthe model performance on downstream tasks across different\\nmodel sizes, while simultaneously exhibiting faster and more\\nefficient inference than PaLM.\\nMed-PaLM [76] is a domain-specific PaLM, and is de-\\nsigned to provide high-quality answers to medical questions.\\nMed-PaLM is finetuned on PaLM using instruction prompt\\ntuning, a parameter-efficient method for aligning LLMs to\\nnew domains using a few exemplars. Med-PaLM obtains very\\nencouraging results on many healthcare tasks, although it is\\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\\nPaLM via med-domain finetuning and ensemble prompting\\n[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\\ndataset (i.e., a benchmark combining six existing open ques-\\ntion answering datasets spanning professional medical exams,\\nresearch, and consumer queries), improving upon Med-PaLM\\nby over 19% and setting a new state-of-the-art.\\nC. Other Representative LLMs\\nIn addition to the models discussed in the previous sub-\\nsections, there are other popular LLMs which do not belong\\nto those three model families, yet they have achieved great\\nperformance and have pushed the LLMs field forward. We\\nbriefly describe these LLMs in this subsection.\\nFLAN: In [78], Wei et al. explored a simple method for\\nimproving the zero-shot learning abilities of language models.\\nThey showed that instruction tuning language models on a\\ncollection of datasets described via instructions substantially\\nimproves zero-shot performance on unseen tasks. They take\\na 137B parameter pretrained language model and instruction\\ntune it on over 60 NLP datasets verbalized via natural language\\ninstruction templates. They call this instruction-tuned model\\nFLAN. Fig 15 provides a comparison of instruction tuning\\nwith pretrain–finetune and prompting.\\nFig.\\n15:\\ncomparison\\nof\\ninstruction\\ntuning\\nwith\\npre-\\ntrain–finetune and prompting. Courtesy of [78].\\nGopher: In [79], Rae et al. presented an analysis of\\nTransformer-based language model performance across a wide\\nrange of model scales — from models with tens of millions of\\nparameters up to a 280 billion parameter model called Gopher.\\nThese models were evaluated on 152 diverse tasks, achieving\\nstate-of-the-art performance across the majority. The number\\nof layers, the key/value size, and other hyper-parameters of\\ndifferent model sizes are shown in Fig 16.\\nFig. 16: Model architecture details of Gopher with different\\nnumber of parameters. Courtesy of [78].\\nT0: In [80], Sanh et al. developed T0, a system for easily\\nmapping any natural language tasks into a human-readable\\nprompted form. They converted a large set of supervised\\ndatasets, each with multiple prompts with diverse wording.\\nThese prompted datasets allow for benchmarking the ability\\nof a model to perform completely held-out tasks. Then, a\\nT0 encoder-decoder model is developed to consume textual\\ninputs and produces target responses. The model is trained on\\na multitask mixture of NLP datasets partitioned into different\\ntasks.\\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\\nwork named ERNIE 3.0 for pre-training large-scale knowledge\\nenhanced models. It fuses auto-regressive network and auto-\\nencoding network, so that the trained model can be easily tai-\\nlored for both natural language understanding and generation\\ntasks using zero-shot learning, few-shot learning or fine-tuning.\\nThey have trained ERNIE 3.0 with 10 billion parameters\\non a 4TB corpus consisting of plain texts and a large-scale\\nknowledge graph. Fig 17 illustrates the model architecture of\\nErnie 3.0.\\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\\nof [81].\\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\\nlanguage models by conditioning on document chunks re-\\ntrieved from a large corpus, based on local similarity with pre-\\nceding tokens. Using a 2-trillion-token database, the Retrieval-\\nEnhanced Transformer (Retro) obtains comparable perfor-\\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\\n25% fewer parameters. As shown in Fig 18, Retro combines\\na frozen Bert retriever, a differentiable encoder and a chunked\\ncross-attention mechanism to predict tokens based on an order\\nof magnitude more data than what is typically consumed\\nduring training.\\nGLaM: In [84], Du et al. proposed a family of LLMs\\nnamed GLaM (Generalist Language Model), which use a\\nsparsely activated mixture-of-experts architecture to scale the\\nmodel capacity while also incurring substantially less training\\ncost compared to dense variants. The largest GLaM has 1.2\\ntrillion parameters, which is approximately 7x larger than GPT-\\n3. It consumes only 1/3 of the energy used to train GPT-3 and\\nrequires half of the computation flops for inference, while still\\nachieving better overall zero, one and few-shot performance\\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\\nof GLAM.\\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\\nfamily of Transformer-based neural language models special-\\nized for dialog, which have up to 137B parameters and are\\npre-trained on 1.56T words of public dialog data and web text.\\nFig. 18: Retro architecture. Left: simplified version where a\\nsequence of length n = 12 is split into l = 3 chunks of size\\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\\n5 tokens each. The retrieval pathway is shown on top. Right:\\nDetails of the interactions in the CCA operator. Causality is\\nmaintained as neighbours of the first chunk only affect the last\\ntoken of the first chunk and tokens from the second chunk.\\nCourtesy of [82].\\nFig. 19: GLaM model architecture. Each MoE layer (the\\nbottom block) is interleaved with a Transformer layer (the\\nupper block). Courtesy of [84].\\nThey showed that fine-tuning with annotated data and enabling\\nthe model to consult external knowledge sources can lead to\\nsignificant improvements towards the two key challenges of\\nsafety and factual grounding.\\nOPT: In [86], Zhang et al. presented Open Pre-trained\\nTransformers (OPT), a suite of decoder-only pre-trained trans-\\nformers ranging from 125M to 175B parameters, which they\\nshare with researchers. The OPT models’ parameters are\\nshown in 20\\nFig. 20: Different OPT Models’ architecture details. Courtesy\\nof [86].\\nChinchilla: In [2], Hoffmann et al. investigated the optimal\\nmodel size and number of tokens for training a transformer\\nlanguage model under a given compute budget. By training\\nover 400 language models ranging from 70 million to over\\n16 billion parameters on 5 to 500 billion tokens, they found\\nthat for compute-optimal training, the model size and the\\nnumber of training tokens should be scaled equally: for every\\ndoubling of model size the number of training tokens should\\nalso be doubled. They tested this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the\\nsame compute budget as Gopher but with 70B parameters and\\n4% more more data.\\nGalactica: In [87], Taylor et al. introduced Galactica, a\\nlarge language model that can store, combine and reason about\\nscientific knowledge. They trained on a large scientific corpus\\nof papers, reference material, knowledge bases and many other\\nsources. Galactica performed well on reasoning, outperforming\\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\\nCodeGen: In [88], Nijkamp et al. trained and released\\na family of large language models up to 16.1B parameters,\\ncalled CODEGEN, on natural language and programming\\nlanguage data, and open sourced the training library JAX-\\nFORMER. They showed the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-\\nthe-art on zero-shot Python code generation on HumanEval.\\nThey further investigated the multi-step paradigm for program\\nsynthesis, where a single program is factorized into multi-\\nple prompts specifying sub-problems. They also constructed\\nan open benchmark, Multi-Turn Programming Benchmark\\n(MTPB), consisting of 115 diverse problem sets that are\\nfactorized into multi-turn prompts.\\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\\ntilingual large-scale sequence-to-sequence (seq2seq) models,\\npre-trained on a mixture of denoising and Causal Language\\nModeling (CLM) tasks, are more efficient few-shot learners\\nthan decoder-only models on various task. They trained a\\n20 billion parameter multilingual seq2seq model called Alexa\\nTeacher Model (AlexaTM 20B) and showed that it achieves\\nstate-of-the-art (SOTA) performance on 1-shot summarization\\ntasks, outperforming a much larger 540B PaLM decoder\\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\\nlayers, 32 attention heads, and dmodel = 4096.\\nSparrow: In [90], Glaese et al. presented Sparrow, an\\ninformation-seeking dialogue agent trained to be more helpful,\\ncorrect, and harmless compared to prompted language model\\nbaselines. They used reinforcement learning from human feed-\\nback to train their models with two new additions to help\\nhuman raters judge agent behaviour. The high-level pipeline\\nof Sparrow model is shown in Fig 21.\\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\\na large language model pretrained on general natural language\\ndata and further trained on technical content, to tackle previous\\nLLM struggle with quantitative reasoning (such as solving\\nmathematics, science, and engineering problems).\\nMoD: In [92], Tay et al. presented a generalized and\\nunified perspective for self-supervision in NLP and show how\\ndifferent pre-training objectives can be cast as one another\\nand how interpolating between different objectives can be\\nFig. 21: Sparrow pipeline relies on human participation to\\ncontinually expand a training set. Courtesy of [90].\\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\\ntraining objective that combines diverse pre-training paradigms\\ntogether. This framework is known as Unifying Language\\nLearning (UL2). An overview of UL2 pretraining paradigm\\nis shown in Fig 21.\\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\\nof [92].\\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\\nparameter open-access language model designed and built\\nthanks to a collaboration of hundreds of researchers. BLOOM\\nis a decoder-only Transformer language model trained on the\\nROOTS corpus, a dataset comprising hundreds of sources in\\n46 natural and 13 programming languages (59 in total). An\\noverview of BLOOM architecture is shown in Fig 23.\\nFig. 23: An overview of BLOOM architecture. Courtesy of\\n[93].\\nGLM: In [94], Zeng et al. introduced GLM-130B, a\\nbilingual (English and Chinese) pre-trained language model\\nwith 130 billion parameters. It was an attempt to open-source\\na 100B-scale model at least as good as GPT-3 (davinci) and\\nunveil how models of such a scale can be successfully pre-\\ntrained.\\nPythia: In [95], Biderman et al. introduced Pythia, a suite\\nof 16 LLMs all trained on public data seen in the exact same\\norder and ranging in size from 70M to 12B parameters. We\\nprovide public access to 154 checkpoints for each one of the\\n16 models, alongside tools to download and reconstruct their\\nexact training dataloaders for further study.\\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\\nparameter model that learns to imitate the reasoning process\\nof large foundation models. Orca learns from rich signals\\nfrom GPT-4 including explanation traces; step-by-step thought\\nprocesses; and other complex instructions, guided by teacher\\nassistance from ChatGPT.\\nStarCoder: In [97], Li et al. introduced StarCoder and\\nStarCoderBase. They are 15.5B parameter models with 8K\\ncontext length, infilling capabilities and fast large-batch in-\\nference enabled by multi-query attention. StarCoderBase is\\ntrained on one trillion tokens sourced from The Stack, a\\nlarge collection of permissively licensed GitHub repositories\\nwith inspection tools and an opt-out process. They fine-tuned\\nStarCoderBase on 35B Python tokens, resulting in the creation\\nof StarCoder. They performed the most comprehensive evalu-\\nation of Code LLMs to date and showed that StarCoderBase\\noutperforms every open Code LLM that supports multiple pro-\\ngramming languages and matches or outperforms the OpenAI\\ncode-cushman-001 model.\\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\\na Multimodal Large Language Model (MLLM) that can per-\\nceive general modalities, learn in context (i.e., few-shot), and\\nfollow instructions (i.e. zero-shot). Specifically, they trained\\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\\nincluding arbitrarily interleaved text and images, image-caption\\npairs, and text data. Experimental results show that KOSMOS-\\n1 achieves impressive performance on (i) language understand-\\ning, generation, and even OCR-free NLP (directly fed with\\ndocument images), (ii) perception-language tasks, including\\nmultimodal dialogue, image captioning, visual question an-\\nswering, and (iii) vision tasks, such as image recognition with\\ndescriptions (specifying classification via text instructions).\\nGemini: In [99], Gemini team introduced a new family of\\nmultimodal models, that exhibit promising capabilities across\\nimage, audio, video, and text understanding. Gemini family\\nincludes three versions: Ultra for highly-complex tasks, Pro\\nfor enhanced performance and deployability at scale, and Nano\\nfor on-device applications. Gemini architecture is built on top\\nof Transformer decoders, and is trained to support 32k context\\nlength (via using efficient attention mechanisms).\\nSome of the other popular LLM frameworks (or techniques\\nused for efficient developments of LLMs) includes Inner-\\nMonologue [100], Megatron-Turing NLG [101], LongFormer\\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],\\nFuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\\n[122].\\nFig 24 provides an overview of some of the most repre-\\nsentative LLM frameworks, and the relevant works that have\\ncontributed to the success of LLMs and helped to push the\\nlimits of LLMs.\\nIII.\\nHOW LLMS ARE BUILT\\nIn this section, we first review the popular architectures\\nused for LLMs, and then discuss data and modeling techniques\\nranging from data preparation, tokenization, to pre-training,\\ninstruction tuning, and alignment.\\nOnce the model architecture is chosen, the major steps\\ninvolved in training an LLM includes: data preparation (col-\\nlection, cleaning, deduping, etc.), tokenization, model pre-\\ntraining (in a self-supervised learning fashion), instruction\\ntuning, and alignment. We will explain each of them in a\\nseparate subsection below. These steps are also illustrated in\\nFig 25.\\nA. Dominant LLM Architectures\\nThe most widely used LLM architectures are encoder-only,\\ndecoder-only, and encoder-decoder. Most of them are based on\\nTransformer (as the building block). Therefore we also review\\nthe Transformer architecture here.\\n1) Transformer: in a ground-breaking work [44], Vaswani\\net al. proposed the Transformer framework, which was orig-\\ninally designed for effective parallel computing using GPUs.\\nThe heart of Transformer is the (self-)attention mechanism,\\nwhich can capture long-term contextual information much\\nmore effectively using GPUs than the recurrence and convo-\\nlution mechanisms. Fig 26 provides a high-level overview of\\ntransformer work. In this section we provide an overview of the\\nmain elements and variants, see [44], [123] for more details.\\nThe Transformer language model architecture, originally\\nproposed for machine translation, consists of an encoder and\\na decoder. The encoder is composed of a stack of N = 6\\nidentical Transformer layers. Each layer has two sub-layers.\\nThe first one is a multi-head self-attention layer, and the other\\none is a simple position-wise fully connected feed-forward\\nnetwork. The decoder is composed of a stack of 6 identical\\nlayers. In addition to the two sub-layers in each encoder layer,\\nthe decoder has a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. The attention\\nfunction can be described as mapping a query and a set of key-\\nvalue pairs to an output, where the query, keys, values, and\\noutput are all vectors. The output is computed as a weighted\\nsum of the values, where the weight assigned to each value\\nis computed by a compatibility function of the query with the\\ncorresponding key. Instead of performing a single attention\\nfunction with dmodel dimensional keys, values and queries,\\nit is found to be beneficial to linearly project the queries,\\nkeys and values h with different, learned linear projections to\\ndk, dk and dv dimensions, respectively. Positional encoding is\\nincorporated to fuse information about the relative or absolute\\nposition of the tokens in the sequence.\\nFig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣shows entities that serve\\nnot only as models but also as approaches. ♦shows only approaches.\\n2) Encoder-Only: For this family, at each stage, the atten-\\ntion layers can access all the words in the initial sentence.\\nThe pre-training of these models usually consist of some-\\nhow corrupting a given sentence (for instance, by masking\\nrandom words in it) and tasking the model with finding or\\nreconstructing the initial sentence. Encoder models are great\\nfor tasks requiring an understanding of the full sequence,\\nsuch as sentence classification, named entity recognition, and\\nextractive question answering. One prominent encoder only\\nmodel is BERT (Bidirectional Encoder Representations from\\nTransformers), proposed in [24].\\n3) Decoder-Only: For these models, at each stage, for any\\nword, the attention layers can only access the words positioned\\nbefore that in the sentence. These models are also sometimes\\ncalled auto-regressive models. The pretraining of these models\\nis usually formulated as predicting the next word (or token)\\nin the sequence. The decoder-only models are best suited for\\ntasks involving text generation. GPT models are prominent\\nexample of this model category.\\n4) Encoder-Decoder: These models use both encoder and\\ndecoder, and are sometimes called sequence-to-sequence mod-\\nels. At each stage, the attention layers of the encoder can access\\nall the words in the initial sentence, whereas the attention\\nlayers of the decoder only accesses the words positioned before\\na given word in the input. These models are usually pre-\\ntrained using the objectives of encoder or decoder models, but\\nusually involve something a bit more complex. For instance,\\nsome models are pretrained by replacing random spans of text\\n(that can contain several words) with a single mask special\\nword, and the objective is then to predict the text that this\\nmask word replaces. Encoder-decoder models are best suited\\nfor tasks about generating new sentences conditioned on a\\ngiven input, such as summarization, translation, or generative\\nquestion answering.\\nB. Data Cleaning\\nData quality is crucial to the performance of language\\nmodels trained on them. Data cleaning techniques such as\\nfiltering, deduplication, are shown to have a big impact on\\nthe model performance.\\nAs an example, in Falcon40B [124], Penedo et al. showed\\nthat properly filtered and deduplicated web data alone can lead\\nto powerful models; even significantly outperforming models\\nfrom the state-of-the-art trained on The Pile. Despite extensive\\nfiltering, they were able to obtain five trillion tokens from\\nCommonCrawl. They also released an extract of 600 billion\\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\\neters language models trained on it. 27 shows the Refinement\\nprocess of CommonCrawl data by this work.\\n1) Data Filtering: Data filtering aims to enhance the qual-\\nity of training data and the effectiveness of the trained LLMs.\\nCommon data filtering techniques include:\\nRemoving Noise: refers to eliminating irrelevant or noisy\\ndata that might impact the model’s ability to generalize well.\\nAs an example, one can think of removing false information\\nfrom the training data, to lower the chance of model generating\\nfalse responses. Two mainstream approaches for quality filter-\\ning includes: classifier-based, and heuristic-based frameworks.\\nHow LLMs Are Built?\\nData Cleaning\\nTokenizations\\nBytePairEncoding\\nWordPieceEncoding\\nSentencePieceEncoding\\nPositional Encoding\\nAbsolute Positional Embeddings\\nRelative Positional Embeddings\\nRotary Position Embeddings\\nRelative Positional Bias\\nModel Pre-training\\nMasked Language Modeling\\nCausal Language Modeling\\nNext Sentence Prediction\\nMixture of Experts\\nFine-tuning and Instruction Tuning\\nAlignment\\nSupervised learning\\nReinforcement Learning from Human Feedback\\nDirect Preference Optimization\\nKahneman-Tversky Optimization\\nDecoding Strategies\\nGreedy Search\\nBeam Search\\nTop-k Sampling\\nTop-p Sampling\\nCost-Effective Training/Inference,\\nAdaptation & Compression\\nOptimized Training\\nZero Redundancy Optimizer\\nReceptance Weighted Key Value\\nLow-Rank Adaption\\nKnowledge Distillation\\nQuantization\\nData Filtering\\nRemoving Noise\\nHandling Outliers\\nAddressing Imbalances\\nText Preprocessing\\nDeduplication\\nLLM Architectures\\nEncoder-Only\\nDecoder-Only\\nEncoder-Decoder\\n...\\nSupervised Fine-tuning\\nGeneral Fine-tuning\\nMulti-turn Instructions\\nInstruction Following\\nFig. 25: This figure shows different components of LLMs.\\nFig. 26: High-level overview of transformer work. Courtesy of\\n[44].\\nFig. 27: Subsequent stages of Macrodata Refinement remove\\nnearly 90% of the documents originally in CommonCrawl.\\nCourtesy of [124].\\nHandling Outliers: Identifying and handling outliers or\\nanomalies in the data to prevent them from disproportionately\\ninfluencing the model.\\nAddressing Imbalances: Balancing the distribution of\\nclasses or categories in the dataset to avoid biases and ensure\\nfair representation. This is specially useful for responsible\\nmodel training and evaluation.\\nText Preprocessing: Cleaning and standardizing text data\\nby removing stop words, punctuation, or other elements that\\nmay not contribute significantly to the model’s learning.\\nDealing with Ambiguities: Resolving or excluding am-\\nbiguous or contradictory data that might confuse the model\\nduring training. This can help the model to provide more\\ndefinite and reliable answers.\\n2) Deduplication: De-duplication refers to the process of\\nremoving duplicate instances or repeated occurrences of the\\nsame data in a dataset. Duplicate data points can introduce\\nbiases in the model training process and reduce the diversity, as\\nthe model may learn from the same examples multiple times,\\npotentially leading to overfitting on those particular instances.\\nSome works [125] have shown that de-duplication improves\\nmodels’ ability to generalize to new, unseen data.\\nThe de-duplication process is particularly important when\\ndealing with large datasets, as duplicates can unintentionally\\ninflate the importance of certain patterns or characteristics.\\nThis is especially relevant in NLP tasks, where diverse and\\nrepresentative training data is crucial for building robust lan-\\nguage models.\\nThe specific de-duplication method can vary based on\\nthe nature of the data and the requirements of the particular\\nlanguage model being trained. It may involve comparing entire\\ndata points or specific features to identify and eliminate du-\\nplicates. At the document level, existing works mainly rely on\\nthe overlap ratio of high-level features (e.g. n-grams overlap)\\nbetween documents to detect duplicate samples.\\nC. Tokenizations\\nTokenization referes to the process of converting a se-\\nquence of text into smaller parts, known as tokens. While\\nthe simplest tokenization tool simply chops text into tokens\\nbased on white space, most tokenization tools rely on a word\\ndictionary. However, out-of-vocabulary (OOV) is a problem\\nin this case because the tokenizer only knows words in its\\ndictionary. To increase the coverage of dictionaries, popular\\ntokenizers used for LLMs are based on sub-words, which can\\nbe combined to form a large number of words, including the\\nwords unseen in training data or words in different languages.\\nIn what follows, we describe three popular tokenizers.\\n1) BytePairEncoding: BytePairEncoding is originally a\\ntype of data compression algorithm that uses frequent patterns\\nat byte level to compress the data. By definition, this algorithm\\nmainly tries to keep the frequent words in their original form\\nand break down ones that are not common. This simple\\nparadigm keeps the vocabulary not very large, but also good\\nenough to represent common words at the same time. Also\\nmorphological forms of the frequent words can be represented\\nvery well if suffix or prefix is also commonly presented in the\\ntraining data of the algorithm.\\n2) WordPieceEncoding: This algorithm is mainly used for\\nvery well-known models such as BERT and Electra. At the\\nbeginning of training, the algorithm takes all the alphabet from\\nthe training data to make sure that nothing will be left as UNK\\nor unknown from the training dataset. This case happens when\\nthe model is given an input that can not be tokenized by the\\ntokenizer. It mostly happens in cases where some characters are\\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\\nmaximize the likelihood of putting all the tokens in vocabulary\\nbased on their frequency.\\n3) SentencePieceEncoding: Although both tokenizers de-\\nscribed before are strong and have many advantages compared\\nto white-space tokenization, they still take assumption of\\nwords being always separated by white-space as granted. This\\nassumption is not always true, in fact in some languages, words\\ncan be corrupted by many noisy elements such as unwanted\\nspaces or even invented words. SentencePieceEncoding tries\\nto address this issue.\\nD. Positional Encoding\\n1) Absolute Positional Embeddings: (APE) [44] has been\\nused in the original Transformer model to preserve the infor-\\nmation of sequence order. Therefore, the positional information\\nof words is added to the input embeddings at the bottom of\\nboth the encoder and decoder stacks. There are various options\\nfor positional encodings, either learned or fixed. In the vanilla\\nTransformer, sine and cosine functions are employed for this\\npurpose. The main drawback of using APE in Transformers\\nis the restriction to a certain number of tokens. Additionally,\\nAPE fails to account for the relative distances between tokens.\\n2) Relative Positional Embeddings: (RPE) [126] involves\\nextending self-attention to take into account the pairwise links\\nbetween input elements. RPE is added to the model at two\\nlevels: first as an additional component to the keys, and\\nsubsequently as a sub-component of the values matrix. This\\napproach looks at the input as a fully-connected graph with\\nlabels and directed edges. In the case of linear sequences, edges\\ncan capture information about the relative position differences\\nbetween input elements. A clipping distance, represented as k\\n2 ≤k ≤n −4, specifies the maximum limit on relative lo-\\ncations. This allows the model to make reasonable predictions\\nfor sequence lengths that are not part of the training data.\\n3) Rotary Position Embeddings: Rotary Positional Em-\\nbedding (RoPE) [127] tackles problems with existing ap-\\nproaches. Learned absolute positional encodings can lack gen-\\neralizability and meaningfulness, particularly when sentences\\nare short. Moreover, current methods like T5’s positional\\nembedding face challenges with constructing a full attention\\nmatrix between positions. RoPE uses a rotation matrix to\\nencode the absolute position of words and simultaneously in-\\ncludes explicit relative position details in self-attention. RoPE\\nbrings useful features like flexibility with sentence lengths, a\\ndecrease in word dependency as relative distances increase,\\nand the ability to improve linear self-attention with relative\\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\\nLLaMA are among models that take advantage of RoPE in\\ntheir architectures.\\n4) Relative Positional Bias: The concept behind this type\\nof positional embedding is to facilitate extrapolation during\\ninference for sequences longer than those encountered in train-\\ning. In [128] Press et al. proposed Attention with Linear Biases\\n(ALiBi). Instead of simply adding positional embeddings to\\nword embeddings, they introduced a bias to the attention scores\\nof query-key pairs, imposing a penalty proportional to their\\ndistance. In the BLOOM model, ALiBi is leveraged.\\nE. Model Pre-training\\nPre-training is the very first step in large language model\\ntraining pipeline, and it helps LLMs to acquire fundamental\\nlanguage understanding capabilities, which can be useful in a\\nwide range of language related tasks. During pre-training, the\\nLLM is trained on a massive amount of (usually) unlabeled\\ntexts, usually in a self-supervised manner. There are different\\napproaches used for pre-training like next sentence prediction\\n[24], two most common ones include, next token prediction\\n(autoregressive language modeling), and masked language\\nmodeling.\\nIn Autoregressive Language Modeling framework, given\\na sequence of n tokens x1, ..., xn, the model tries to predict\\nnext token xn+1 (and sometimes next sequence of tokens) in\\nan auto-regressive fashion. One popular loss function in this\\ncase is the log-likelihood of predicted tokens as shown in Eq\\n2\\nLALM(x) =\\nN\\nX\\ni=1\\np(xi+n|xi, ..., xi+n−1)\\n(1)\\nGiven the auto-regressive nature of this framework, the\\ndecoder-only models are naturally better suited to learn how\\nto accomplish these task.\\nIn Masked Language Modeling, some words are masked\\nin a sequence and the model is trained to predict the masked\\nwords based on the surrounding context. Sometimes people\\nrefer to this approach as denoising autoencoding, too. If we\\ndenote the masked/corrupted samples in the sequence x, as ˜x,\\nthen the training objective of this approach can be written as:\\nLMLM(x) =\\nN\\nX\\ni=1\\np(˜x|x\\\\˜x)\\n(2)\\nAnd more recently, Mixture of Experts (MoE) [130],\\n[131] have become very popular in LLM space too. MoEs\\nenable models to be pre-trained with much less compute,\\nwhich means one can dramatically scale up the model or\\ndataset size with the same compute budget as a dense model.\\nMoE consists of two main elements: Sparse MoE layers,\\nwhich are used instead of dense feed-forward network (FFN)\\nlayers, and have a certain number of “experts” (e.g. 8), in\\nwhich each expert is a neural network. In practice, the experts\\nare FFNs, but they can also be more complex networks. A gate\\nnetwork or router, that determines which tokens are sent to\\nwhich expert. It is worth noting that, one can send a token\\nto more than one expert. How to route a token to an expert\\nis one of the big decisions when working with MoEs - the\\nrouter is composed of learned parameters and is pretrained at\\nthe same time as the rest of the network. Fig 29 provides an\\nillustration of a Switch Transformer encoder block, which are\\nused in MoE.\\nF. Fine-tuning and Instruction Tuning\\nEarly language models such as BERT trained using self-\\nsupervision as explained in section III-E were not able to\\nperform specific tasks. In order for the foundation model to be\\nuseful it needed to be fine-tuned to a specific task with labeled\\ndata (so-called supervised fine-tuning or SFT for short). For\\nexample, in the original BERT paper [24], the model was fine-\\ntuned to 11 different tasks. While more recent LLMs no longer\\nrequire fine-tuning to be used, they can still benefit from task\\nor data-specific fine-tuning. For example, OpenAI reports that\\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\\nwhen fine-tuned with task specific data 2.\\nFine-tuning does not need to be performed to a single\\ntask though, and there are different approaches to multi-task\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\nor more tasks is known to improve results and reduce the\\ncomplexity of prompt engineering, and it can serve as an\\n2https://platform.openai.com/docs/guides/fine-tuning\\n(a) Absolute Positional Embeddings [129]\\n(b) Relative Positional Embeddings\\n(c) Rotary Positional Embedding [127]\\n(d) Relative Positional Bias [128]\\nFig. 28: Various positional encodings are employed in LLMs.\\nFig. 29: : Illustration of a Switch Transformer encoder block.\\nThey replaced the dense feed forward network (FFN) layer\\npresent in the Transformer with a sparse Switch FFN layer\\n(light blue). . Courtesy of [131].\\nalternative to retrieval augmented generation. Furthermore,\\nthere are other reasons why it might be advisable to fine-tune.\\nFor example, one might want to fine-tune to expose the model\\nto new or proprietary data that it has not been exposed to\\nduring pre-training.\\nAn important reason to fine-tune LLMs is to align the\\nresponses to the expectations humans will have when providing\\ninstructions through prompts. This is the so-called instruction\\ntuning [133]. We dive into the details of how to design\\nand engineer prompts in section IV-B, but in the context\\nof instruction tuning, it is important to understand that the\\ninstruction is a prompt that specifies the task that the LLM\\nshould accomplish. Instruction tuning datasets such as Natural\\nInstructions [134] include not only the task definition but other\\ncomponents such as positive/negative examples or things to\\navoid.\\nThe specific approach and instruction datasets used to\\ninstruction-tune an LLM varies, but, generally speaking, in-\\nstruction tuned models outperform their original foundation\\nmodels they are based on. For example, InstructGPT [59]\\noutperforms GPT-3 on most benchmarks. The same is true\\nfor Alpaca [62] when compared to LLaMA.\\nSelf-Instruct [135], proposed by Wang et al. is also a\\npopular approach along this line, in which they introduced a\\nframework for improving the instruction-following capabilities\\nof pre-trained language models by bootstrapping their own\\ngenerations. Their pipeline generates instructions, input, and\\noutput samples from a language model, then filters invalid or\\nsimilar ones before using them to fine tune the original model.\\nG. Alignment\\nAI Alignment is the process of steering AI systems towards\\nhuman goals, preferences, and principles. LLMs, pre-trained\\nfor word prediction, often exhibit unintended behaviors. For\\nexample, they might generate contents that are toxic, harmful,\\nmisleading and biased.\\nInstruction tuning, discussed above, gets LLMs a step\\ncloser to being aligned. However, in many cases, it is important\\nto include further steps to improve the alignment of the model\\nand avoid unintended behaviors 3. We review the most popular\\n3According to very recent research by Ethayarajh et al. [136], further\\nalignment besides SFT mainly improves models of at least 7B parameters.\\nFor smaller models, SFT is sufficient.\\napproaches to alignment in this subsection.\\nRLHF (reinforcement learning from human feedback) and\\nRLAIF (reinforcement learning from AI feedback) are two\\npopular approaches. RLHF uses a reward model to learn\\nalignment from human feedback. This reward model, after\\nbeing tuned, is able to rate different outputs and score them\\naccording to their alignment preferences given by humans. The\\nreward model gives feedback to the original LLM and this\\nfeedback is used to tune the LLM further [137]. Reinforcement\\nlearning from AI feedback on the other hand, directly connects\\na pretrained and well-aligned model to the LLM and helps it\\nto learn from larger and more aligned models [138].\\nIn another recent work (known as DPO) [139], Rafailov\\net al. discussed that RLHF is a complex and often unstable\\nprocedure, and tried to address this with a new approach. They\\nleveraged a mapping between reward functions and optimal\\npolicies to show that this constrained reward maximization\\nproblem can be optimized exactly with a single stage of policy\\ntraining, essentially solving a classification problem on the\\nhuman preference data. The resulting algorithm, which they\\ncalled Direct Preference Optimization (DPO), is stable, per-\\nformant, and computationally lightweight, eliminating the need\\nfor fitting a reward model, sampling from the LM during fine-\\ntuning, or performing significant hyperparameter tuning. They\\nobserved that fine-tuning with DPO exceeds RLHF’s ability to\\ncontrol sentiment of generations and improves response quality\\nin summarization. Fig 30 shows the high-level comparison\\nbetween DPO vs RLHF.\\nFig. 30: DPO optimizes for human preferences while avoiding\\nreinforcement learning. Existing methods for fine-tuning lan-\\nguage models with human feedback first fit a reward model\\nto a dataset of prompts and human preferences over pairs of\\nresponses, and then use RL to find a policy that maximizes\\nthe learned reward. In contrast, DPO directly optimizes for\\nthe policy best satisfying the preferences with a simple classi-\\nfication objective, without an explicit reward function or RL.\\nCourtesy of [139].\\nEven more recently Ethayarajh et al. proposed a new align-\\nment approach called the Kahneman-Tversky Optimization\\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\\ndoes not require paired preference data (x, yw, yl), and it\\nonly needs (x,y) and knowledge of whether y is desirable or\\nundesirable. KTO-aligned models are shown to be good or\\nbetter than DPO-aligned models at scales from 1B to 30B,\\ndespite not using paired preferences. KTO is also far easier to\\nuse in the real world than preference optimization methods, as\\nthe kind of data it needs is far more abundant. As an example,\\nevery retail company has a lot of customer interaction data and\\nwhether that interaction was successful (e.g., purchase made)\\nor unsuccessful (e.g., no purchase made). However, They have\\nlittle to no counterfactual data (i.e., what would have made\\nan unsuccessful customer interaction yl into a successful one\\nyw). Fig 31 shows a high-level comparison between KTO and\\nother alignment approaches discussed above.\\nFig. 31: LLM alignment involves supervised finetuning fol-\\nlowed by optimizing a human-centered loss (HALO). How-\\never, the paired preferences that existing approaches need are\\nhard-to-obtain. In contrast, KTO uses a far more abundant\\nkind of data, making it much easier to use in the real world.\\nCourtesy of [136].\\nH. Decoding Strategies\\nDecoding refers to the process of text generation using pre-\\ntrained LLMs. Given an input prompt, the tokenizer translates\\neach token in the input text into a corresponding token ID.\\nThen, the language model uses these token IDs as input and\\npredicts the next most likely token (or a sequence of tokens).\\nFinally, the model generates logits, which are converted to\\nprobabilities using a softmax function. Different decoding\\nstrategies have been proposed. Some of the most popular ones\\nare greedy search, beam search, as well as different sample\\ntechniques such as top-K, top-P (Nucleus sampling).\\n1) Greedy Search: Greedy search takes the most probable\\ntoken at each step as the next token in the sequence, discarding\\nall other potential options. As you can imagine, this is a simple\\napproach and can loose a lot of temporal consistency and\\ncoherency. It only considers the most probable token at each\\nstep, without considering the overall effect on the sequence.\\nThis property makes it fast, but it also means that it can miss\\nout on better sequences that might have appeared with slightly\\nless probable next tokens.\\n2) Beam Search: Unlike greedy search that only considers\\nthe next most probable token, beam search takes into account\\nthe N most likely tokens, where N denotes the number of\\nbeams. This procedure is repeated until a predefined maxi-\\nmum sequence length is reached or an end-of-sequence token\\nappears. At this point, the sequence of tokens (AKA “beam”)\\nwith the highest overall score is chosen as the output. For\\nexample for beam size of 2 and maximum length of 5,\\nthe beam search needs to keep track of 25 = 32 possible\\nsequences. So it is more computationally intensive than greedy\\nsearch.\\n3) Top-k Sampling: Top-k sampling is a technique that\\nuses the probability distribution generated by the language\\nmodel to select a token randomly from the k most likely\\noptions.\\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)=\\n12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\\nand the model outputs A 60% of the time, and B, 40% of\\nthe time. This approach ensures that we prioritize the most\\nprobable tokens while introducing an element of randomness\\nin the selection process.\\nThe randomness is usually introduced via the concept of\\ntemperature. The temperature T is a parameter that ranges from\\n0 to 1, which affects the probabilities generated by the softmax\\nfunction, making the most likely tokens more influential. In\\npractice, it simply consists of dividing the input logits by\\ntemperature value:\\nsoftmax(xi) =\\nexi/T\\nP\\nj exj/T\\n(3)\\nA low temperature setting significantly alters the proba-\\nbility distribution (and is commonly used in text generation\\nto control the level of “creativity” in the generated output),\\nwhile a large temperature prioritizes the tokens with higher\\nprobabilities. Top-k is a creative way of sampling, and can be\\nused along with beam search. The sequence chosen by top-\\nk sampling may not be the sequence with highest probability\\nin beam search. But it’s important to remember that highest\\nscores do not always lead to more realistic or meaningful\\nsequences.\\n4) Top-p Sampling: Top-p sampling, also known as Nu-\\ncleus sampling, takes a slightly different approach from top-k\\nsampling. Instead of selecting the top k most probable tokens,\\nnucleus sampling chooses a cutoff value p such that the sum of\\nthe probabilities of the selected tokens exceeds p. This forms\\na “nucleus” of tokens from which to randomly choose the next\\ntoken. In other words, in top-p sampling the language model\\nexamines the most probable tokens in descending order and\\nkeeps adding them to the list until the sum of probabilities\\nsurpasses the threshold p. As you can imagine, this could be\\nbetter specially for scenarios in which top-k tokens do not have\\na large probability mass. Unlike top-k sampling, the number\\nof tokens included in the nucleus sampling is not fixed. This\\nvariability often results in a more diverse and creative output,\\nmaking nucleus sampling popular for text generation related\\ntasks.\\nI.\\nCost-Effective Training/Inference/Adaptation/Compression\\nIn this part, we review some of the popular approaches\\nused for more cost-friendly (and compute-friendly) training\\nand usage of LLMs.\\n1) Optimized Training: There are many frameworks de-\\nveloped for optimized training of LLMs, here we introduce\\nsome of the prominent ones.\\nZeRO:\\nIn [140], Rajbhandari et al. developed a novel\\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\\nmemory, vastly improving training speed of LLMs while\\nincreasing the model size that can be efficiently trained. ZeRO\\neliminates memory redundancies in data- and model-parallel\\ntraining while retaining low communication volume and high\\ncomputational granularity, allowing one to scale the model\\nsize proportional to the number of devices with sustained high\\nefficiency.\\nRWKV: In [141], Peng et al. proposed a novel model\\narchitecture, Receptance Weighted Key Value (RWKV), that\\ncombines the efficient parallelizable training of Transformers\\nwith the efficient inference of RNNs. Their approach leverages\\na linear attention mechanism and allows them to formulate the\\nmodel as either a Transformer or an RNN, which parallelizes\\ncomputations during training and maintains constant compu-\\ntational and memory complexity during inference, leading to\\nthe first non-transformer architecture to be scaled to tens of\\nbillions of parameters. RWKV architecture is shown in Fig\\n32. The Time Complexity comparison of RWKV with different\\nFig. 32: RWKV architecture. Courtesy of [141].\\nTransformers are provided in Fig 33.\\nFig. 33: Time Complexity comparison of RWKV with different\\nTransformers. Here T denotes the sequence length, d the\\nfeature dimension, and c is MEGA’s chunk size of quadratic\\nattention. Courtesy of [141].\\n2) Low-Rank Adaption (LoRA): Low-Rank Adaptation is\\na popular and lightweight training technique that significantly\\nreduces the number of trainable parameters, and is based\\non a crucial insight that the difference between the fine-\\ntuned weights for a specialized task and the initial pre-trained\\nweights often exhibits “low intrinsic rank” - meaning that\\nit can be approximated well by a low rank matrix [142].\\nFig. 34: An illustration of LoRA reparametrizan. Only A and\\nB trained during this process. Courtesy of [142].\\nTraining with LoRA is much faster, memory-efficient, and\\nproduces smaller model weights (a few hundred MBs), that are\\neasier to store and share. One property of low-rank matrices\\nis that they can be represented as the product of two smaller\\nmatrices. This realization leads to the hypothesis that this delta\\nbetween fine-tuned weights and initial pre-trained weights can\\nbe represented as the matrix product of two much smaller\\nmatrices. By focusing on updating these two smaller matrices\\nrather than the entire original weight matrix, computational\\nefficiency can be substantially improved.\\nSpecifically, for a pre-trained weight matrix W0 ∈Rd×k,\\nLoRA constrains its update by representing the latter with\\na low-rank decomposition W0 + ∆W = W0 + BA, where\\nB ∈Rd×r , A ∈Rr×k, and the rank r ≪min(d, k). During\\ntraining, W0 is frozen and does not receive gradient updates,\\nwhile A and B contain trainable parameters. It is worth\\nmentioning that both W0 and ∆W = BA are multiplied with\\nthe same input, and their respective output vectors are summed\\ncoordinate-wise. For h = W0x, their modified forward pass\\nyields: h = W0x + ∆Wx = W0x + BAx. Usually a random\\nGaussian initialization is used for A, and zero initialization\\nfor B, so ∆W = BA is zero at the beginning of training.\\nThey then scale ∆Wx by αr, where α is a constant in r. This\\nreparametrization is illustrated in Figure 34\\nIt is worth mentioning that LoRA can be applied to any a\\nsubset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architec-\\nture, there are four weight matrices in the self-attention module\\n(Wq , Wk, Wv , Wo), and two in the MLP module. Most of\\nthe time, LoRA is focused on adapting the attention weights\\nonly for downstream tasks, and freezes the MLP modules, so\\nthey are not trained in downstream tasks both for simplicity\\nand parameter-efficiency.\\n3) Knowledge Distillation: Knowledge distillation is the\\nprocess of learning from a larger model [143]. Earlier days of\\nbest-performing models release have proven that this approach\\nis very useful even if it is used in an API distillation approach.\\nIt is also referred to as an approach to distill the knowledge of\\nnot a single model but in fact multiple models into a smaller\\none. Creating smaller models by this approach yields smaller\\nmodel sizes that can be used even on edge devices. Knowledge\\ndistillation as shown in Fig 35, illustrates a general setup of\\nthis training scheme.\\nFig. 35: A generic knowledge distillation framework with\\nstudent and teacher (Courtesy of [144]).\\nKnowledge can be transferred by different forms of learn-\\ning: response distillation, feature distillation, and API distilla-\\ntion. Response distillation is concerned only with the outputs\\nof the teacher model and tries to teach the student model\\nhow to exactly or at least similarly perform (in the sense of\\nprediction) as the teacher. Feature distillation not only uses\\nthe last layer but also intermediate layers as well to create a\\nbetter inner representation for the student model. This helps the\\nsmaller model to have a similar representation as the teacher\\nmodel.\\nAPI distillation is the process of using an API (typically\\nfrom an LLM provider such as OpenAI) to train smaller\\nmodels. In the case of LLMs, it is used to train the model\\nfrom the direct output of the larger model which makes it very\\nsimilar to response distillation. Many concerns are raised by\\nthis type of distillation because in cases where the model itself\\nis not openly available, a (usually) paid API is exposed for end\\nusers. On the other hand, while users pay for each call, how to\\nuse the predictions is limited, for example, OpenAI prohibits\\nusage of its API to create LLMs that later will be used to\\ncompete with it. The main value in such case is training data.\\n4) Quantization: deep learning in its core, is a set of\\nmathematical functions applied to matrices, with a specific\\nprecision for model weights. Reducing the precision of the\\nweights can be used to reduce the size of the model and also\\nmake it faster. As an example, Float-32 operations compared\\nto Int-8 operations are slower. This process, which is called\\nquantization, can be applied in different phases. Main ap-\\nproaches for model quantization can be categorized as: post\\ntraining quantization and quantization-aware training. Post-\\ntraining quantization is concerned with quantized trained mod-\\nels in two well-known methods: dynamic and static. Dynamic\\npost-training quantization computes the range of quantization\\non the runtime and is slower compared to static. Quantization-\\naware training adds quantization criteria into training, and\\na quantized model is trained and optimized during training\\nprocess. This approach ensures that the end model will have\\ngood performance and also does not need to be quantized after\\ntraining.\\nIV.\\nHOW LLMS ARE USED AND AUGMENTED\\nOnce the LLMs are trained, we can use them to generate\\ndesired outputs for a variety of tasks. LLMs can be used\\ndirectly through basic prompting. However, in order to exploit\\ntheir full potential or to address some of the shortcomings,\\nwe need to augment the models through some external means.\\nIn this section we first provide a brief overview of the main\\nshortcoming of LLMs, with a deeper look at the issue of\\nhallucination. We then describe how prompting and some aug-\\nmentation approaches can not only address those limitations\\nbut also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n•\\nThey don’t have state/memory. LLMs on their own\\ncannot remember even what was sent to them in the\\nprevious prompt. That is an important limitation for\\nmany of the uses cases that require some form of state.\\n•\\nThey are stochastic/probabilistic. If you send the same\\nprompt to an LLM several times, you are likely to get\\ndifferent responses. While there are parameters, and\\nin particular the temperature, to limit the variability\\nin the response, this is an inherent property of their\\ntraining that can create issues.\\n•\\nThey have stale information and, on their own, don’t\\nhave access to external data. An LLM on its own does\\nnot even know about the current time or day and does\\nnot have access to any information that was not present\\nin its training set.\\n•\\nThey are generally very large. This means that many\\ncostly GPU machines are needed for training and\\nserving. In some cases, largest models have poor\\nSLAs, particularly in terms of latency.\\n•\\nThey hallucinate. LLMs do not have a notion of\\n”truth” and they have usually been trained on a mix\\nof good and bad content. They can produce very\\nplausible but untruthful answers.\\nWhile the previous limitations can all become important\\nfor some applications, it is worth for us to dive a bit into the\\nlast one, hallucinations, since it has gathered a lot of interest\\nover the past few months and it has also sparked many of the\\nprompt approaches and LLM augmentation methods we will\\nlater describe.\\nHallucination: In the realm of Large Language Models\\n(LLMs), the phenomenon of ”hallucinations” has garnered\\nsignificant attention. Defined in the literature, notably in the\\n”Survey of Hallucination in Natural Language Generation”\\npaper [145], hallucination in an LLM is characterized as\\n”the generation of content that is nonsensical or unfaithful\\nto the provided source.” This terminology, although rooted in\\npsychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:\\n1)\\nIntrinsic Hallucinations: These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2)\\nExtrinsic Hallucinations: These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.\\nThe definition of ’source’ in LLM contexts varies with the\\ntask. In dialogue-based tasks, it refers to ’world knowledge’,\\nwhereas in text summarization, it pertains to the input text\\nitself. This distinction plays a crucial role in evaluating and\\ninterpreting hallucinations. The impact of hallucinations is also\\nhighly context-dependent. For instance, in creative endeavors\\nlike poem writing, hallucinations might be deemed acceptable\\nor even beneficial.\\nLLMs, trained on diverse datasets including the internet,\\nbooks, and Wikipedia, generate text based on probabilistic\\nmodels without an inherent understanding of truth or falsity.\\nRecent advancements like instruct tuning and Reinforcement\\nLearning from Human Feedback (RLHF) have attempted to\\nsteer LLMs towards more factual outputs, but the fundamental\\nprobabilistic nature and its inherent limitations remain. A\\nrecent study, “Sources of Hallucination by Large Language\\nModels on Inference Tasks” [146], highlights two key aspects\\ncontributing to hallucinations in LLMs: the veracity prior and\\nthe relative frequency heuristic, underscoring the complexities\\ninherent in LLM training and output generation.\\nEffective automated measurement of hallucinations in\\nLLMs requires a combination of statistical and model-based\\nmetrics.\\nStatistical Metrics:\\n•\\nMetrics like ROUGE [147] and BLEU [148] are com-\\nmon for assessing text similarity, focusing on intrinsic\\nhallucinations.\\n•\\nAdvanced metrics such as PARENT [149], PARENT-\\nT [150], and Knowledge F1 [151] are utilized when\\nstructured knowledge sources are available. These\\nmetrics, while effective, have limitations in capturing\\nsyntactic and semantic nuances.\\nModel-Based Metrics:\\n•\\nIE-Based Metrics: Utilize Information Extraction\\nmodels to simplify knowledge into relational tuples,\\nthen compare these with the source.\\n•\\nQA-Based Metrics: Assess the overlap between gen-\\nerated content and the source through a question-\\nanswering framework (see [152]).\\n•\\nNLI-Based Metrics: Use Natural Language Inference\\ndatasets to evaluate the truthfulness of a generated\\nhypothesis based on a given premise (see [153]).\\n•\\nFaithfulness Classification Metrics: Offer a refined\\nassessment by creating task-specific datasets for a\\nnuanced evaluation (see [154]).\\nDespite advances in automated metrics, human judgment\\nremains a vital piece. It typically involves two methodologies:\\nB) Augmenting LLMs through\\nexternal knowledge - RAG\\nHow LLMs Are Used and Augmented\\nC) Using External Tools\\nD) LLM Agents\\nFunctionality of an LLM-based agent\\nTool Access and Utilization\\nDecision Making\\nPrompt engineering techniques for agents\\nReasoning without Observation\\nReason and Act\\nDialog-Enabled Resolving Agents\\na) RAG-aware prompting techniques\\na) Tool-aware prompting techniques\\nA)\\xa0LLM limitations\\nHallucination\\nHallucination Quantification\\nAutomated metrics\\nHuman judgment\\nStatistical Metrics\\nModel-Based Metrics\\nScoring\\nComparative Analysis\\nIE-Based Metrics\\nQA-Based Metrics\\nNLI-Based Metrics\\nB)\\xa0Using LLMs\\n\\xa0Prompt Design and Engineering\\n1) Chain of Thought\\nZero-Shot CoT\\nManual CoT\\n5) Expert Prompting\\n6) Chains\\n2) Tree of Thought\\n7) Rails\\nTopical Rails\\nFact-Checking Rails\\nJailbreaking Rails\\n8) Automatic Prompt Engineering\\nPrompt Generation\\nPrompt Scoring\\nRefinement and Iteration\\n3)\\xa0Self-Consistency\\n4) Reflection\\nComponents of a RAG\\nRetrieval\\xa0\\nGeneration\\xa0\\nAugmentation\\nRAG Tools\\nLangChain\\xa0\\nLlamaIndex\\nHayStack\\nMeltano\\nCohere Coral\\nFlowise AI\\nFig. 36: How LLMs Are Used and Augmented.\\n1)\\nScoring: Human evaluators rate the level of halluci-\\nnation within a predefined scale.\\n2)\\nComparative Analysis: Evaluators compare gener-\\nated content against baseline or ground-truth refer-\\nences, adding an essential layer of subjective assess-\\nment.\\nFactScore [155] is a recent example of a metric that can be\\nused both for human and model-based evaluation. The metric\\nbreaks an LLM generation into “atomic facts”. The final score\\nis computed as the sum of the accuracy of each atomic fact,\\ngiving each of them equal weight. Accuracy is a binary number\\nthat simply states whether the atomic fact is supported by the\\nsource. The authors implement different automation strategies\\nthat use LLMs to estimate this metric.\\nFinally, mitigating hallucinations in LLMs is a multifaceted\\nchallenge, requiring tailored strategies to suit various applica-\\ntions. Those include:\\n•\\nProduct Design and User Interaction Strategies such\\nas use case design, structuring the input/output, or\\nproviding mechanisms for user feedback.\\n•\\nData Management and Continuous Improvement.\\nMaintaining and analyzing a tracking set of hallucina-\\ntions is essential for ongoing model improvement.\\n•\\nPrompt Engineering and Metaprompt Design. Many\\nof the advanced prompt techniques described in IV-B\\nsuch as Retrieval Augmented Generation directly ad-\\ndress hallucination risks.\\n•\\nModel Selection and Configuration for Hallucination\\nMitigation. For exemple, larger models with lower\\ntemperature settings usually perform better. Also,\\ntechniques such as RLHF or domain-sepcific fine-\\ntuning can mitigate hallucination risks.\\nB. Using LLMs: Prompt Design and Engineering\\nA prompt in generative AI models is the textual input\\nprovided by users to guide the model’s output. This could\\nrange from simple questions to detailed descriptions or specific\\ntasks. Prompts generally consist of instructions, questions,\\ninput data, and examples. In practice, to elicit a desired\\nresponse from an AI model, a prompt must contain either\\ninstructions or questions, with other elements being optional.\\nAdvanced prompts involve more complex structures, such as\\n”chain of thought” prompting, where the model is guided to\\nfollow a logical reasoning process to arrive at an answer.\\nPrompt engineering is a rapidly evolving discipline that\\nshapes the interactions and outputs of LLMs and other gen-\\nerative AI models. The essence of prompt engineering lies in\\ncrafting the optimal prompt to achieve a specific goal with\\na generative model. This process is not only about instructing\\nthe model but also involves some understanding of the model’s\\ncapabilities and limitations, and the context within which it\\noperates.\\nPrompt engineering transcends the mere construction of\\nprompts; it requires a blend of domain knowledge, understand-\\ning of the AI model, and a methodical approach to tailor\\nprompts for different contexts. This might involve creating\\ntemplates that can be programmatically modified based on a\\ngiven dataset or context. For example, generating personalized\\nresponses based on user data might use a template that is\\ndynamically filled with relevant user information.\\nFurthermore, prompt engineering is an iterative and ex-\\nploratory process, akin to traditional machine learning prac-\\ntices such as model evaluation or hyperparameter tuning. The\\nrapid growth of this field suggests its potential to revolutionize\\ncertain aspects of machine learning, moving beyond traditional\\nmethods like feature or architecture engineering. On the other\\nhand, traditional engineering practices such as version con-\\ntrol and regression testing need to be adapted to this new\\nparadigm just like they were adapted to other machine learning\\napproaches [156].\\nIn the following paragraphs we detail some of the most\\ninteresting and popular prompt engineering approaches.\\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\\ntechnique, initially described in the paper “Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models”[34]\\nby Google researchers, represents a pivotal advancement in\\nprompt engineering for Large Language Models (LLMs).\\nThis approach hinges on the understanding that LLMs, while\\nproficient in token prediction, are not inherently designed for\\nexplicit reasoning. CoT addresses this by guiding the model\\nthrough essential reasoning steps.\\nCoT is based on making the implicit reasoning process of\\nLLMs explicit. By outlining the steps required for reasoning,\\nthe model is directed closer to a logical and reasoned output,\\nespecially in scenarios demanding more than simple informa-\\ntion retrieval or pattern recognition.\\nCoT prompting manifests in two primary forms:\\n1)\\nZero-Shot CoT: This form involves instructing the\\nLLM to “think step by step”, prompting it to de-\\nconstruct the problem and articulate each stage of\\nreasoning.\\n2)\\nManual CoT: A more complex variant, it requires\\nproviding step-by-step reasoning examples as tem-\\nplates for the model. While yielding more effective\\nresults, it poses challenges in scalability and mainte-\\nnance.\\nManual CoT is more effective than zero-shot. However,\\nthe effectiveness of this example-based CoT depends on the\\nchoice of diverse examples, and constructing prompts with\\nsuch examples of step by step reasoning by hand is hard and\\nerror prone. That is where automatic CoT [157] comes into\\nplay.\\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\\n[158] prompting technique is inspired by the concept of\\nconsidering various alternative solutions or thought processes\\nbefore converging on the most plausible one. ToT is based\\non the idea of branching out into multiple ”thought trees”\\nwhere each branch represents a different line of reasoning.\\nThis method allows the LLM to explore various possibilities\\nand hypotheses, much like human cognitive processes where\\nmultiple scenarios are considered before determining the most\\nlikely one.\\nA critical aspect of ToT is the evaluation of these reasoning\\npaths. As the LLM generates different branches of thought,\\neach is assessed for its validity and relevance to the query.\\nThis process involves real-time analysis and comparison of\\nthe branches, leading to a selection of the most coherent and\\nlogical outcome.\\nToT is particularly useful in complex problem-solving\\nscenarios where a single line of reasoning might not suffice.\\nIt allows LLMs to mimic a more human-like problem-solving\\napproach, considering a range of possibilities before arriving\\nat a conclusion. This technique enhances the model’s ability\\nto handle ambiguity, complexity, and nuanced tasks, making it\\na valuable tool in advanced AI applications.\\n3) Self-Consistency:\\nSelf-Consistency [159] utilizes an\\nensemble-based method, where the LLM is prompted to gen-\\nerate multiple responses to the same query. The consistency\\namong these responses serves as an indicator of their accuracy\\nand reliability.\\nThe Self-Consistency approach is grounded in the principle\\nthat if an LLM generates multiple, similar responses to the\\nsame prompt, it is more likely that the response is accurate.\\nThis method involves asking the LLM to tackle a query mul-\\ntiple times, each time analyzing the response for consistency.\\nThis technique is especially useful in scenarios where factual\\naccuracy and precision are paramount.\\nThe consistency of responses can be measured using vari-\\nous methods. One common approach is to analyze the overlap\\nin the content of the responses. Other methods may include\\ncomparing the semantic similarity of responses or employing\\nmore sophisticated techniques like BERT-scores or n-gram\\noverlaps. These measures help in quantifying the level of\\nagreement among the responses generated by the LLM.\\nSelf-Consistency has significant applications in fields\\nwhere the veracity of information is critical. It is particularly\\nrelevant in scenarios like fact-checking, where ensuring the\\naccuracy of information provided by AI models is essential.\\nBy employing this technique, prompt engineers can enhance\\nthe trustworthiness of LLMs, making them more reliable for\\ntasks that require high levels of factual accuracy.\\n4) Reflection: Reflection [160] involves prompting LLMs\\nto assess and potentially revise their own outputs based on\\nreasoning about the correctness and coherence of their re-\\nsponses. The concept of Reflection centers on the ability of\\nLLMs to engage in a form of self-evaluation. After generating\\nan initial response, the model is prompted to reflect on its\\nown output, considering factors like factual accuracy, logical\\nconsistency, and relevance. This introspective process can lead\\nto the generation of revised or improved responses.\\nA key aspect of Reflection is the LLM’s capacity for\\nself-editing. By evaluating its initial response, the model can\\nidentify potential errors or areas of improvement. This iterative\\nprocess of generation, reflection, and revision enables the LLM\\nto refine its output, enhancing the overall quality and reliability\\nof its responses.\\n5) Expert Prompting: Expert Prompting [161] enhances the\\ncapabilities of Large Language Models (LLMs) by simulating\\nthe responses of experts in various fields. This method involves\\nprompting the LLMs to assume the role of an expert and re-\\nspond accordingly, providing high-quality, informed answers.\\nA key strategy within Expert Prompting is the multi-expert\\napproach. The LLM is prompted to consider responses from\\nmultiple expert perspectives, which are then synthesized to\\nform a comprehensive and well-rounded answer. This tech-\\nnique not only enhances the depth of the response but also\\nincorporates a range of viewpoints, reflecting a more holistic\\nunderstanding of the subject matter.\\n6) Chains: Chains refer to the method of linking multiple\\ncomponents in a sequence to handle complex tasks with Large\\nLanguage Models (LLMs). This approach involves creating a\\nseries of interconnected steps or processes, each contributing\\nto the final outcome. The concept of Chains is based on\\nthe idea of constructing a workflow where different stages\\nor components are sequentially arranged. Each component in\\na Chain performs a specific function, and the output of one\\nserves as the input for the next. This end-to-end arrangement\\nallows for more complex and nuanced processing, as each\\nstage can be tailored to handle a specific aspect of the task.\\nChains can vary in complexity and structure, depending on\\nthe requirements. In “PromptChainer: Chaining Large Lan-\\nguage Model Prompts through Visual Programming” [162],\\nthe authors not only describe the main challenges in designing\\nchains, but also describe a visual tool to support those tasks.\\n7) Rails: Rails in advanced prompt engineering refer to\\na method of guiding and controlling the output of Large\\nLanguage Models (LLMs) through predefined rules or tem-\\nplates. This approach is designed to ensure that the model’s\\nresponses adhere to certain standards or criteria, enhancing the\\nrelevance, safety, and accuracy of the output. The concept of\\nRails involves setting up a framework or a set of guidelines\\nthat the LLM must follow while generating responses. These\\nguidelines are typically defined using a modeling language or\\ntemplates known as Canonical Forms, which standardize the\\nway natural language sentences are structured and delivered.\\nRails can be designed for various purposes, depending on\\nthe specific needs of the application:\\n•\\nTopical Rails: Ensure that the LLM sticks to a\\nparticular topic or domain.\\n•\\nFact-Checking Rails: Aimed at minimizing the gen-\\neration of false or misleading information.\\n•\\nJailbreaking Rails: Prevent the LLM from generating\\nresponses that attempt to bypass its own operational\\nconstraints or guidelines.\\n8) Automatic\\nPrompt\\nEngineering\\n(APE):\\nAutomatic\\nPrompt Engineering (APE) [163] focuses on automating the\\nprocess of prompt creation for Large Language Models\\n(LLMs). APE seeks to streamline and optimize the prompt\\ndesign process, leveraging the capabilities of LLMs themselves\\nto generate and evaluate prompts. APE involves using LLMs\\nin a self-referential manner where the model is employed\\nto generate, score, and refine prompts. This recursive use of\\nLLMs enables the creation of high-quality prompts that are\\nmore likely to elicit the desired response or outcome.\\nThe methodology of APE can be broken down into several\\nkey steps:\\n•\\nPrompt Generation: The LLM generates a range of\\npotential prompts based on a given task or objective.\\n•\\nPrompt Scoring: Each generated prompt is then\\nevaluated for its effectiveness, often using criteria\\nlike clarity, specificity, and likelihood of eliciting the\\ndesired response.\\n•\\nRefinement and Iteration: Based on these evalua-\\ntions, prompts can be refined and iterated upon, further\\nenhancing their quality and effectiveness.\\nC. Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a\\nknowledge graph, see figure 38 ). The relevant information is\\nthen added to the original prompt and fed to the LLM in order\\nfor the model to generate the final response. A RAG system\\nincludes three important components: Retrieval, Generation,\\nAugmentation [165].\\na) RAG-aware prompting techniques: Because of the\\nimportance of RAG to build advanced LLM systems, several\\nRAG-aware prompting techniques have been developed re-\\ncently. One such technique is Forward-looking Active Retrieval\\nAugmented Generation (FLARE)\\nForward-looking Active Retrieval Augmented Generation\\n(FLARE) [168] enhances the capabilities of Large Language\\nModels (LLMs) by iteratively combining prediction and in-\\nformation retrieval. FLARE represents an evolution in the\\nuse of retrieval-augmented generation, aimed at improving the\\naccuracy and relevance of LLM responses.\\nFLARE involves an iterative process where the LLM\\nactively predicts upcoming content and uses these predictions\\nas queries to retrieve relevant information. This method con-\\ntrasts with traditional retrieval-augmented models that typically\\nretrieve information once and then proceed with generation. In\\nFLARE, this process is dynamic and ongoing throughout the\\ngeneration phase. In FLARE, each sentence or segment gener-\\nated by the LLM is evaluated for confidence. If the confidence\\nlevel is below a certain threshold, the model uses the generated\\ncontent as a query to retrieve relevant information, which is\\nthen used to regenerate or refine the sentence. This iterative\\nFig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\\nFig. 38: This is one example of synthesizing the KG as a\\nretriever with LLMs [167].\\nprocess ensures that each part of the response is informed by\\nthe most relevant and current information available.\\nFor more details on RAG framework and its relevant works,\\nwe refer the readers to this survey of retrieval augmented\\ngenerations [165].\\nD. Using External Tools\\nRetrieving information from an external knowledge source\\nas described above is only one of the potential ways to augment\\nan LLM. More generally, an LLM can access any number\\nof external tools (e.g. an API to a service) to augment its\\nfunctionality. In that regards, RAG can be seen as a specific\\ninstance of the broader category of the so called ”tools”.\\nTools in this context are external functions or services that\\nLLMs can utilize. These tools extend the range of tasks an\\nLLM can perform, from basic information retrieval to complex\\ninteractions with external databases or APIs.\\nIn the paper ”Toolformer: Language Models Can Teach\\nThemselves to Use Tools” [169], the authors go beyond simple\\ntool usage by training an LLM to decide what tool to use\\nwhen, and even what parameters the API needs. Tools include\\ntwo different search engines, or a calculator. In the following\\nexamples, the LLM decides to call an external Q&A tool,\\na calculator, and a Wikipedia Search Engine More recently,\\nresearchers at Berkeley have trained a new LLM called Gorilla\\n[67] that beats GPT-4 at the use of APIs, a specific but quite\\ngeneral tool.\\na) Tool-aware prompting techniques: Similarly to what\\nwas described with RAG, several tool-aware prompting ap-\\nproaches have been developed to make usage of tools more\\nscalable. A popular technique is the so called Automatic Multi-\\nstep Reasoning and Tool-use (ART).\\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\\nis a prompt engineering technique that combines automated\\nchain of thought prompting with the use of external tools.\\nART represents a convergence of multiple prompt engineering\\nstrategies, enhancing the ability of Large Language Models\\n(LLMs) to handle complex tasks that require both reasoning\\nand interaction with external data sources or tools.\\nART involves a systematic approach where, given a task\\nand input, the system first identifies similar tasks from a task\\nlibrary. These tasks are then used as examples in the prompt,\\nguiding the LLM on how to approach and execute the current\\ntask. This method is particularly effective when tasks require a\\ncombination of internal reasoning and external data processing\\nor retrieval.\\nE. LLM Agents\\nThe idea of AI agents has been well-explored in the history\\nof AI. An agent is typically an autonomous entity that can\\nperceive the environment using its sensors, make a judgment\\nbased on the state it currently is, and accordingly act based on\\nthe actions that are available to it.\\nIn the context of LLMs, an agent refers to a system based\\non a specialized instantiation of an (augmented) LLM that\\nis capable of performing specific tasks autonomously. These\\nagents are designed to interact with users and environment to\\nmake decisions based on the input and the intended goal of\\nthe interaction. Agents are based on LLMs equipped with the\\nability to access and use tools, and to make decisions based on\\nthe given input. They are designed to handle tasks that require\\na degree of autonomy and decision-making, typically beyond\\nsimple response generation.\\nThe functionalities of a generic LLM-based agent include:\\n•\\nTool Access and Utilization: Agents have the capabil-\\nity to access external tools and services, and to utilize\\nthese resources effectively to accomplish tasks.\\n•\\nDecision Making: They can make decisions based on\\nthe input, context, and the tools available to them,\\noften employing complex reasoning processes.\\nAs an example, an LLM that has access to a function (or\\nan API) such as weather API, can answer any question related\\nto the weather of the specific place. In other words, it can use\\nAPIs to solve problems. Furthermore, if that LLM has access\\nto an API that allows to make purchases, a purchasing agent\\ncan be built to not only have capabilities to read information\\nfrom the external world, but also act on it [171].\\nFig. 40 shows another example of LLM-based agents for\\nconversational information seeking [36], where an LLM is\\naugmented with a set of plug-and-play modules, including\\na working memory that tracks the dialog state, a policy that\\nmakes an execution plan for the task and selects next system\\naction, an action executor that performs an action selected by\\nthe policy (consolidating evidence from external knowledge,\\nor prompting the LLM to generate responses), and a utility\\nthat accesses the alignment of the LLM’s responses with user\\nexpectations or specific business requirements, and generate\\nfeedback to improve agent performance.\\nFor more details on LLM-based AI agents see recent survey\\n[172], [173], [174].\\na) Prompt engineering techniques for agents:\\nLike\\nRAG and Tools, prompt engineering techniques that specif-\\nically address the needs of LLM-based agents have been\\ndeveloped. Three such examples are Reasoning without Ob-\\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\\nEnabled Resolving Agents (DERA).\\nReasoning without Observation (ReWOO) [175] aims to\\ndecouple reasoning from direct observations. ReWOO operates\\nby enabling LLMs to formulate comprehensive reasoning plans\\nor meta-plans without immediate reliance on external data\\nor tools. This approach allows the agent to create a struc-\\ntured framework for reasoning that can be executed once the\\nnecessary data or observations are available. In ReWOO, the\\nLLM initially develops a plan (a series of steps) that outlines\\nhow to approach and solve a given problem. This meta-\\nplanning phase is crucial as it sets the stage for the agent to\\nprocess information once it becomes available. The execution\\nphase then involves integrating actual data or observations into\\nthe pre-specified plan, leading to coherent and contextually\\nrelevant responses. ReWOO offers significant advantages in\\nterms of token efficiency and robustness to tool failure. It\\nenables LLMs to handle tasks where immediate access to\\nexternal data is not available, relying instead on a well-\\nstructured reasoning framework. This method is particularly\\nadvantageous in scenarios where data retrieval is costly, slow,\\nor uncertain, allowing the LLM-based agent to maintain a high\\nlevel of performance and reliability.\\nReason and Act (ReAct)[176] prompts LLMs to generate\\nnot only verbal reasoning but also actionable steps, thus\\nenhancing the model’s dynamic problem-solving capabilities.\\nReAct is grounded in the principle of integrating reasoning\\nwith action. In this approach, the LLM is prompted to alternate\\nbetween generating reasoning traces (explanations) and taking\\nactions (steps or commands) in an interleaved manner. This\\napproach allows the model to dynamically reason about a prob-\\nlem, and propose and take concrete actions simultaneously.\\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\\ncialized AI agents that can engage in dialogue, resolve queries,\\nand make decisions based on interactive exchanges. DERA\\nis developed based on the idea of utilizing multiple agents\\nwithin a dialog context, each with specific roles and functions.\\nThese agents can include Researchers, who gather and analyze\\ninformation, and Deciders, who make final judgments based\\non the information provided. This division of roles allows for\\na well-organized and efficient approach to problem-solving\\nand decision-making. DERA is particularly advantageous in\\nscenarios requiring complex decision-making and problem-\\nsolving, such as those in medical diagnostics or customer ser-\\nvice. The collaborative and interactive nature of DERA agents\\nallows them to handle intricate queries with a level of depth\\nand nuance that single-agent systems might struggle with.\\nMoreover, this approach aligns well with human decision-\\nmaking processes, making AI reasoning more relatable and\\ntrustworthy.\\nV.\\nPOPULAR DATASETS FOR LLMS\\nLarge language models exhibit promising accomplish-\\nments, but the main question that arises is how effectively\\nthey function and how their performance can be assessed in\\nspecific tasks or applications.\\nThe evaluation of LLMs poses particular challenges due\\nto the evolving landscape of their applications. The original\\nintent behind developing LLMs was to boost the performance\\nof NLP tasks such as translation, summarization, question-\\nanswering, and so on [178]. However, it is evident today\\nthat these models are finding utility across diverse domains\\nincluding code generation and finance. Moreover, the eval-\\nuation of LLMs encompasses several critical considerations\\nsuch as fairness and bias, fact-checking, and reasoning. In\\nthis section, we outline the commonly used benchmarks for\\nassessing LLMs. These benchmarks are categorized based on\\ntraining or evaluating the LLM Capabilities.\\nA. Datasets\\nfor\\nBasic\\nTasks:\\nlanguage\\nmodel-\\ning/understanding/generation\\nThis section provides an overview of the benchmarks and\\ndatasets suited to evaluate the basic abilities of LLMs.\\n•\\nNatural Questions [179] is a QA dataset that consists\\nof real anonymized, aggregated queries submitted to\\nthe Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5 search results, and annotates a\\nlong answer (typically a paragraph) and a short answer\\nFig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\\nFig. 40: A LLM-based agent for conversational information\\nseeking. Courtesy of [36].\\n(one or more entities) if present on the page, or marks\\nnull if no long/short answer is present.\\n•\\nMMLU [180] is intended to evaluate the knowl-\\nedge gained in zero-shot and few-shot scenarios. That\\nmeans that MMLU assesses both the general knowl-\\nedge and problem-solving ability of a model. It covers\\n57 subjects in STEM, humanities, social sciences,\\nand other areas. The benchmark varies in complexity,\\nranging from elementary to advanced professional.\\nIt is worth mentioning that the main contribution of\\nthis dataset is for multi-task language understanding,\\nquestion answering, and arithmetic reasoning.\\n•\\nMBPP [181] stands for “Mostly Basic Python Prob-\\nlems” and provides a benchmark for evaluating the\\nperformance of models designed for code generation.\\nThe benchmark encompasses 974 short Python pro-\\ngrams including a wide range of topics, including\\nfundamental programming concepts and standard li-\\nbrary usage, and more. Each challenge comprises a\\ntask description, a code solution, and three automated\\ntest cases.\\n•\\nHumanEval [182] is a dataset for code generation\\ntask. This dataset consists of 164 hand-crafted pro-\\ngramming challenges. Each challenge is accompanied\\nby a function signature, docstring, code body, and mul-\\ntiple unit tests. The main intuition behind developing\\nthis dataset is to guarantee the exclusion of its contents\\nfrom training datasets for code generation models.\\n•\\nAPPS [183] is designed for code generation task\\nfocusing on the Python programming language. The\\nAPPS dataset contains a collection of 232, 444 Python\\nprograms. Each program in the dataset has an average\\nof 18 lines of Python code. Additionally, APPS offers\\naccess to a repository of 10, 000 unique programming\\nexercises, each with text-based problem descriptions.\\nThe final aspect to highlight is that the it includes test\\ncases.\\n•\\nWikiSQL [184] is crafted for code generation task and\\nit has 87,726 carefully labeled pairs of SQL queries\\nand corresponding natural language questions from\\nWikipedia tables. The SQL queries comprise three\\nsubsets: test sets (17, 284 examples), development\\n(9, 145 examples), and training (61, 297 examples).\\n•\\nTriviaQA [185] is designed for QA task. This\\ndataset\\ncomprises\\nmore\\nthan\\n650, 000\\nquestion-\\nanswer-evidence triples. There are 95, 000 question-\\nanswer pairs in this dataset, each authored by trivia en-\\nthusiasts and supported by an average of six indepen-\\ndently sourced evidence documents. These documents\\nare automatically acquired from Wikipedia or broader\\nweb search results. The dataset is categorized into\\ntwo segments, including those with authentic answers\\nfrom Wikipedia and web domains, and verified sets\\nembody the accurately answered questions along with\\ntheir associated documents from both Wikipedia and\\nonline.\\nFig. 41: Dataset applications.\\n•\\nRACE [186] suits for reading comprehension task.\\nThis dataset is based on English tests completed by\\nChinese students from middle school and high school,\\naged 12 to 18, and it contains roughly 28, 000 texts\\nand 100, 000 questions rigorously prepared by human\\nspecialists, primarily English instructors. This dataset\\ncontains a wide range of subjects that were purpose-\\nfully chosen to assess students’ comprehension and\\nreasoning abilities. This dataset is available in three\\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\\nM refers to the middle school examinations, whereas\\nRACE-H denotes the high school tests. Finally, RACE\\nis the synthesis of RACE-M and RACE-H.\\n•\\nSQuAD [187] stands for “Stanford Question Answer-\\ning Dataset” and is a crowdsourced reading compre-\\nhension dataset based on Wikipedia articles. It has\\napproximately 100, 000 question-answer pairs con-\\nnected to more than 500 articles. The answers to\\nthese questions are typically text fragments or spans\\ntaken from the corresponding reading passages. The\\nquestions may be unanswerable in some cases. The\\ndataset is divided into three sets: an 80% training set,\\na 10% development set, and a 10% hidden test set.\\nFig. 42: Datasets licensed under different licenses.\\n•\\nBoolQ [188] is a yes/no question-answering dataset\\nwhere the goal is reading comprehension task. BoolQ\\nincludes 15, 942 examples. Each example is a triplet\\nthat includes a question, a relevant paragraph, and\\nthe solution. Although the main intuition behind\\nthis dataset is for reading comprehension, it can be\\nused for reasoning, natural language inference, and\\nquestion-answering tasks.\\n•\\nMultiRC [189] is another dataset that fits reading\\ncomprehension task. MultiRC contains brief para-\\ngraphs as well as multi-sentence questions that can\\nbe answered using the information in the paragraph.\\nThe paragraphs in this dataset come from a variety\\nof sources, including news, fiction, historical texts,\\nWikipedia articles, discussions on society and law,\\nelementary school science textbooks, and 9/11 re-\\nports. Each question has many response choices, with\\none or more of them being correct. Answering the\\nquestions requires reasoning across several sentences.\\nMultiRC dataset encompasses around 6, 000 multi-\\nsentence questions gathered from over 800 paragraphs.\\nOn average, each question offers about two valid\\nanswer alternatives out of a total of five.\\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\\nfollowing\\nThis section centers on the benchmarks and datasets em-\\nployed to evaluate the emergent abilities of LLMs.\\n•\\nGSM8K [190] is designed to evaluate the model’s\\nability for multi-step mathematical reasoning. GSM8K\\nincludes 8.5K linguistically diverse grade school math\\nword problems written by humans. The dataset is split\\ninto two sets: a training set with 7.5K problems,\\nand a test set with 1K problems. These problems\\nneed 2 to 8 steps to be solved. Solutions mainly\\nare a series of elementary calculations using basic\\narithmetic operations.\\n•\\nMATH [191] enables to assess how well models can\\nsolve math problems. MATH dataset hast 12, 500\\nproblems from high school math competitions. Each\\nproblem in the dataset has a step-by-step solution and\\na final answer enclosed in a box. The problems cover\\na wide range of topics and have different levels of\\ncomplexity. There are seven subjects in total. Further-\\nmore, the difficulty of each problem is rated based\\non the AoPS standards on a scale from ′1′ to ′5′. A\\n′1′ shows the easiest problems in a subject, while ′5′\\nrepresents the most difficult. In terms of formatting,\\nall problems and solutions are presented using LATEX\\nand the Asymptote vector graphics language.\\n•\\nHellaSwag [192] is designed to assess commonsense\\nreasoning in LLMs. This benchmark includes 70, 000\\nmultiple-choice questions. Each question is derived\\nfrom one of two domains: ActivityNet or WikiHow,\\nand presents four answer choices regarding what\\nmight happen in the following situation. The correct\\nanswer provides an actual statement describing the\\nupcoming event, but the three wrong answers are\\ncreated to confuse machines.\\n•\\nAI2 Reasoning Challenge (ARC) [193] is used\\nfor commonsense reasoning. This benchmark encom-\\npasses 7, 787 science examination questions. These\\nquestions are in English, and most of them are set\\nup in a multiple-choice format. The questions have\\nbeen divided into two groups: a Challenge Set with\\n2, 590 difficult questions and an Easy Set with 5,197\\nquestions. Each collection has also been pre-divided\\ninto Train, Development, and Test subsets.\\n•\\nPIQA [194] is intended to evaluate the language\\nrepresentations on their knowledge of physical com-\\nmonsense. In this dataset, the focus is on everyday\\nsituations with a preference for uncommon solutions.\\nThe central task is a multiple-choice question answer-\\ning, where a question (q) is provided along with two\\npotential solutions (s1, s2). Then, the best solution is\\nchosen by whether a model or a human. For each\\nquestion, only one of the solutions is the correct\\nanswer.\\n•\\nSIQA [195] provides a framework for evaluating mod-\\nels’ ability for commonsense reasoning about social\\nsituations. SIQA dataset has 38, 000 multiple-choice\\nquestions designed to assess emotional and social\\nintelligence in everyday circumstances. This dataset\\ncovers a wide variety of social scenarios. In SIQA,\\nthe potential answers is a mixture of human-selected\\nresponses and machine-generated ones that have been\\nfiltered through adversarial processes.\\n•\\nOpenBookQA (OBQA) [196] is a new kind of\\nquestion-answering dataset where answering its ques-\\ntions requires additional common and commonsense\\nknowledge not contained in the book and rich text\\ncomprehension. This dataset includes around 6,000\\nmultiple-choice questions. Each question is linked to\\none core fact, as well as an additional collection\\nof over 6000 facts. The questions were developed\\nusing a multi-stage crowdsourcing and expert filter-\\ning procedure. OpenBookQA questions are difficult\\nbecause they need multi-hop reasoning with limited\\nbackground.\\n•\\nTruthfulQA [197] is designed specifically to eval-\\nuate the truthfulness of language models in gen-\\nerating answers to questions. This dataset includes\\n817 questions, written by authors, from 38 different\\ncategories, including health, law, finance, and politics.\\nThese questions are purposefully designed to chal-\\nlenge human responders, as they may contain common\\nmisunderstandings that lead to incorrect answers.\\n•\\nOPT-IML Bench [103] is a comprehensive bench-\\nmark for Instruction Meta-Learning. It covers 2000\\nNLP tasks from 8 existing benchmarks. The OPT-IML\\nBench consists of a training set with 17.9 M examples,\\na dev set with 145K samples, and a test set with 321K\\nsamples.\\nC. Datasets for Augmented: using external knowledge/tools\\nThis section focuses on datasets designed for the aug-\\nmented abilities of LLMs.\\n•\\nHotpotQA [198] is designed to cover a diverse and\\nexplainable question-answering dataset that necessi-\\ntates multi-hop reasoning. This dataset is derived from\\nthe English Wikipedia. It consists of roughly 113, 000\\nquestions. Each question in the dataset comes with\\ntwo paragraphs, called gold paragraphs, from two\\nWikipedia articles. Also, there is a list of sentences\\nin those paragraphs that crowdworkers have picked as\\nimportant for answering the question.\\n•\\nToolQA [199] is a question answering benchmark\\nto evaluate LLMs’ ability to use external tools for\\nanswering questions.\\n•\\nGPT4Tools serves as an instructional dataset, gener-\\nated by instructing advanced teachers (such as Chat-\\nGPT), with instructions conditioned on visual content\\nand tool descriptions. This process results in the\\ngeneration of instructions related to the use of tools.\\nThere are three versions of this dataset. The first\\nversion comprises 71,000 instruction-following data\\npoints utilized to fine-tune the GPT4Tools model. The\\nnext version consists of manually cleaned instruction\\ndata used for validation, covering instructions related\\nto the tools from the first version. The last version is\\ncleaned instruction data used for testing and includes\\ninstructions related to some tools that are not present\\nin the first version.\\nVI.\\nPROMINENT LLMS’ PERFORMANCE ON\\nBENCHMARKS\\nIn this section we first provide an overview of some of\\npopular metrics used for evaluating the performance of LLMs\\nunder different scenarios. We then look at the performance\\nof prominent large language models on some of the popular\\ndatasets and benchmarks.\\nA. Popular Metrics for Evaluating LLMs\\nEvaluating the performance of generative language models\\ndepends on the underlying task they are going to be used for.\\nTasks that are mostly about selecting a choice out of given\\nones (such as sentiment analysis), can be seen as simple as\\nclassification and their performance can be evaluated using\\nclassification metrics. Metrics such as accuracy, precision,\\nrecall, F1, etc are applicable in this case. It is also important to\\nnote that the answers generated by the model for specific tasks\\nsuch as multi-choice question answering are always either True\\nor False. If the answer is not in a set of options, it can be seen\\nas False as well.\\nHowever, some tasks that are purely open-ended text gener-\\nation cannot be evaluated in the same way as for categorization.\\nDifferent metrics are required for the specific purpose of the\\nevaluation. Code generation is a very different case in open-\\nended generative evaluations. The generated code must pass\\nthe test suite but on the other hand, it is also important\\nto understand if a model is capable of generating different\\nTABLE II: LLM Datasets Overview.\\nBenchmark Name\\nEvaluation Metric\\nLeaderboard\\nSource\\npaperswithcode\\nHumanEval\\nPASS@k\\nLink\\nLink\\nLink\\nMBPP\\nPASS@k, Accuracy\\n-\\nLink\\nLink\\nAPPS\\nPASS@k, Accuracy\\n-\\nLink\\nLink\\nWikiSQL\\nAccuracy\\n-\\nLink\\nLink\\nCoNaLa\\nBLEU\\nLink\\nLink\\nCodeParrot\\nPASS@k\\n-\\nLink\\n-\\nHellaSwag\\nAccuracy\\nLink\\nLink\\nLink\\nAI2\\nReasoning\\nChallenge (ARC)\\nAccuracy\\nLink\\nLink\\nLink\\nBoolQ\\nAccuracy\\n-\\nLink\\nLink\\nMultiRC\\nF1-score, Accuracy\\n-\\nLink\\nLink\\nCNN/Daily Mail [200]\\nAccuracy\\n-\\nLink\\n-\\nSQuAD\\nF1-score, EM\\nLink\\nLink\\nLink\\nRACE\\nAccuracy\\n-\\nLink\\nLink\\nCNN/Daily Mail [201]\\nROUGE\\n-\\nLink\\nLink\\nDrop\\nF1-score, EM\\nLink\\nLink\\nLink\\nQuAC\\nF1-score, HEQ-Q, HEQ-D\\nLink\\nLink\\nLink\\nTriviaQA\\nEM, F1-score, Accuracy\\nLink\\nLink\\nLink\\nNatural Questions\\nEM, F1-score, Accuracy\\nLink\\nLink\\nLink\\nStrategyQA\\nAccuracy, Recall@10, SARI\\nLink\\nLink\\nLink\\nCoQA\\nF1-score\\nLink\\nLink\\nLink\\nXSum\\nROUGE\\n-\\nLink\\nLink\\nSAMSum\\nROUGE\\n-\\n-\\nLink\\nWikiSum\\nROUGE\\n-\\nLink\\n-\\nDialogSum\\nROUGE\\n-\\nLink\\nLink\\nTruthfulQA\\nMC1 , MC2, % true, % info, BLEURT\\nLink\\nLink\\nLink\\nMMLU\\nAccuracy\\nLink\\nLink\\nLink\\nGSM8K\\nAccuracy\\nLink\\nLink\\nLink\\nPIQA\\nAccuracy\\nLink\\nLink\\nLink\\nSIQA\\nAccuracy\\nLink\\nLink\\nLink\\nOpenBookQA (OBQA)\\nAccuracy\\nLink\\nLink\\nLink\\nHotpotQA\\nEM, F1-score, Joint EM, Joint F1-score,\\nLink\\nLink\\nLink\\nMATH\\nAccuracy\\n-\\nLink\\nLink\\nCommonsenseQA\\nAccuracy\\nLink\\nLink\\nLink\\nNatural Instructions\\nROUGE-L, Human\\nLink\\nLink\\nLink\\nBIG-bench\\nAccuracy, Average\\n-\\nLink\\nLink\\nToolTalk\\nSuccess rate, Precision, Recall, Incorrect\\naction rate, Percent of failing error types\\n-\\nLink\\nLink\\nMetaTool\\nAccuracy, Precision, Recall, F1-score\\n-\\nLink\\nLink\\nGPT4Tools\\nSuccessful Rate of Thought, Successful\\nRate of Action, Successful Rate of Ar-\\nguments, Success Rate\\n-\\nLink\\nLink\\nAPI-Bank\\nCorrectness, ROUGE, Error(API Hallu-\\ncination, Has Exception, Invalid Input\\nParameters, False API Call Format, API\\nCall, Miss Input Parameters)\\n-\\nLink\\nLink\\nAlpaca-CoT\\n-\\n-\\nLink\\nLink\\nsolutions as a code, what is the probability of selecting the\\ncorrect one among them. Pass@k is a very good metric in this\\ncase. It works in this manner that given a problem, different\\nsolutions as code are generated. They are tested for correctness\\nusing different functionality tests. Afterward, from generated\\nn solutions, and the respective c number of them being correct\\nequation 4 provides the final value.\\npass@k :=\\nE\\nProblems\\n\"\\n1 −\\n\\x00n−c\\nk\\n\\x01\\n\\x00n\\nk\\n\\x01\\n#\\n(4)\\nExact match (EM) is another metric that is mostly con-\\ncerned with exact matches from (pre-defined) answers. It\\ncounts a prediction as correct if it exactly matches one of\\nmore than one desired reference text token by token. In some\\ncases, it can be the same as accuracy and the equation 5 shows\\nthe mathematical definition. Here M is total number of correct\\nanswers and N is the total number of questions [202].\\nEM = M\\nN\\n(5)\\nHuman equivalence score (HEQ) on the other hand, is an\\nalternative to F1 score [203]. HEQ-Q represents the precision\\nof individual questions, wherein an answer is deemed correct\\nif the model’s F1 score surpasses the average human F1 score.\\nLikewise, HEQ-D denotes the precision of each dialogue; it is\\ndeemed accurate when all questions within the dialogue meet\\nthe criteria of HEQ [182].\\nEvaluation of other generative tasks such as machine trans-\\nlation are based on metrics such as Rouge and BLEU. These\\nscores work well when there is a reference text as ground\\ntruth (such as translation) and a hypothesis that is generated\\nby the generative model, in our case the LLM. These scores\\nare mostly used for cases where the goal is to detect the\\nsimilarity of the answer and ground truth in a computation\\nmanner. In a computation manner, it meant that nothing more\\nthan N-Grams would be used. However, metrics such as BERT-\\nScore are also good for these cases but they are also heavily\\nTABLE III: LLM categories and respective definitions.\\nClassification\\nCategory\\nDescription\\nSize\\nSmall\\nNumber of parameters ≤1B\\nMedium\\n1B < Number of parameters ≤10B\\nLarge\\n10B < Number of parameters ≤100B\\nVery Large\\n100B < Number of parameters\\nType\\nFoundation model\\nPretrained language model\\nInstruction model\\nPretrained and instruction fine-tuned language model\\nChat model\\nPretrained, instruction fine-tuned, and chat fine-tuned language model\\nOrigin\\nOriginal model\\nAn original model released with either Foundation, Instruction, or Chat model\\nTuned model\\nFine-tuned version of an original model\\nAvailability\\nPublicly available\\nModel and weights are available due to request to without request\\nPublicly unavailable\\nModel and weights are not publicly available\\nTABLE IV: Different LLM categorization.\\nModel\\nSize\\n#Params (B)\\nType\\nAvailability\\nOrigin\\nDavinci-002\\nVery Large\\n175\\nInstruction\\nUnavailable\\nTuned\\nDavinci-003\\nVery Large\\n175\\nInstruction\\nUnavailable\\nTuned\\nGPT 3.5-turbo\\nLarge\\n20\\nChat\\nUnavailable\\nTuned\\nFalcon 7B\\nMedium\\n7\\nFoundation\\nPublic\\nOriginal\\nAlpaca\\nLarge\\n13\\nChat\\nPublic\\nTuned\\nPythia 7B\\nMedium\\n7\\nFoundation\\nPublic\\nOriginal\\nPythia 12B\\nLarge\\n12\\nFoundation\\nPublic\\nOriginal\\nLLAMA 7B\\nMedium\\n7\\nChat\\nPublic\\nOriginal\\nLLAMA 2 7B\\nMedium\\n7\\nChat\\nPublic\\nTuned\\nLLAMA 2 7B\\nMedium\\n7\\nFoundation\\nPublic\\nOriginal\\nVicuna 13B\\nLarge\\n13\\nFoundation\\nPublic\\nTuned\\nVicuna 7B\\nMedium\\n7\\nFoundation\\nPublic\\nTuned\\nClaude\\nLarge\\n93\\nChat\\nUnavailable\\nOriginal\\nClaude 2\\nVery Large\\n137\\nChat\\nUnavailable\\nOriginal\\nerroneous because another model is used to judge. Still, even\\ntoday, evaluating purely generated content is very hard and\\nno completely fitting metric is not found, metrics are either\\nlooking for simplistic features such as N-Gram, SkipGram,\\netc, or they are models with unknown accuracy and preciseness\\n[204].\\nGenerative evaluation metrics are also another type of eval-\\nuation metric for LLMs that use another LLM for evaluating\\nthe answer. However, depending on the task itself, evaluation\\ncan be possible in this way or not. Another dependency\\nthat makes generative evaluation error-prone is reliance on\\nthe prompt itself. RAGAS is one of the good examples that\\nincorporate the usage of generative evaluation.\\nVarious benchmarks and leaderboards have been proposed\\nto address the most challenging question in the world of\\nlarge language models: Which one is better? However not\\na simple answer can address this question. The answer de-\\npends on various aspects of large language models. Section V\\nshows the categorical presentation of different tasks and the\\nmost important datasets in each category. We will follow the\\nsame categorization and provide a comparison based on each\\ncategory. After providing comparison for each category, we\\nwill provide a broad overview of aggregated performance by\\naveraging the reported performance metric on different tasks.\\nEvaluating different LLMs can be seen also from different\\nperspectives. For example, a LLM with a drastically fewer\\nnumber of parameters is not completely comparable to one\\nwith a larger number of parameters. From this perspective, we\\nwill categorize LLMs in four categories as well: small (less\\nthan or equal to 1 billion parameters), medium (between 1 and\\n10 billion), large (between 10 and 100 billion), and very large\\n(more than 100 billion). Another classification for the LLMs\\nwe use is their primary use case. We consider each LLM to\\nbe either: Foundation model (pretrained language model with\\nno instruction fine-tuning and chat fine-tuning), Instruction\\nmodel (pretrained language model with only instruction fine-\\ntuning), and Chat model (pretrained language model with\\ninstruction and chat fine-tuning). Apart from all the catego-\\nrization described, another category is required to distinguish\\nbetween original models and tuned ones. Original models are\\nthose that have been released as a foundation model or a fine-\\ntuned one. Tuned models are those that grasped the original\\nmodel and tuned it with different datasets or even different\\ntraining approaches. It is also good to note that original models\\nare usually foundation models that have been fine-tuned on\\nspecific datasets or even different approaches. Availability of\\nthe model weights regardless of the license is another category\\nin our classification. Models that have their weights publicly\\navailable (even through request) are noted as Public models\\nwhile others are noted as Private. Table III shows all of these\\ndefinitions and abbreviations used in the rest of the article.\\nFigure 43 illustrate these visually.\\nAccording to the provided categorizations, we can catego-\\nrize and label each notable LLM as shown in table IV. As can\\nbe seen from this table, models categorized as very large are\\nalso unavailable as well.\\nB. LLMs’ Performance on Different Tasks\\nCommonsense reasoning is one of the important capabili-\\nties each model can obtain. This capability denotes the ability\\nof the model to use prior knowledge in combination with\\nreasoning skills. In the case of HellaSwag for example, finding\\nthe continuation of text is challenging because the given text\\ncontains a partial part of the story while the given choices\\nas continuation are tricky to select, and without having prior\\nLarge\\nLanguage\\nModels\\nParameters\\nAvailability\\nOriginality\\nType\\nSmall LM\\n# of params <1B\\nMedium LM\\n1B < # of params <10B\\nLarge LM\\n10B < # of params <100B\\nVery Large LM\\n100B < # of params\\nTuned\\nFine tuning\\nOriginal\\nPublic\\nPrivate\\nFoundation\\nInstruction\\nChat\\nFine tuned models that are originally\\nbased on original models.\\nExample: Alpaca (based on LLaMA)\\nOriginal models that are not fine\\ntuned or based on any other\\npretrained model.\\nExample: LLaMA\\nModel weights are publicly released\\nand is available.\\nExample: LLaMA\\nModel weights are NOT publicly\\nreleased and is NOT available.\\nExample: GPT-4\\nPretrained model with no instruction\\nor chat fine-tuning.\\nExample: MPT-7B\\nPretrained model that is\\nalso fine-tuned on\\ninstruction following.\\nExample: MPT-7B-instruct\\nPretrained model that is\\nalso fine-tuned on chat.\\nExample: MPT-7B-chat\\nFig. 43: LLM categorizations.\\nknowledge about the world it is not possible. This specific kind\\nof reasoning deserves high attention because it is related to\\nutilizing previous knowledge with open text-described scenes\\nor facts. As can be seen from table V not just Unavailable\\nmodels but also Public ones can achieve good results on\\nvarious tests.\\nTABLE V: Commonsense reasoning comparison.\\nModel\\nOBQA\\nHellaSwag\\nDavinci-003\\n51\\n83.4\\nFalcon 7B\\n44.4\\n76.3\\nAlpaca\\n43.4\\n73.9\\nPythia 7B\\n37.2\\n64\\nPythia 12B\\n43.2\\n68.1\\nLLAMA 7B\\n42.4\\n73\\nDolly 6B\\n41.2\\n67.6\\nDolly 12B\\n40.4\\n71\\nAlpaca 7B\\n43.4\\n73.9\\nAlpaca Lora 7B\\n42.6\\n74\\nGPT-J 6.7B\\n38.2\\n66.2\\nLLama 7B\\n42.4\\n73\\nLLama 13B\\n42.2\\n76.2\\nPythia 6.7B\\n37.2\\n64\\nPythia 12B\\n38\\n67.3\\nStableLM Tuned\\n33.4\\n53.6\\nKoala 13B\\n42.8\\n72.6\\nMosaic mpt-7B\\n42.6\\n76.3\\nLLAMA 2 70B\\n-\\n87.33\\nLLAMA 65B\\n-\\n86.09\\nFalcon 40B\\n-\\n85.3\\nFalcon 180B\\n-\\n88.86\\nMPT Instruct 30B\\n-\\n84.31\\nMPT Instruct 7B\\n-\\n77.91\\nYi 6B\\n-\\n76.42\\nYi 34B\\n-\\n85.69\\nGPT-4\\n-\\n95.3\\nGemini Ultra\\n-\\n87.8\\nFrom the results presented in Table V it is clear that GPT-4\\nachieves best results for HellaSwag while Davinci-003 is best\\nmodel for OBQA. It is also good to note that results for OBQA\\nare not reported for all of the models and possibly davinci-003\\nis not the best model achieving highest results on OBQA.\\nNot all models report their performance on all datasets, and\\nbecause of that, the number of models for which performance\\nis reported in different tables varies.\\nTABLE VI: Symbolic reasoning comparison.\\nModel\\nCobjects\\nPenguins\\nGPT-NeoX\\n26\\n33.56\\nOPT 66B\\n31.2\\n28.08\\nBloomberg GPT\\n34.8\\n37.67\\nBLOOM 176B\\n36.8\\n40.41\\nPaLM 540B\\n38\\n44.5\\nGopher-280B\\n49.2\\n40.6\\nChinchilla-70B\\n59.7\\n48.7\\nPaLM 2\\n61.2\\n65.8\\nWorld knowledge is mostly about general knowledge ques-\\ntions, for example, in Wikifact dataset questions such as ”Who\\nis the author of a specific well-known book” can be found and\\nreferences are also provided. Table VII shows the results.\\nTABLE VII: World knowledge comparison.\\nModel\\nTriviaQA\\nNaturalQ\\nWebQ\\nARC\\nBLOOM\\n-\\n-\\n-\\n32.9\\nBLOOM 176B\\n-\\n-\\n-\\n50.85\\nBloomberg GPT\\n-\\n-\\n-\\n48.63\\nChinchilla\\n-\\n35.5\\n-\\n-\\nCodex + REPLUG\\n76.8\\n44.7\\n-\\n-\\nGAL 120B\\n-\\n-\\n-\\n67.9\\nGLaM 62B/64E\\n75.8\\n32.5\\n15.5\\n50.3\\nGopher\\n-\\n28.2\\n-\\n-\\nGPT-3 175B\\n71.2\\n29.9\\n41.5\\n85.2\\nGPT-4\\n-\\n-\\n-\\n96.4\\nGPT-NeoX\\n-\\n-\\n-\\n45.39\\nLLaMA 13B\\n-\\n-\\n-\\n52.7\\nLLaMA 2 70B\\n85\\n33\\n-\\n-\\nLLaMA 33B\\n-\\n24.9\\n-\\n57.8\\nLLaMA 65B\\n72.6\\n39.9\\n-\\n-\\nLLaMA 7B\\n-\\n-\\n-\\n47.6\\nMistral 7B\\n69.9\\n28.8\\n-\\n55.5\\nNeo-6B\\n-\\n13.7\\n-\\n-\\nOPT\\n-\\n-\\n-\\n31.1\\nOPT 66B\\n-\\n-\\n-\\n44.54\\nOPT-175B\\n-\\n-\\n-\\n43.94\\nOPT-175B\\n-\\n-\\n-\\n25.6\\nPaLM 2-L\\n86.1\\n37.5\\n28.2\\n95.1\\nPaLM 2-M\\n81.7\\n32\\n26.9\\n64.9\\nPaLM 2-S\\n75.2\\n25.3\\n21.8\\n59.6\\nPaLM-540B\\n81.4\\n39.6\\n43.5\\n87.1\\nphi-1.5-web 1.3B\\n-\\n-\\n-\\n44.9\\nSparseGPT\\n-\\n-\\n-\\n38.99\\nSparseGPT\\n-\\n-\\n-\\n39.85\\nSparseGPT\\n-\\n-\\n-\\n41.3\\nFor some specific use-case models, it is highly demanded to\\nhave coding and code-generation capability. Table VIII shows\\nthe results of different models on coding capability.\\nTABLE VIII: Coding capability comparison.\\nModel\\nHumanEval\\nGemini Ultra\\n74.4\\nGemini Pro\\n67.7\\nGPT-4\\n67\\nWizardCoder 15B\\n57.3\\nphi-1 1.3B\\n50.6\\nCode Llama\\n48.8\\nGPT-3.5\\n48.1\\nOctoCoder\\n46.2\\nphi-1-small\\n45\\nPaLM 2-S\\n37.6\\nInstructCodeT5+ 16B\\n35\\nMistral 7B\\n30.5\\nLLaMA 2\\n29.9\\nphi-1-base\\n29\\nCodex-12B\\n28.81\\nPaLM 540B\\n26.2\\nCodeT5+ 2B\\n24.2\\nLLaMA 65B\\n23.7\\nLLaMA 33B\\n21.7\\nPaLM 62B\\n15.9\\nLLaMA 13B\\n15.8\\nLaMDA 137B\\n14\\nMIM-350M\\n13.7\\nLLaMA 7B\\n10.5\\nPaLM 8B\\n3.6\\nArithmetic reasoning is another challenging reasoning ca-\\npability to achieve. GSM8K for example contains grade school\\nmathematical questions with respect to their answers. Table IX\\nprovides an insight for different model comparisons.\\nTABLE IX: Arithmetic reasoning comparison.\\nModel\\nGSM8k\\nMATH\\nGemini Ultra\\n94.4\\n53.2\\nGPT-4\\n87.1\\n42.5\\nGemini Pro\\n86.5\\n32.6\\nToRA 70B\\n84.3\\n49.7\\nMathCoder-L-70B\\n83.9\\n-\\nMetaMath 70B\\n82.3\\n26\\nMuggleMATH 70B\\n82.3\\n-\\nMathCoder-CL-34B\\n81.7\\n45.2\\nToRA-Code 34B\\n80.7\\n50.8\\nMetaMath-Mistral-7B\\n77.7\\n-\\nArithmo2-Mistral-7B\\n76.4\\n-\\nToRA-Code 13B\\n75.8\\n48.1\\nArithmo-Mistral-7B\\n74.7\\n-\\nMathCoder-CL-13B\\n74.1\\n35.9\\nMuggleMATH 13B\\n74\\n-\\nCodeT5+\\n73.8\\n-\\nKwaiYiiMath 13B\\n73.3\\n-\\nToRA-Code 7B\\n72.6\\n44.6\\nMathCoder-L-13B\\n72.6\\n29.9\\nMetaMath 13B\\n71\\n22.5\\nLLaMA 65B\\n69.7\\n10.6\\nMuggleMATH 7B\\n68.4\\n-\\nMathCoder-CL-7B\\n67.8\\n23.3\\nMetaMath 7B\\n66.4\\n19.4\\nRFT 70B\\n64.8\\n-\\nMathCoder-L-7B\\n64.2\\n-\\nOrca 2-13B\\n59.14\\n-\\nU-PaLM\\n58.5\\n-\\nPaLM-540B\\n58.1\\n8.8\\nLLaMA 2 70B\\n56.8\\n-\\nRFT 13B\\n55.3\\n-\\nLLaMA 33B\\n53.1\\n7.1\\nMistral 7B\\n52.2\\n13.1\\nRFT 7B\\n51.2\\n-\\nLLaMA 65B\\n50.9\\n20.5\\nOrca 2-7B\\n47.23\\n-\\nText-davinci-002\\n40.7\\n19.1\\nLLaMA 33B\\n35.6\\n3.9\\nGPT-Neo-2.7B\\n19.5\\n-\\nLLaMA 7B\\n18.1\\n2.9\\nPaLM 540B\\n17.9\\n8.8\\nLLaMA 13B\\n17.8\\n3.9\\nLLaMA 7B\\n11\\n2.9\\nGPT-Neo-125M\\n7.5\\n-\\nPaLM 8B\\n4.1\\n1.5\\nGPT-2\\n-\\n5.4\\nGPT-3 175B\\n-\\n5.2\\nPaLM 62B\\n-\\n4.4\\nGPT-3-13B\\n-\\n3\\nLLaMA 7B\\n11\\n2.9\\nPaLM 8B\\n-\\n1.5\\nLarge language models in some cases are hallucinating an-\\nswers simply because they are next-token prediction machines.\\nHallucination is one of the important factors in measuring\\nhow much a large language model is trustworthy and reliable.\\nMeasuring hallucination on the other hand is also not easy as it\\nseems because each fact can be written in different styles and\\neven the smallest changes in writing make it hard to detect.\\nIt is fair to assume if any particular LLM is more capable\\nto detect hallucination of false information in text, it is also\\nmore trustworthy. HaluEval is one of the datasets that aims to\\nmeasure hallucination in this field [205]. Evaluation can also be\\nperformed by another model judging the response with regard\\nto the actual answer [206]. Table X shows the evaluation of\\ndifferent models based on these datasets.\\nVII.\\nCHALLENGES AND FUTURE DIRECTIONS\\nAs we have seen in the previous sections, large language\\nmodels have achieved impressive results in the past 1-2 years.\\nTABLE X: Hallucination evaluation\\nModel\\nHHEM\\nHaluEval QA\\nHaluEval Dialogue\\nHaluEval Sum.\\nHaluEval General\\nGPT 4\\n97\\n-\\n-\\n-\\n-\\nGPT 4 Turbo\\n97\\n-\\n-\\n-\\n-\\nGPT 3.5 Turbo\\n96.5\\n62.59\\n72.4\\n58.53\\n79.44\\nDavinci002\\n-\\n60.05\\n60.81\\n47.77\\n80.42\\nDavinci003\\n-\\n49.65\\n68.37\\n48.07\\n80.4\\nGPT-3\\n-\\n49.21\\n50.02\\n51.23\\n72.72\\nGoogle Gemini Pro\\n95.2\\n-\\n-\\n-\\n-\\nLlama 2 70B\\n94.9\\n-\\n-\\n-\\n-\\nLlama 2 7B\\n94.4\\n49.6\\n43.99\\n49.55\\n20.46\\nLlama 2 13B\\n94.1\\n-\\n-\\n-\\n-\\nCohere-Chat\\n92.5\\n-\\n-\\n-\\n-\\nCohere\\n91.5\\n-\\n-\\n-\\n-\\nClaude 2\\n91.5\\n69.78\\n64.73\\n57.75\\n75\\nClaude 1\\n67.6\\n64.83\\n53.76\\n73.88\\nMicrosoft Phi 2\\n91.5\\n-\\n-\\n-\\n-\\nGoogle Palm 2 (beta)\\n91.4\\n-\\n-\\n-\\n-\\nMixtral 8x7B\\n90.7\\n-\\n-\\n-\\n-\\nAmazon Titan Express\\n90.6\\n-\\n-\\n-\\n-\\nMistral 7B\\n90.6\\n-\\n-\\n-\\n-\\nGoogle Palm 2 Chat (beta)\\n90\\n-\\n-\\n-\\n-\\nGoogle Palm 2\\n87.9\\n-\\n-\\n-\\n-\\nGoogle Palm 2 Chat\\n72.8\\n-\\n-\\n-\\n-\\nChatGLM\\n-\\n47.93\\n44.41\\n48.57\\n30.92\\nFalcon\\n-\\n39.66\\n29.08\\n42.71\\n18.98\\nVicuna\\n-\\n60.34\\n46.35\\n45.62\\n19.48\\nAlpaca\\n-\\n6.68\\n17.55\\n20.63\\n9.54\\nAt the same time this is still a new and extremely active\\nresearch area where the pace of innovation is increasing rather\\nthan slowing down. As in any other evolving area though, there\\nare still numerous challenges ahead. Here we briefly mention\\nsome of the challenges and main active areas which are known\\nso far. It is worth noting that LLM challenges are discussed\\nin details in a work by Kaddour et al. [207].\\nA. Smaller and more efficient Language Models\\nThis is a survey on large language models, and there\\nhas been an initial push towards ”larger is better” that has\\nclearly been rewarded with ever larger models like GPT-\\n4 getting better accuracy and performance in benchmarks.\\nHowever, those large models are costly and inefficient in\\nseveral dimensions (e.g. high latency). In response to all of\\nthis, there is a current research trend to come up with Small\\nLanguage Models (SLMs) as a cost-effective alternative to\\nLLMs, particularly when used on specific tasks that might not\\nrequire the full generality of larger models. Prominent works\\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\\nfrom Microsoft.\\nMore generally, we should expect many research efforts in\\nthis area of how to train smaller and more efficient models.\\nTechniques such as parameter-efficient fine-tuning (PEFT),\\nteacher/student, and other forms of distillation – see section\\nIII-I – will continue to be used to build a smaller model out\\nof larger ones.\\nB. New Post-attention Architectural Paradigms\\nTransformer blocks have been a crucial and constant part of\\nmost of current LLM frameworks, and it’s a big question mark\\nhow much longer this architecture will be in vogue, and what\\nwill be the next big architectural break-through in the field of\\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\\nmany architectures go in and out of fashion, including LSTM,\\nGRU, seq2seq, but Transformers have been the dominant\\napproach since its inception. As described earlier, attention is\\nthe main mechanism driving transformers. More recently, there\\nhas been promising research in alternative approaches that are\\nbeing labelled as post-attention.\\nAn important class of such class of post-attention models\\nare the so called State Space Models (SSMs). While the notion\\nof State Space Models has a long history in machine learning,\\nit should be noted that in the context of language models, SSM\\nis usually used in reference to the newer Structure State Space\\nModel architecture or S4 for short (see Gu et al. [29]). Some\\nrecent models in this category are Mamba [30], Hyena [210],\\nand Striped Hyena [211].\\nWhile all of those models are very competitive in terms of\\nperformance in leaderboards and efficiency, they also address\\nan important challenge in more traditional attention-based\\narchitectures: the lack of support for larger context windows.\\nHaving a good answer to many prompts requires context.\\nFor example, the response to ”Recommend some good movies\\nfor me” requires a lot of context about ”me” as well as what\\nmovies are available and which ones I have not watched.\\nContext length is especially important for RAG, where large\\nportions of text might be retrieved and injected into the prompt\\nfor generation (see section IV-C.\\nThe longer the context length, the more tokens we can\\nsqueeze into the context. The more information the model has\\naccess to, the better its response will be. But on the other\\nhand, with very long context, it would be hard for the model\\nto remember everything and efficiently process all the informa-\\ntion. Attention-based models are highly inefficient for longer\\ncontexts and that is why we should expect more research in\\ndifferent mechanisms that enable processing longer contexts\\nand generally come up with more efficient architectures.\\nThat being said, new architectures might not only propose\\nalternatives for the attention mechanism but rather rethink the\\nwhole Transformer architecture. As an early example of this,\\nMonarch Mixer [212] proposes a new architecture that uses\\nthe same sub-quadratic primitive that achieves high hardware\\nefficiency on GPUs – Monarch matrices – along both sequence\\nlength and model dimension.\\nOn the other end of the spectrum, it is worth mentioning\\nthat there are some attention-compatible architectural mecha-\\nnisms that have been recently gaining steam and proving their\\nvalue in creating better and more powerful LLMs. Probably\\nthe best example of such mechanism is Mixture of Experts\\n(MoE). MoEs have been around in machine learning for years,\\neven before the Deep Learning Era [213], but they have been\\ngaining popularity since then, and particularly in the context\\nof Transformer models and LLMs.\\nIn LLMs, MoEs allow to train an extremely large model\\nthan is then only partially instantiated during inference\\nwhen some of the experts are turned off wherever the gat-\\ning/weighting function has a low weight assigned to them. As\\nan example, the GLaM model has 1.2 trillion parameters, but\\nduring inference only 2 out of the 64 experts are used [84].\\nMoEs are nowadays an important component of the so-\\ncalled frontier LLMs (i.e. the most advanced and capable\\nmodels). GPT-4 itself is rumored to be based on a MoE\\narchitecture, and some of the best performing LLMs such as\\nMixtral [117], are basically an MoE version of pre-existing\\nLLMs.\\nFinally, it is important to note that MoEs can be used as a\\ncomponent of any architecture regardless of whether it is based\\non attention or not. In fact, MoEs have also been applied to\\nSSM-based LLMs like Mamba citepioro2024moemamba. We\\nshould continue to see MoE-driven improvements in the future\\nregardless of the underlying architecture.\\nC. Multi-modal Models\\nFuture LLMs are expected to be multi-modal and handle\\na variety of data types, such as text, images, and videos,\\naudio, in a unified manner. This opens up possibilities for\\nmore diverse applications in fields like question answering,\\ncontent generation, creative arts, and healthcare, robotics, and\\nbeyond. There are already several prominent multi-modal\\nLLMs out there, including: LLAVA [214], LLAVA-Plus [215],\\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\\nexpected to be continued. Evaluation of these models also is a\\nnew research topic, especially conversational generative vision\\nmodels [217]. Multi-modal LLMs can unlock huge potentials\\nin a variety of tasks, and there has already been a descent\\nprogress in this direction, which needs a dedicated paper to\\ndiscuss all its details.\\nD. Improved LLM Usage and Augmentation techniques\\nAs we described in sectionIV, many of the shortcomings\\nand limitations of LLMs such as hallucination can be ad-\\ndressed through advanced prompt engineering, use of tools,\\nor other augmentation techniques. We should expect not only\\ncontinued, but accelerated research in this area. It is worth\\nmentioning that, in the specific case of software engineering,\\nsome works ([218]) tried to automatically eliminate this issue\\nfrom the overall software engineering workflow\\nLLM-based systems are already starting to replace ma-\\nchine learning systems that were until recently using other\\napproaches. As a clear example of this, LLMs are now being\\ndeployed to better understand people preference and interests,\\nand provide more personalized interactions, whether in cus-\\ntomer service, content recommendation, or other applications.\\nThis involves better understanding of user preferences, and\\nanalyzing their past interactions and using them as the context.\\nWe will continue to see research in the application and usage\\nof LLMs for not only personalization and recommendations,\\nbut many other application areas using other machine learning\\ntechniques.\\nFinally, another important area of research we expect to\\ngather increased attention is that of LLM-based agents and\\nmulti-agent systems [172], [173], [174]. The development of\\nLLM systems with access to external tools and decision-\\nmaking capabilities is both exciting and challenging. We will\\nsee continued research and progress in this important area that\\nsome argue could lead to Artificial General Intelligence (AGI).\\nE. Security and Ethical/Responsible AI\\nEnsuring the robustness and security of LLMs against\\nadversarial attacks and other vulnerabilities is a critical area\\nof research [219]. As LLMs are increasingly deployed in real-\\nworld applications, they need to be protected from potential\\nthreats, to prevent them being used to manipulate people or\\nspread mis-information.\\nAddressing ethical concerns and biases in LLMs is another\\nactive area of research. Efforts are being made to ensure that\\nLLMs are fair, unbiased, and capable of handling sensitive\\ninformation responsibly. As LLMs are being used more and\\nmore by a large number of people on a daily basis, making\\nsure they are unbiased and behave responsibly is crucial.\\nVIII.\\nCONCLUSION\\nThis paper present a survey of LLMs developed in the\\npast few years. We first provide an overview of early pre-\\ntrained language models (e.g., as BERT), then review three\\npopular LLM families (GPT, LLaMA, PaLM), and other\\nrepresentative LLMs. We then survey methods and techniques\\nof building, augmenting, and using LLMs. We review popular\\nLLM datasets and benchmarks, and compare performance of\\na set of prominent models on public benchmarks. Finally, we\\npresent open challenges and future research directions.\\nREFERENCES\\n[1]\\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\\n[2]\\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\\net al., “Training compute-optimal large language models,” arXiv\\npreprint arXiv:2203.15556, 2022.\\n[3]\\nC. E. Shannon, “Prediction and entropy of printed english,” Bell system\\ntechnical journal, vol. 30, no. 1, pp. 50–64, 1951.\\n[4]\\nF. Jelinek, Statistical methods for speech recognition.\\nMIT press,\\n1998.\\n[5]\\nC. Manning and H. Schutze, Foundations of statistical natural lan-\\nguage processing.\\nMIT press, 1999.\\n[6]\\nC. D. Manning, An introduction to information retrieval.\\nCambridge\\nuniversity press, 2009.\\n[7]\\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\\nB. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\\nmodels,” arXiv preprint arXiv:2303.18223, 2023.\\n[8]\\nC. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He et al., “A comprehensive survey on pretrained foundation mod-\\nels: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419,\\n2023.\\n[9]\\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\\ntrain, prompt, and predict: A systematic survey of prompting methods\\nin natural language processing,” ACM Computing Surveys, vol. 55,\\nno. 9, pp. 1–35, 2023.\\n[10]\\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint\\narXiv:2301.00234, 2022.\\n[11]\\nJ. Huang and K. C.-C. Chang, “Towards reasoning in large language\\nmodels: A survey,” arXiv preprint arXiv:2212.10403, 2022.\\n[12]\\nS. F. Chen and J. Goodman, “An empirical study of smoothing\\ntechniques for language modeling,” Computer Speech & Language,\\nvol. 13, no. 4, pp. 359–394, 1999.\\n[13]\\nY. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\\nlanguage model,” Advances in neural information processing systems,\\nvol. 13, 2000.\\n[14]\\nH. Schwenk, D. D´echelotte, and J.-L. Gauvain, “Continuous space\\nlanguage models for statistical machine translation,” in Proceedings\\nof the COLING/ACL 2006 Main Conference Poster Sessions, 2006,\\npp. 723–730.\\n[15]\\nT. Mikolov, M. Karafi´at, L. Burget, J. Cernock`y, and S. Khudanpur,\\n“Recurrent neural network based language model.” in Interspeech,\\nvol. 2, no. 3.\\nMakuhari, 2010, pp. 1045–1048.\\n[16]\\nA. Graves, “Generating sequences with recurrent neural networks,”\\narXiv preprint arXiv:1308.0850, 2013.\\n[17]\\nP.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning\\ndeep structured semantic models for web search using clickthrough\\ndata,” in Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management, 2013, pp. 2333–2338.\\n[18]\\nJ. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\\nConversational Information Retrieval. Springer Nature, 2023, vol. 44.\\n[19]\\nI. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\\nwith neural networks,” Advances in neural information processing\\nsystems, vol. 27, 2014.\\n[20]\\nK. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On\\nthe properties of neural machine translation: Encoder-decoder ap-\\nproaches,” arXiv preprint arXiv:1409.1259, 2014.\\n[21]\\nH. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll´ar,\\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to\\nvisual concepts and back,” in Proceedings of the IEEE conference\\non computer vision and pattern recognition, 2015, pp. 1473–1482.\\n[22]\\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:\\nA neural image caption generator,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2015, pp.\\n3156–3164.\\n[23]\\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations. corr\\nabs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.\\n[24]\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805, 2018.\\n[25]\\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\\n[26]\\nP. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\\nwith disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.\\n[27]\\nX. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao,\\nA. Zhang, L. Zhang et al., “Pre-trained models: Past, present and\\nfuture,” AI Open, vol. 2, pp. 225–250, 2021.\\n[28]\\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\\nmodels for natural language processing: A survey,” Science China\\nTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\\n[29]\\nA. Gu, K. Goel, and C. R´e, “Efficiently modeling long sequences with\\nstructured state spaces,” 2022.\\n[30]\\nA. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\\n[31]\\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,\\n“Palm: Scaling language modeling with pathways,” arXiv preprint\\narXiv:2204.02311, 2022.\\n[32]\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\\nOpen and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971, 2023.\\n[33]\\nOpenAI,\\n“GPT-4\\nTechnical\\nReport,”\\nhttps://arxiv.org/pdf/2303.\\n08774v3.pdf, 2023.\\n[34]\\nJ.\\nWei,\\nX.\\nWang,\\nD.\\nSchuurmans,\\nM.\\nBosma,\\nb.\\nichter,\\nF.\\nXia,\\nE.\\nChi,\\nQ.\\nV.\\nLe,\\nand\\nD.\\nZhou,\\n“Chain-of-thought\\nprompting\\nelicits\\nreasoning\\nin\\nlarge\\nlanguage\\nmodels,”\\nin\\nAdvances in Neural Information Processing Systems, S. Koyejo,\\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35.\\nCurran Associates, Inc., 2022, pp. 24 824–24 837.\\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\n[35]\\nG. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al., “Augmented language models: a survey,” arXiv preprint\\narXiv:2302.07842, 2023.\\n[36]\\nB. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang,\\nL. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try\\nagain: Improving large language models with external knowledge and\\nautomated feedback,” arXiv preprint arXiv:2302.12813, 2023.\\n[37]\\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\\n“React: Synergizing reasoning and acting in language models,” arXiv\\npreprint arXiv:2210.03629, 2022.\\n[38]\\nD. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal\\nrepresentations by error propagation,” 1985.\\n[39]\\nJ. L. Elman, “Finding structure in time,” Cognitive science, vol. 14,\\nno. 2, pp. 179–211, 1990.\\n[40]\\nM. V. Mahoney, “Fast text compression with neural networks.” in\\nFLAIRS conference, 2000, pp. 230–234.\\n[41]\\nT. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y, “Strate-\\ngies for training large scale neural network language models,” in 2011\\nIEEE Workshop on Automatic Speech Recognition & Understanding.\\nIEEE, 2011, pp. 196–201.\\n[42]\\ntmikolov.\\nrnnlm.\\n[Online].\\nAvailable:\\nhttps://www.fit.vutbr.cz/\\n∼imikolov/rnnlm/\\n[43]\\nS. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, “Deep learning–based text classification: a comprehensive\\nreview,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40,\\n2021.\\n[44]\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\\nAdvances in neural information processing systems, vol. 30, 2017.\\n[45]\\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language represen-\\ntations,” arXiv preprint arXiv:1909.11942, 2019.\\n[46]\\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-\\ntraining text encoders as discriminators rather than generators,” arXiv\\npreprint arXiv:2003.10555, 2020.\\n[47]\\nG. Lample and A. Conneau, “Cross-lingual language model pretrain-\\ning,” arXiv preprint arXiv:1901.07291, 2019.\\n[48]\\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\\nQ. V. Le, “Xlnet: Generalized autoregressive pretraining for language\\nunderstanding,” Advances in neural information processing systems,\\nvol. 32, 2019.\\n[49]\\nL. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\\nnatural language understanding and generation,” Advances in neural\\ninformation processing systems, vol. 32, 2019.\\n[50]\\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv-\\ning language understanding by generative pre-training,” 2018.\\n[51]\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[52]\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\\nwith a unified text-to-text transformer,” The Journal of Machine\\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\\n[53]\\nL. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\\n[54]\\nK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked\\nsequence to sequence pre-training for language generation,” arXiv\\npreprint arXiv:1905.02450, 2019.\\n[55]\\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,” arXiv preprint arXiv:1910.13461, 2019.\\n[56]\\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems, vol. 33, pp. 1877–1901, 2020.\\n[57]\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\\nplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al.,\\n“Evaluating large language models trained on code,” arXiv preprint\\narXiv:2107.03374, 2021.\\n[58]\\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[59]\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nNeural Information Processing Systems, vol. 35, pp. 27 730–27 744,\\n2022.\\n[60]\\nOpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\\n//openai.com/blog/chatgpt\\n[61]\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\\n2: Open foundation and fine-tuned chat models,” arXiv preprint\\narXiv:2307.09288, 2023.\\n[62]\\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, “Alpaca: A strong, replicable instruction-\\nfollowing model,” Stanford Center for Research on Foundation Mod-\\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6,\\np. 7, 2023.\\n[63]\\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-\\nficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\\n2023.\\n[64]\\nX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nand D. Song, “Koala: A dialogue model for academic research,” Blog\\npost, April, vol. 1, 2023.\\n[65]\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\\n“Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\\n[66]\\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,\\nJ. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models\\nfor code,” arXiv preprint arXiv:2308.12950, 2023.\\n[67]\\nS. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large\\nlanguage model connected with massive apis,” 2023.\\n[68]\\nA. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\\nS. Naidu, “Giraffe: Adventures in expanding context lengths in llms,”\\narXiv preprint arXiv:2308.10882, 2023.\\n[69]\\nB. Huang, “Vigogne: French instruction-following and chat models,”\\nhttps://github.com/bofenghuang/vigogne, 2023.\\n[70]\\nY. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can\\ncamels go? exploring the state of instruction tuning on open resources,”\\narXiv preprint arXiv:2306.04751, 2023.\\n[71]\\nS. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski,\\nand P. Miło´s, “Focused transformer: Contrastive training for context\\nscaling,” arXiv preprint arXiv:2307.03170, 2023.\\n[72]\\nD.\\nMahan,\\nR.\\nCarlow,\\nL.\\nCastricato,\\nN.\\nCooper,\\nand\\nC.\\nLaforte,\\n“Stable\\nbeluga\\nmodels.”\\n[Online].\\nAvailable:\\n[https://huggingface.co/stabilityai/StableBeluga2](https://\\nhuggingface.co/stabilityai/StableBeluga2)\\n[73]\\nY. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar-\\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling\\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399,\\n2022.\\n[74]\\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,\\nY. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-\\nfinetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\\n[75]\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical\\nreport,” arXiv preprint arXiv:2305.10403, 2023.\\n[76]\\nK. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\\nmodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,\\n2022.\\n[77]\\nK. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-\\nlevel medical question answering with large language models,” arXiv\\npreprint arXiv:2305.09617, 2023.\\n[78]\\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\\nA. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot\\nlearners,” arXiv preprint arXiv:2109.01652, 2021.\\n[79]\\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\\nmodels: Methods, analysis & insights from training gopher,” arXiv\\npreprint arXiv:2112.11446, 2021.\\n[80]\\nV. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi-\\ntask prompted training enables zero-shot task generalization,” arXiv\\npreprint arXiv:2110.08207, 2021.\\n[81]\\nY. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-\\ntraining for language understanding and generation,” arXiv preprint\\narXiv:2107.02137, 2021.\\n[82]\\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\\net al., “Improving language models by retrieving from trillions of\\ntokens,” in International conference on machine learning.\\nPMLR,\\n2022, pp. 2206–2240.\\n[83]\\nO. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technical\\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\\n[84]\\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\\nY. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of\\nlanguage models with mixture-of-experts,” in International Conference\\non Machine Learning.\\nPMLR, 2022, pp. 5547–5569.\\n[85]\\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\\nT. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language\\nmodels for dialog applications,” arXiv preprint arXiv:2201.08239,\\n2022.\\n[86]\\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\\nC. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained\\ntransformer language models,” arXiv preprint arXiv:2205.01068, 2022.\\n[87]\\nR. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\\navia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large\\nlanguage model for science,” arXiv preprint arXiv:2211.09085, 2022.\\n[88]\\nE. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\\nS. Savarese, and C. Xiong, “Codegen: An open large language\\nmodel for code with multi-turn program synthesis,” arXiv preprint\\narXiv:2203.13474, 2022.\\n[89]\\nS. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al.,\\n“Alexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model,” arXiv preprint arXiv:2208.01448, 2022.\\n[90]\\nA. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu,\\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al.,\\n“Improving alignment of dialogue agents via targeted human judge-\\nments,” arXiv preprint arXiv:2209.14375, 2022.\\n[91]\\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\\nV. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,\\n“Solving quantitative reasoning problems with language models,”\\nAdvances in Neural Information Processing Systems, vol. 35, pp.\\n3843–3857, 2022.\\n[92]\\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster,\\nH. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning\\nparadigms,” arXiv preprint arXiv:2205.05131, 2022.\\n[93]\\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-\\nparameter open-access multilingual language model,” arXiv preprint\\narXiv:2211.05100, 2022.\\n[94]\\nA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\\nW. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained\\nmodel,” arXiv preprint arXiv:2210.02414, 2022.\\n[95]\\nS. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,\\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\\n“Pythia: A suite for analyzing large language models across train-\\ning and scaling,” in International Conference on Machine Learning.\\nPMLR, 2023, pp. 2397–2430.\\n[96]\\nS. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\\nA. Awadallah, “Orca: Progressive learning from complex explanation\\ntraces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.\\n[97]\\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source\\nbe with you!” arXiv preprint arXiv:2305.06161, 2023.\\n[98]\\nS. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv,\\nL. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you\\nneed: Aligning perception with language models,” arXiv preprint\\narXiv:2302.14045, 2023.\\n[99]\\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly\\ncapable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\\n[100]\\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue:\\nEmbodied reasoning through planning with language models,” arXiv\\npreprint arXiv:2207.05608, 2022.\\n[101]\\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti\\net al., “Using deepspeed and megatron to train megatron-turing\\nnlg 530b, a large-scale generative language model,” arXiv preprint\\narXiv:2201.11990, 2022.\\n[102]\\nI. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\\ndocument transformer,” arXiv preprint arXiv:2004.05150, 2020.\\n[103]\\nS. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language\\nmodel instruction meta learning through the lens of generalization,”\\narXiv preprint arXiv:2212.12017, 2022.\\n[104]\\nY. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\\nand F. Wei, “Language models are general-purpose interfaces,” arXiv\\npreprint arXiv:2206.06336, 2022.\\n[105]\\nZ. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,\\nand C. Gan, “Principle-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,” arXiv preprint\\narXiv:2305.03047, 2023.\\n[106]\\nW. E. team, “Palmyra-base Parameter Autoregressive Language\\nModel,” https://dev.writer.com, 2023.\\n[107]\\n——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\\n[108]\\nYandex.\\nYalm.\\n[Online].\\nAvailable:\\nhttps://github.com/yandex/\\nYaLM-100B\\n[109]\\nM. Team et al., “Introducing mpt-7b: a new standard for open-source,\\ncommercially usable llms,” 2023.\\n[110]\\nA. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2:\\nTeaching small language models how to reason,” 2023.\\n[111]\\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and\\nG. Neubig, “Pal: Program-aided language models,” in International\\nConference on Machine Learning.\\nPMLR, 2023, pp. 10 764–10 799.\\n[112]\\nAnthropic. claude. [Online]. Available: https://www.anthropic.com/\\nnews/introducing-claude\\n[113]\\nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\\n“Codegen2: Lessons for training llms on programming and natural\\nlanguages,” arXiv preprint arXiv:2305.02309, 2023.\\n[114]\\nL. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada,\\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct\\ndistillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.\\n[115]\\nX. team. Grok. [Online]. Available: https://grok.x.ai/\\n[116]\\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\\nand J. Zhou, “Qwen-vl: A frontier large vision-language model with\\nversatile abilities,” arXiv preprint arXiv:2308.12966, 2023.\\n[117]\\nmixtral.\\nmixtral.\\n[Online].\\nAvailable:\\nhttps://mistral.ai/news/\\nmixtral-of-experts/\\n[118]\\nD. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei,\\nA. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative\\nlanguage model for multimodal document understanding,” 2023.\\n[119]\\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\\nY. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:\\nWhen the large language model meets programming – the rise of code\\nintelligence,” 2024.\\n[120]\\nF. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge\\nfusion of large language models,” 2024.\\n[121]\\nP. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source\\nsmall language model,” 2024.\\n[122]\\nC. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan,\\n“Llama pro: Progressive llama with block expansion,” 2024.\\n[123]\\nX. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\\nM. Kazi, “Transformer models: an introduction and catalog,” 2023.\\n[124]\\nG. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-\\nweb dataset for falcon llm: outperforming curated corpora with web\\ndata, and web data only,” arXiv preprint arXiv:2306.01116, 2023.\\n[125]\\nD. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al.,\\n“Scaling laws and interpretability of learning from repeated data,”\\narXiv preprint arXiv:2205.10487, 2022.\\n[126]\\nP. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\\nposition representations,” arXiv preprint arXiv:1803.02155, 2018.\\n[127]\\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En-\\nhanced transformer with rotary position embedding,” arXiv preprint\\narXiv:2104.09864, 2021.\\n[128]\\nO. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention\\nwith linear biases enables input length extrapolation,” arXiv preprint\\narXiv:2108.12409, 2021.\\n[129]\\nG. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in\\nlanguage pre-training,” arXiv preprint arXiv:2006.15595, 2020.\\n[130]\\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\\n[131]\\nW. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\\nto trillion parameter models with simple and efficient sparsity,” The\\nJournal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\\n2022.\\n[132]\\nR. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n“Parameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,” 2021.\\n[133]\\nS. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\\nmodels: A survey,” 2023.\\n[134]\\nS. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task\\ngeneralization via natural language crowdsourcing instructions,” arXiv\\npreprint arXiv:2104.08773, 2021.\\n[135]\\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\\ngenerated instructions,” arXiv preprint arXiv:2212.10560, 2022.\\n[136]\\nK. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\\nreport.pdf\\n[137]\\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\\nD. Amodei, “Deep reinforcement learning from human preferences,”\\nAdvances in neural information processing systems, vol. 30, 2017.\\n[138]\\nH. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car-\\nbune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from\\nhuman feedback with ai feedback,” arXiv preprint arXiv:2309.00267,\\n2023.\\n[139]\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\\nC. Finn, “Direct preference optimization: Your language model is\\nsecretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.\\n[140]\\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory\\noptimizations toward training trillion parameter models,” in SC20: In-\\nternational Conference for High Performance Computing, Networking,\\nStorage and Analysis.\\nIEEE, 2020, pp. 1–16.\\n[141]\\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing\\nrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\\n[142]\\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\\narXiv preprint arXiv:2106.09685, 2021.\\n[143]\\nG. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\\n[144]\\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation:\\nA survey,” International Journal of Computer Vision, vol. 129, pp.\\n1789–1819, 2021.\\n[145]\\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J.\\nBang, A. Madotto, and P. Fung, “Survey of hallucination in natural\\nlanguage generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023.\\n[Online]. Available: https://doi.org/10.1145/3571730\\n[146]\\nN. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\\nM. Steedman, “Sources of hallucination by large language models on\\ninference tasks,” 2023.\\n[147]\\nC.-Y.\\nLin,\\n“ROUGE:\\nA\\npackage\\nfor\\nautomatic\\nevaluation\\nof\\nsummaries,” in Text Summarization Branches Out.\\nBarcelona, Spain:\\nAssociation for Computational Linguistics, Jul. 2004, pp. 74–81.\\n[Online]. Available: https://aclanthology.org/W04-1013\\n[148]\\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\\nautomatic evaluation of machine translation,” in Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics,\\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311–\\n318. [Online]. Available: https://aclanthology.org/P02-1040\\n[149]\\nB. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\\nW. Cohen, “Handling divergent reference texts when evaluating\\ntable-to-text generation,” in Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics, A. Korhonen,\\nD. Traum, and L. M`arquez, Eds.\\nFlorence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online].\\nAvailable: https://aclanthology.org/P19-1483\\n[150]\\nZ. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful\\nneural table-to-text generation with content-matching constraints,”\\nin Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter,\\nand J. Tetreault, Eds.\\nOnline: Association for Computational\\nLinguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https:\\n//aclanthology.org/2020.acl-main.101\\n[151]\\nH. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis-\\ntent dialogues by exploiting natural language inference,” Proceedings\\nof the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp.\\n8878–8885, Apr. 2020.\\n[152]\\nO. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,\\nand O. Abend, “q2: Evaluating factual consistency in knowledge-\\ngrounded dialogues via question generation and question answering,”\\nin Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing, M.-F. Moens, X. Huang, L. Specia,\\nand S. W.-t. Yih, Eds.\\nOnline and Punta Cana, Dominican Republic:\\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856–7870.\\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\\n[153]\\nN. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution\\nin dialogue systems: The BEGIN benchmark,” Transactions of the\\nAssociation for Computational Linguistics, vol. 10, pp. 1066–1083,\\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\\n[154]\\nS. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\\nY. Liu, and D. Z. Hakkani-T¨ur, “Rome was built in 1776: A case study\\non factual correctness in knowledge-grounded response generation,”\\nArXiv, vol. abs/2110.05456, 2021.\\n[155]\\nS. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\\nL. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic\\nevaluation of factual precision in long form text generation,” 2023.\\n[156]\\nD. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\\nV. Chaudhary, and M. Young, “Machine learning: The high interest\\ncredit card of technical debt,” in SE4ML: Software Engineering for\\nMachine Learning (NIPS 2014 Workshop), 2014.\\n[157]\\nZ. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought\\nprompting in large language models,” 2022.\\n[158]\\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\\nlarge language models,” 2023.\\n[159]\\nP. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero-\\nresource black-box hallucination detection for generative large lan-\\nguage models,” 2023.\\n[160]\\nN. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\\nand S. Yao, “Reflexion: Language agents with verbal reinforcement\\nlearning,” 2023.\\n[161]\\nS. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\\nK. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi,\\nA. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and\\neecs curriculum using large language models,” 2023.\\n[162]\\nT. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\\nCai, “Promptchainer: Chaining large language model prompts through\\nvisual programming,” 2022.\\n[163]\\nY. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\\nJ. Ba, “Large language models are human-level prompt engineers,”\\n2023.\\n[164]\\nP. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\\nN. Goyal, H. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and\\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\\nNLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2005.11401\\n[165]\\nY. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\\nH. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997, 2023.\\n[166]\\nA. W. Services. (Year of publication, e.g., 2023) Question answering\\nusing retrieval augmented generation with foundation models in\\namazon\\nsagemaker\\njumpstart.\\nAccessed:\\nDate\\nof\\naccess,\\ne.g.,\\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\\n[167]\\nS. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large\\nlanguage models and knowledge graphs: A roadmap,” arXiv preprint\\narXiv:2306.08302, 2023.\\n[168]\\nZ. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\n2023.\\n[169]\\nT. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” 2023.\\n[170]\\nB. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\\nand M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use\\nfor large language models,” 2023.\\n[171]\\nY. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:\\nSolving ai tasks with chatgpt and its friends in huggingface,” arXiv\\npreprint arXiv:2303.17580, 2023.\\n[172]\\nZ. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou et al., “The rise and potential of large language model\\nbased agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.\\n[173]\\nL. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y. Lin et al., “A survey on large language model\\nbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\\n[174]\\nZ. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\\nR. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-\\nFei, and J. Gao, “Agent ai: Surveying the horizons of multimodal\\ninteraction,” arXiv preprint arXiv:2401.03568, 2024.\\n[175]\\nB. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo:\\nDecoupling reasoning from observations for efficient augmented lan-\\nguage models,” 2023.\\n[176]\\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\\n“React: Synergizing reasoning and acting in language models,” 2023.\\n[177]\\nV. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc-\\ning large language model completions with dialog-enabled resolving\\nagents,” 2023.\\n[178]\\nY. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang,\\nand X. Xie, “A survey on evaluation of large language models,” 2023.\\n[179]\\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\\nQ.\\nLe,\\nand\\nS.\\nPetrov,\\n“Natural\\nquestions:\\nA\\nbenchmark\\nfor\\nquestion answering research,” Transactions of the Association for\\nComputational Linguistics, vol. 7, pp. 452–466, 2019. [Online].\\nAvailable: https://aclanthology.org/Q19-1026\\n[180]\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\n2021.\\n[181]\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\\nlanguage models,” arXiv preprint arXiv:2108.07732, 2021.\\n[182]\\nE. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\\nand L. Zettlemoyer, “QuAC: Question answering in context,” in\\nProceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, E. Riloff, D. Chiang, J. Hockenmaier, and\\nJ. Tsujii, Eds.\\nBrussels, Belgium: Association for Computational\\nLinguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available:\\nhttps://aclanthology.org/D18-1241\\n[183]\\nD. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring\\ncoding challenge competence with apps,” NeurIPS, 2021.\\n[184]\\nV. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured\\nqueries from natural language using reinforcement learning,” arXiv\\npreprint arXiv:1709.00103, 2017.\\n[185]\\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for reading\\ncomprehension,” in Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\nR. Barzilay and M.-Y. Kan, Eds.\\nVancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].\\nAvailable: https://aclanthology.org/P17-1147\\n[186]\\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale\\nReAding comprehension dataset from examinations,” in Proceedings\\nof the 2017 Conference on Empirical Methods in Natural Language\\nProcessing, M. Palmer, R. Hwa, and S. Riedel, Eds.\\nCopenhagen,\\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\\n785–794. [Online]. Available: https://aclanthology.org/D17-1082\\n[187]\\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+\\nquestions for machine comprehension of text,” in Proceedings of\\nthe 2016 Conference on Empirical Methods in Natural Language\\nProcessing, J. Su, K. Duh, and X. Carreras, Eds.\\nAustin, Texas:\\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383–2392.\\n[Online]. Available: https://aclanthology.org/D16-1264\\n[188]\\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural\\nyes/no\\nquestions,”\\nCoRR,\\nvol.\\nabs/1905.10044,\\n2019.\\n[Online].\\nAvailable: http://arxiv.org/abs/1905.10044\\n[189]\\nD. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\\n“Looking beyond the surface:a challenge set for reading compre-\\nhension over multiple sentences,” in Proceedings of North American\\nChapter of the Association for Computational Linguistics (NAACL),\\n2018.\\n[190]\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\\nJ. Schulman, “Training verifiers to solve math word problems,”\\nCoRR,\\nvol.\\nabs/2110.14168,\\n2021.\\n[Online].\\nAvailable:\\nhttps:\\n//arxiv.org/abs/2110.14168\\n[191]\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\\nwith the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online].\\nAvailable: https://arxiv.org/abs/2103.03874\\n[192]\\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:\\nCan a machine really finish your sentence?” 2019.\\n[193]\\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try\\narc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018.\\n[Online]. Available: http://arxiv.org/abs/1803.05457\\n[194]\\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA:\\nreasoning about physical commonsense in natural language,” CoRR,\\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\\n1911.11641\\n[195]\\nM. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa:\\nCommonsense reasoning about social interactions,” CoRR, vol.\\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\\n09728\\n[196]\\nT. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of\\narmor conduct electricity? A new dataset for open book question\\nanswering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1809.02789\\n[197]\\nS. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\\n[198]\\nZ. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov,\\nand C. D. Manning, “Hotpotqa: A dataset for diverse, explainable\\nmulti-hop question answering,” CoRR, vol. abs/1809.09600, 2018.\\n[Online]. Available: http://arxiv.org/abs/1809.09600\\n[199]\\nY. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A\\ndataset for llm question answering with external tools,” arXiv preprint\\narXiv:2306.13304, 2023.\\n[200]\\nD. Chen, J. Bolton, and C. D. Manning, “A thorough examination\\nof the cnn/daily mail reading comprehension task,” in Association for\\nComputational Linguistics (ACL), 2016.\\n[201]\\nR. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text\\nsummarization using sequence-to-sequence rnns and beyond,” arXiv\\npreprint arXiv:1602.06023, 2016.\\n[202]\\nY. Bai and D. Z. Wang, “More than reading comprehension: A survey\\non datasets and metrics of textual question answering,” arXiv preprint\\narXiv:2109.12264, 2021.\\n[203]\\nH.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in\\nhistory for conversational machine comprehension,” arXiv preprint\\narXiv:1810.06683, 2018.\\n[204]\\nS. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A\\nsurvey on evaluation metrics for machine translation,” Mathematics,\\nvol. 11, no. 4, p. 1006, 2023.\\n[205]\\nJ. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:\\nA large-scale hallucination evaluation benchmark for large language\\nmodels,” in Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing, 2023, pp. 6449–6464.\\n[206]\\nSimon\\nMark\\nHughes,\\n“Hughes\\nhallucination\\nevaluation\\nmodel\\n(hhem)\\nleaderboard,”\\n2024,\\nhttps://huggingface.co/spaces/vectara/\\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\\n[207]\\nJ. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\\nR. McHardy, “Challenges and applications of large language models,”\\narXiv preprint arXiv:2307.10169, 2023.\\n[208]\\nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al.,\\n“Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\\n[209]\\nY. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T.\\nLee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv\\npreprint arXiv:2309.05463, 2023.\\n[210]\\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\\nY. Bengio, S. Ermon, and C. R´e, “Hyena hierarchy: Towards larger\\nconvolutional language models,” 2023.\\n[211]\\nM. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\\nA. Thomas, “StripedHyena: Moving Beyond Transformers with\\nHybrid Signal Processing Models,” 12 2023. [Online]. Available:\\nhttps://github.com/togethercomputer/stripedhyena\\n[212]\\nD. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\\nB. Spector, M. Poli, A. Rudra, and C. R´e, “Monarch mixer: A simple\\nsub-quadratic gemm-based architecture,” 2023.\\n[213]\\nG. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture\\nmodels,” Annual review of statistics and its application, vol. 6, pp.\\n355–378, 2019.\\n[214]\\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv\\npreprint arXiv:2304.08485, 2023.\\n[215]\\nS. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus:\\nLearning to use tools for creating multimodal agents,” arXiv preprint\\narXiv:2311.05437, 2023.\\n[216]\\nS. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any\\nmultimodal llm,” arXiv preprint arXiv:2309.05519, 2023.\\n[217]\\nN. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\\nD. Z¨uhlke, “Convgenvismo: Evaluation of conversational generative\\nvision models,” 2023.\\n[218]\\nN. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unit\\ntest improvement using large language models at meta,” arXiv preprint\\narXiv:2402.09171, 2024.\\n[219]\\nL. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang,\\nW. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large\\nlanguage models,” arXiv preprint arXiv:2401.05561, 2024.\\n[220]\\nMicrosoft.\\nDeepspeed.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nmicrosoft/DeepSpeed\\n[221]\\nHuggingFace. Transformers. [Online]. Available: https://github.com/\\nhuggingface/transformers\\n[222]\\nNvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\\nMegatron-LM\\n[223]\\nBMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\\nBMTrain\\n[224]\\nEleutherAI.\\ngpt-neox.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nEleutherAI/gpt-neox\\n[225]\\nmicrosoft. Lora. [Online]. Available: https://github.com/microsoft/\\nLoRA\\n[226]\\nColossalAI.\\nColossalai.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nhpcaitech/ColossalAI\\n[227]\\nFastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\\nFastChat\\n[228]\\nskypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\\nskypilot\\n[229]\\nvllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\\n[230]\\nhuggingface. text-generation-inference. [Online]. Available: https:\\n//github.com/huggingface/text-generation-inference\\n[231]\\nlangchain.\\nlangchain.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nlangchain-ai/langchain\\n[232]\\nbentoml. Openllm. [Online]. Available: https://github.com/bentoml/\\nOpenLLM\\n[233]\\nembedchain. embedchain. [Online]. Available: https://github.com/\\nembedchain/embedchain\\n[234]\\nmicrosoft. autogen. [Online]. Available: https://github.com/microsoft/\\nautogen\\n[235]\\nbabyagi.\\nbabyagi.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nyoheinakajima/babyagi\\n[236]\\nguidance.\\nguidance.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nguidance-ai/guidance\\n[237]\\nprompttools. prompttools. [Online]. Available: https://github.com/\\nhegelai/prompttools\\n[238]\\npromptfoo.\\npromptfoo.\\n[Online].\\nAvailable:\\nhttps://github.com/\\npromptfoo/promptfoo\\n[239]\\nfacebook.\\nfaiss.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nfacebookresearch/faiss\\n[240]\\nmilvus. milvus. [Online]. Available: https://github.com/milvus-io/\\nmilvus\\n[241]\\nqdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\\n[242]\\nweaviate. weaviate. [Online]. Available: https://github.com/weaviate/\\nweaviate\\n[243]\\nllama index. llama-index. [Online]. Available: https://github.com/\\nrun-llama/llama index\\nAPPENDIX\\n1. Open Source Toolkits For LLM Development and\\nDeployment\\nThere are various frameworks and libraries developed for\\nLLM training, evaluation, and deployment, and covering every\\nsingle framework is out of this paper’s scope. But we try to\\nprovide a brief introduction of some of the most popular ones,\\ngrouped into different categories.\\nA. LLM Training/Inference Frameworks\\nSome of the popular frameworks which are useful for LLM\\ntraining includes (note that some of them can be used beyond\\nLLM training too):\\nDeepSpeed [220] is a deep learning optimization library\\nthat makes distributed training and inference easy, efficient,\\nand effective. DeepSpeed enables world’s most powerful lan-\\nguage models like MT-530B and BLOOM. It is an easy-\\nto-use deep learning optimization software suite that powers\\nunprecedented scale and speed for both training and inference.\\nWith DeepSpeed you can:\\nTransformers [221] is library by HuggingFace which\\nprovides thousands of pretrained models to perform tasks on\\ndifferent modalities such as text, vision, and audio. Using\\npretrained models one can reduce compute costs, carbon\\nfootprint, and save the time and resources required to train\\na model from scratch.\\nMegatron-LM [222] is a large, powerful transformer\\ndeveloped by the Applied Deep Learning Research team\\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\\nquence, and pipeline), and multi-node pre-training of trans-\\nformer based models such as GPT, BERT, and T5 using mixed\\nprecision.\\nBMTrain [223] is an efficient large model training toolkit\\nthat can be used to train large models with tens of billions of\\nparameters. It can train models in a distributed manner while\\nkeeping the code as simple as stand-alone training.\\nGPT-NeoX [224] leverages many of the same features and\\ntechnologies as the popular Megatron-DeepSpeed library but\\nwith substantially increased usability and novel optimizations.\\nLoRA [225] library provides the support for Low-Rank\\nAdaptation of Large Language Models. It reduces the number\\nof trainable parameters by learning pairs of rank-decompostion\\nmatrices while freezing the original weights. This vastly\\nreduces the storage requirement for large language models\\nadapted to specific tasks and enables efficient task-switching\\nduring deployment all without introducing inference latency.\\nLoRA also outperforms several other adaptation methods in-\\ncluding adapter, prefix-tuning, and fine-tuning.\\nColossalAI library [226] provides a collection of parallel\\ncomponents. It aims to support developers to write their\\ndistributed deep learning models just like how they write their\\nmodel on their laptop. They provide user-friendly tools to\\nkickstart distributed training and inference in a few lines. In\\nterms of Parallelism strategies, they support: Data Parallelism,\\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\\nOptimizer (ZeRO) [140], and Auto-Parallelism.\\nB. Deployment Tools\\nWe provide an overview of some of the most popular LLM\\ndeployment tools here.\\nFastChat [227] is an open platform for training, serv-\\ning, and evaluating large language model based chatbots.\\nFastChat’s core features include: The training and evaluation\\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\\na distributed multi-model serving system with web UI and\\nOpenAI-compatible RESTful APIs.\\nSkypilot [228] is a framework for running LLMs, AI,\\nand batch jobs on any cloud, offering maximum cost savings,\\nhighest GPU availability, and managed execution.\\nvLLM [229] is a fast and easy-to-use library for LLM in-\\nference and serving. vLLM seamlessly supports many Hugging\\nFace models, including the following architectures: Aquila,\\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\\nYi, and many more.\\ntext-generation-inference [230] is a toolkit for deploying\\nand serving Large Language Models (LLMs). TGI enables\\nhigh-performance text generation for the most popular open-\\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\\nGPT-NeoX, and more.\\nLangChain [231] is a framework for developing applica-\\ntions powered by language models. It enables applications that:\\n•\\nAre context-aware: connect a language model to\\nsources of context (prompt instructions, few shot ex-\\namples, content to ground its response in, etc.)\\n•\\nReason: rely on a language model to reason (about\\nhow to answer based on provided context, what ac-\\ntions to take, etc.)\\nOpenLLM [232] is an open-source platform designed to\\nfacilitate the deployment and operation of large language mod-\\nels (LLMs) in real-world applications. With OpenLLM, you\\ncan run inference on any open-source LLM, deploy them on\\nthe cloud or on-premises, and build powerful AI applications.\\nEmbedchain [233] is an Open Source RAG Framework\\nthat makes it easy to create and deploy AI apps. Embedchain\\nstreamlines the creation of RAG applications, offering a seam-\\nless process for managing various types of unstructured data.\\nIt efficiently segments data into manageable chunks, generates\\nrelevant embeddings, and stores them in a vector database for\\noptimized retrieval.\\nAutogen [234] is a framework that enables the devel-\\nopment of LLM applications using multiple agents that can\\nconverse with each other to solve tasks. AutoGen agents\\nare customizable, conversable, and seamlessly allow human\\nparticipation. They can operate in various modes that employ\\ncombinations of LLMs, human inputs, and tools.\\nBabyAGI [235] is an autonomous Artificial Intelligence\\nagent, that is designed to generate and execute tasks based on\\ngiven objectives. It harnesses cutting-edge technologies from\\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\\nand achieve specific goals. In this blog post, we will dive\\ninto the unique features of BabyAGI and explore how it can\\nstreamline task automation.\\nC. Prompting Libraries\\nGuidance [236] is a programming paradigm that offers\\nsuperior control and efficiency compared to conventional\\nprompting and chaining. It allows users to constrain generation\\n(e.g. with regex and CFGs) as well as to interleave control\\n(conditional, loops) and generation seamlessly.\\nPromptTools [237] offers a set of open-source, self-\\nhostable tools for experimenting with, testing, and evaluating\\nLLMs, vector databases, and prompts. The core idea is to\\nenable developers to evaluate using familiar interfaces like\\ncode, notebooks, and a local playground.\\nPromptBench [?] is a Pytorch-based Python package for\\nEvaluation of Large Language Models (LLMs). It provides\\nuser-friendly APIs for researchers to conduct evaluation on\\nLLMs.\\nPromptfoo [238] is a tool for testing and evaluating LLM\\noutput quality. It systematically test prompts, models, and\\nRAGs with predefined test cases.\\nD. VectorDB\\nFaiss [239] is a library developed by Facebook AI Re-\\nsearch that provides efficient similarity search and clustering\\nof dense vectors. It is designed for use with large-scale,\\nhigh-dimensional data and supports several index types and\\nalgorithms for various use cases.\\nMilvus [240] is an open-source vector database built to\\npower embedding similarity search and AI applications. Mil-\\nvus makes unstructured data search more accessible, and pro-\\nvides a consistent user experience regardless of the deployment\\nenvironment.\\nQdrant [241] is a vector similarity search engine and\\nvector database. It provides a production-ready service with a\\nconvenient API to store, search, and manage points—vectors\\nwith an additional payload Qdrant is tailored to extended\\nfiltering support. environment.\\nWeaviate [242] is an open-source, GraphQL-based vec-\\ntor search engine that enables similarity search on high-\\ndimensional data. While it is open-source, the commercial ver-\\nsion offers additional features, support, and managed services.\\nSome of the other popular options includes LlamaIndex\\n[243] and Pinecone.\\n', '1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various approaches in their respective contexts,\\nand speculate on upcoming trends and innovations.\\nOur contributions are as follows:\\n• In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, delineating\\nits evolution through paradigms including naive RAG,\\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024\\n2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current assessment methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, emphasizing\\npotential enhancements to tackle current challenges.\\nThe paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-\\ntraining data, it initially lacks the capacity to provide up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the\\n3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\\nwidespread adoption of ChatGPT. The Naive RAG follows\\na traditional process that includes indexing, retrieval, and\\ngeneration, which is also characterized as a “Retrieve-Read”\\nframework [7].\\nIndexing starts with the cleaning and extraction of raw data\\nin diverse formats like PDF, HTML, Word, and Markdown,\\nwhich is then converted into a uniform plain text format. To\\naccommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model\\nand stored in vector database. This step is crucial for enabling\\nefficient similarity searches in the subsequent retrieval phase.\\nRetrieval. Upon receipt of a user query, the RAG system\\nemploys the same encoding model utilized during the indexing\\nphase to transform the query into a vector representation.\\nIt then computes the similarity scores between the query\\nvector and the vector of chunks within the indexed corpus.\\nThe system prioritizes and retrieves the top K chunks that\\ndemonstrate the greatest similarity to the query. These chunks\\nare subsequently used as the expanded context in prompt.\\nGeneration. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges. The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrelevance, toxicity, or bias in the outputs,\\ndetracting from the quality and reliability of the responses.\\nAugmentation Hurdles. Integrating retrieved information\\nwith the different task can be challenging, sometimes resulting\\nin disjointed or incoherent outputs. The process may also\\nencounter redundancy when similar information is retrieved\\nfrom multiple sources, leading to repetitive responses. Deter-\\nmining the significance and relevance of various passages and\\nensuring stylistic and tonal consistency add further complexity.\\nFacing complex issues, a single retrieval based on the original\\nquery may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].\\n4\\nFig. 3.\\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common\\nmethods include query rewriting query transformation, query\\nexpansion and other techniques [7], [9]–[11].\\nPost-Retrieval Process. Once relevant context is retrieved,\\nit’s crucial to integrate it effectively with the query. The main\\nmethods in post-retrieval process include rerank chunks and\\ncontext compressing. Re-ranking the retrieved information to\\nrelocate the most relevant content to the edges of the prompt is\\na key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre-ranking to uncover both explicit and transformative knowl-\\nedge [16]. The Memory module leverages the LLM’s memory\\nto guide retrieval, creating an unbounded memory pool that\\n5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes with higher latency and ethical considerations regarding\\ndata retrieval. On the other hand, FT is more static, requiring\\nretraining for updates but enabling deep customization of the\\nmodel’s behavior and style. It demands significant compu-\\ntational resources for dataset preparation and training, and\\nwhile it can reduce hallucinations, it may face challenges with\\nunfamiliar data.\\nIn multiple evaluations of their performance on various\\nknowledge-intensive tasks across different topics, [28] re-\\nvealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new\\nknowledge. Additionally, it was found that LLMs struggle\\nto learn new factual information through unsupervised fine-\\ntuning. The choice between RAG and FT depends on the\\nspecific needs for data dynamics, customization, and com-\\nputational capabilities in the application context. RAG and\\nFT are not mutually exclusive and can complement each\\nother, enhancing a model’s capabilities at different levels.\\nIn some instances, their combined use may lead to optimal\\nperformance. The optimization process involving RAG and FT\\nmay require multiple iterations to achieve satisfactory results.\\nIII. RETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval\\ngranularity, pre-processing of the retrieval, and selection of\\nthe corresponding embedding model.\\nA. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in\\nrecent researches towards utilizing content generated by LLMs\\nthemselves for retrieval and enhancement purposes.\\n6\\nTABLE I\\nSUMMARY OF RAG METHODS\\nMethod\\nRetrieval Source\\nRetrieval\\nData Type\\nRetrieval\\nGranularity\\nAugmentation\\nStage\\nRetrieval\\nprocess\\nCoG [29]\\nWikipedia\\nText\\nPhrase\\nPre-training\\nIterative\\nDenseX [30]\\nFactoidWiki\\nText\\nProposition\\nInference\\nOnce\\nEAR [31]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nUPRISE [20]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nRAST [32]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nSelf-Mem [17]\\nDataset-base\\nText\\nSentence\\nTuning\\nIterative\\nFLARE [24]\\nSearch Engine,Wikipedia\\nText\\nSentence\\nTuning\\nAdaptive\\nPGRA [33]\\nWikipedia\\nText\\nSentence\\nInference\\nOnce\\nFILCO [34]\\nWikipedia\\nText\\nSentence\\nInference\\nOnce\\nRADA [35]\\nDataset-base\\nText\\nSentence\\nInference\\nOnce\\nFilter-rerank [36]\\nSynthesized dataset\\nText\\nSentence\\nInference\\nOnce\\nR-GQA [37]\\nDataset-base\\nText\\nSentence Pair\\nTuning\\nOnce\\nLLM-R [38]\\nDataset-base\\nText\\nSentence Pair\\nInference\\nIterative\\nTIGER [39]\\nDataset-base\\nText\\nItem-base\\nPre-training\\nOnce\\nLM-Indexer [40]\\nDataset-base\\nText\\nItem-base\\nTuning\\nOnce\\nBEQUE [9]\\nDataset-base\\nText\\nItem-base\\nTuning\\nOnce\\nCT-RAG [41]\\nSynthesized dataset\\nText\\nItem-base\\nTuning\\nOnce\\nAtlas [42]\\nWikipedia, Common Crawl\\nText\\nChunk\\nPre-training\\nIterative\\nRAVEN [43]\\nWikipedia\\nText\\nChunk\\nPre-training\\nOnce\\nRETRO++ [44]\\nPre-training Corpus\\nText\\nChunk\\nPre-training\\nIterative\\nINSTRUCTRETRO [45]\\nPre-training corpus\\nText\\nChunk\\nPre-training\\nIterative\\nRRR [7]\\nSearch Engine\\nText\\nChunk\\nTuning\\nOnce\\nRA-e2e [46]\\nDataset-base\\nText\\nChunk\\nTuning\\nOnce\\nPROMPTAGATOR [21]\\nBEIR\\nText\\nChunk\\nTuning\\nOnce\\nAAR [47]\\nMSMARCO,Wikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRA-DIT [27]\\nCommon Crawl,Wikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRAG-Robust [48]\\nWikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRA-Long-Form [49]\\nDataset-base\\nText\\nChunk\\nTuning\\nOnce\\nCoN [50]\\nWikipedia\\nText\\nChunk\\nTuning\\nOnce\\nSelf-RAG [25]\\nWikipedia\\nText\\nChunk\\nTuning\\nAdaptive\\nBGM [26]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nCoQ [51]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nToken-Elimination [52]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nPaperQA [53]\\nArxiv,Online Database,PubMed\\nText\\nChunk\\nInference\\nIterative\\nNoiseRAG [54]\\nFactoidWiki\\nText\\nChunk\\nInference\\nOnce\\nIAG [55]\\nSearch Engine,Wikipedia\\nText\\nChunk\\nInference\\nOnce\\nNoMIRACL [56]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nToC [57]\\nSearch Engine,Wikipedia\\nText\\nChunk\\nInference\\nRecursive\\nSKR [58]\\nDataset-base,Wikipedia\\nText\\nChunk\\nInference\\nAdaptive\\nITRG [59]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nRAG-LongContext [60]\\nDataset-base\\nText\\nChunk\\nInference\\nOnce\\nITER-RETGEN [14]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nIRCoT [61]\\nWikipedia\\nText\\nChunk\\nInference\\nRecursive\\nLLM-Knowledge-Boundary [62]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nRAPTOR [63]\\nDataset-base\\nText\\nChunk\\nInference\\nRecursive\\nRECITE [22]\\nLLMs\\nText\\nChunk\\nInference\\nOnce\\nICRALM [64]\\nPile,Wikipedia\\nText\\nChunk\\nInference\\nIterative\\nRetrieve-and-Sample [65]\\nDataset-base\\nText\\nDoc\\nTuning\\nOnce\\nZemi [66]\\nC4\\nText\\nDoc\\nTuning\\nOnce\\nCRAG [67]\\nArxiv\\nText\\nDoc\\nInference\\nOnce\\n1-PAGER [68]\\nWikipedia\\nText\\nDoc\\nInference\\nIterative\\nPRCA [69]\\nDataset-base\\nText\\nDoc\\nInference\\nOnce\\nQLM-Doc-ranking [70]\\nDataset-base\\nText\\nDoc\\nInference\\nOnce\\nRecomp [71]\\nWikipedia\\nText\\nDoc\\nInference\\nOnce\\nDSP [23]\\nWikipedia\\nText\\nDoc\\nInference\\nIterative\\nRePLUG [72]\\nPile\\nText\\nDoc\\nInference\\nOnce\\nARM-RAG [73]\\nDataset-base\\nText\\nDoc\\nInference\\nIterative\\nGenRead [13]\\nLLMs\\nText\\nDoc\\nInference\\nIterative\\nUniMS-RAG [74]\\nDataset-base\\nText\\nMulti\\nTuning\\nOnce\\nCREA-ICL [19]\\nDataset-base\\nCrosslingual,Text\\nSentence\\nInference\\nOnce\\nPKG [75]\\nLLM\\nTabular,Text\\nChunk\\nInference\\nOnce\\nSANTA [76]\\nDataset-base\\nCode,Text\\nItem\\nPre-training\\nOnce\\nSURGE [77]\\nFreebase\\nKG\\nSub-Graph\\nTuning\\nOnce\\nMK-ToD [78]\\nDataset-base\\nKG\\nEntity\\nTuning\\nOnce\\nDual-Feedback-ToD [79]\\nDataset-base\\nKG\\nEntity Sequence\\nTuning\\nOnce\\nKnowledGPT [15]\\nDataset-base\\nKG\\nTriplet\\nInference\\nMuti-time\\nFABULA [80]\\nDataset-base,Graph\\nKG\\nEntity\\nInference\\nOnce\\nHyKGE [81]\\nCMeKG\\nKG\\nEntity\\nInference\\nOnce\\nKALMV [82]\\nWikipedia\\nKG\\nTriplet\\nInference\\nIterative\\nRoG [83]\\nFreebase\\nKG\\nTriplet\\nInference\\nIterative\\nG-Retriever [84]\\nDataset-base\\nTextGraph\\nSub-Graph\\nInference\\nOnce\\n7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data, such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains [29]).\\nSemi-structured data. typically refers to data that contains a\\ncombination of text and table information, such as PDF. Han-\\ndling semi-structured data poses challenges for conventional\\nRAG systems due to two main reasons. Firstly, text splitting\\nprocesses may inadvertently separate tables, leading to data\\ncorruption during retrieval. Secondly, incorporating tables into\\nthe data can complicate semantic similarity searches. When\\ndealing with semi-structured data, one approach involves lever-\\naging the code capabilities of LLMs to execute Text-2-SQL\\nqueries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both\\nof these methods are not optimal solutions, indicating substan-\\ntial research opportunities in this area.\\nStructured data, such as knowledge graphs (KGs) [86] ,\\nwhich are typically verified and can provide more precise in-\\nformation. KnowledGPT [15] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness. In response to the limitations of\\nLLMs in understanding and answering questions about textual\\ngraphs, G-Retriever [84] integrates Graph Neural Networks\\n4https://hotpotqa.github.io/wiki-readme.html\\n5https://github.com/facebookresearch/DPR\\n(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree\\n(PCST) optimization problem for targeted graph retrieval. On\\nthe contrary, it requires additional effort to build, validate,\\nand maintain structured databases. On the contrary, it requires\\nadditional effort to build, validate, and maintain structured\\ndatabases.\\nLLMs-Generated Content. Addressing the limitations of\\nexternal auxiliary information in RAG, some research has\\nfocused on exploiting LLMs’ internal knowledge. SKR [58]\\nclassifies questions as known or unknown, applying retrieval\\nenhancement selectively. GenRead [13] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts\\noften contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool\\nwith a retrieval-enhanced generator, using a memory selec-\\ntor to choose outputs that serve as dual problems to the\\noriginal question, thus self-enhancing the generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model\\nperformance and task effectiveness.\\n2) Retrieval Granularity: Another important factor besides\\nthe data format of the retrieval source is the granularity of\\nthe retrieved data. Coarse-grained retrieval units theoretically\\ncan provide more relevant information for the problem, but\\nthey may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity\\nincreases the burden of retrieval and does not guarantee seman-\\ntic integrity and meeting the required knowledge. Choosing\\n8\\nthe appropriate retrieval granularity during inference can be\\na simple and effective strategy to improve the retrieval and\\ndownstream task performance of dense retrievers.\\nIn text, retrieval granularity ranges from fine to coarse,\\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\\nument. Among them, DenseX [30]proposed the concept of\\nusing propositions as retrieval units. Propositions are defined\\nas atomic expressions in the text, each encapsulating a unique\\nfactual segment and presented in a concise, self-contained nat-\\nural language format. This approach aims to enhance retrieval\\nprecision and relevance. On the Knowledge Graph (KG),\\nretrieval granularity includes Entity, Triplet, and sub-Graph.\\nThe granularity of retrieval can also be adapted to downstream\\ntasks, such as retrieving Item IDs [40]in recommendation tasks\\nand Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-\\nmented, and transformed into Embeddings to be stored in a\\nvector database. The quality of index construction determines\\nwhether the correct context can be obtained in the retrieval\\nphase.\\n1) Chunking Strategy: The most common method is to split\\nthe document into chunks on a fixed number of tokens (e.g.,\\n100, 256, 512) [88]. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise. How-\\never, chunks leads to truncation within sentences, prompting\\nthe optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-\\ntheless, these approaches still cannot strike a balance between\\nsemantic completeness and context length. Therefore, methods\\nlike Small2Big have been proposed, where sentences (small)\\nare used as the retrieval unit, and the preceding and following\\nsentences are provided as (big) context to LLMs [90].\\n2) Metadata Attachments: Chunks can be enriched with\\nmetadata information such as page number, file name, au-\\nthor,category timestamp. Subsequently, retrieval can be filtered\\nbased on this metadata, limiting the scope of the retrieval.\\nAssigning different weights to document timestamps during\\nretrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-\\numents, metadata can also be artificially constructed. For\\nexample, adding summaries of paragraph, as well as intro-\\nducing hypothetical questions. This method is also known as\\nReverse HyDE. Specifically, using LLM to generate questions\\nthat can be answered by the document, then calculating the\\nsimilarity between the original question and the hypothetical\\nquestion during retrieval to reduce the semantic gap between\\nthe question and the answer.\\n3) Structural Index: One effective method for enhancing\\ninformation retrieval is to establish a hierarchical structure for\\nthe documents. By constructing In structure, RAG system can\\nexpedite the retrieval and processing of pertinent data.\\nHierarchical index structure. File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-\\nmaries are stored at each node, aiding in the swift traversal\\nof data and assisting the RAG system in determining which\\nchunks to extract. This approach can also mitigate the illusion\\ncaused by block extraction issues.\\nKnowledge Graph index. Utilize KG in constructing the\\nhierarchical structure of documents contributes to maintaining\\nconsistency. It delineates the connections between different\\nconcepts and entities, markedly reducing the potential for\\nillusions. Another advantage is the transformation of the\\ninformation retrieval process into instructions that LLM can\\ncomprehend, thereby enhancing the accuracy of knowledge\\nretrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building\\nan index between multiple documents using KG. This KG\\nconsists of nodes (representing paragraphs or structures in the\\ndocuments, such as pages and tables) and edges (indicating\\nsemantic/lexical similarity between paragraphs or relationships\\nwithin the document structure), effectively addressing knowl-\\nedge retrieval and reasoning problems in a multi-document\\nenvironment.\\nC. Query Optimization\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language\\nis not well-organized. Another difficulty lies in language\\ncomplexity ambiguity. Language models often struggle when\\ndealing with specialized vocabulary or ambiguous abbrevi-\\nations with multiple meanings. For instance, they may not\\ndiscern whether “LLM” refers to large language model or a\\nMaster of Laws in a legal context.\\n1) Query Expansion: Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nMulti-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.\\nSub-Query. The process of sub-question planning represents\\nthe generation of the necessary sub-questions to contextualize\\nand fully answer the original question when combined. This\\nprocess of adding relevant context is, in principle, similar\\nto query expansion. Specifically, a complex question can be\\ndecomposed into a series of simpler sub-questions using the\\nleast-to-most prompting method [92].\\nChain-of-Verification(CoVe). The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing halluci-\\nnations. Validated expanded queries typically exhibit higher\\nreliability [93].\\n9\\n2) Query Transformation: The core concept is to retrieve\\nchunks based on a transformed query instead of the user’s\\noriginal query.\\nQuery Rewrite.The original queries are not always optimal\\nfor LLM retrieval, especially in real-world scenarios. There-\\nfore, we can prompt LLM to rewrite the queries. In addition to\\nusing LLM for query rewriting, specialized smaller language\\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\\nmentation of the query rewrite method in the Taobao, known\\nas BEQUE [9] has notably enhanced recall effectiveness for\\nlong-tail queries, resulting in a rise in GMV.\\nAnother query transformation method is to use prompt\\nengineering to let LLM generate a query based on the original\\nquery for subsequent retrieval. HyDE [11] construct hypothet-\\nical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.\\nUsing the Step-back Prompting method [10], the original\\nquery is abstracted to generate a high-level concept question\\n(step-back question). In the RAG system, both the step-back\\nquestion and the original query are used for retrieval, and both\\nthe results are utilized as the basis for language model answer\\ngeneration.\\n3) Query Routing: Based on varying queries, routing to\\ndistinct RAG pipeline,which is suitable for a versatile RAG\\nsystem designed to accommodate diverse scenarios.\\nMetadata Router/ Filter. The first step involves extracting\\nkeywords (entity) from the query, followed by filtering based\\non the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific\\napprach see Semantic Router 6. Certainly, a hybrid routing\\napproach can also be employed, combining both semantic and\\nmetadata-based methods for enhanced query routing.\\nD. Embedding\\nIn RAG, retrieval is achieved by calculating the similarity\\n(e.g. cosine similarity) between the embeddings of the ques-\\ntion and document chunks, where the semantic representation\\ncapability of embedding models plays a key role. This mainly\\nincludes a sparse encoder (BM25) and a dense retriever (BERT\\narchitecture Pre-training language models). Recent research\\nhas introduced prominent embedding models such as AngIE,\\nVoyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-\\nditionally, C-MTEB focuses on Chinese capability, covering\\n6 tasks and 35 datasets. There is no one-size-fits-all answer\\nto “which embedding model to use.” However, some specific\\nmodels are better suited for particular use cases.\\n1) Mix/hybrid Retrieval : Sparse and dense embedding\\napproaches capture different relevance features and can ben-\\nefit from each other by leveraging complementary relevance\\ninformation. For instance, sparse retrieval models can be used\\n6https://github.com/aurelio-labs/semantic-router\\n7https://huggingface.co/spaces/mteb/leaderboard\\nto provide initial search results for training dense retrieval\\nmodels. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval\\nmodels can enhance the zero-shot retrieval capability of dense\\nretrieval models and assist dense retrievers in handling queries\\ncontaining rare entities, thereby improving robustness.\\n2) Fine-tuning Embedding Model: In instances where the\\ncontext significantly deviates from pre-training corpus, partic-\\nularly within highly specialized disciplines such as healthcare,\\nlegal practice, and other sectors replete with proprietary jargon,\\nfine-tuning the embedding model on your own domain dataset\\nbecomes essential to mitigate such discrepancies.\\nIn addition to supplementing domain knowledge, another\\npurpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).\\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\\ngenerator to create task-specific retrievers, addressing chal-\\nlenges in supervised fine-tuning, particularly in data-scarce\\ndomains. Another approach, LLM-Embedder [97], exploits\\nLLMs to generate reward signals across multiple downstream\\ntasks. The retriever is fine-tuned with two types of supervised\\nsignals: hard labels for the dataset and soft rewards from\\nthe LLMs. This dual-signal approach fosters a more effective\\nfine-tuning process, tailoring the embedding model to diverse\\ndownstream applications. REPLUG [72] utilizes a retriever\\nand an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and\\neffective training method enhances the performance of the\\nretrieval model by using an LM as the supervisory signal,\\neliminating the need for specific cross-attention mechanisms.\\nMoreover, inspired by RLHF (Reinforcement Learning from\\nHuman Feedback), utilizing LM-based feedback to reinforce\\nthe retriever through reinforcement learning.\\nE. Adapter\\nFine-tuning models may present challenges, such as in-\\ntegrating functionality through an API or addressing con-\\nstraints arising from limited local computational resources.\\nConsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool\\nthat are suitable for a given zero-shot task input. AAR\\n(Augmentation-Adapted Retriver) [47] introduces a universal\\nadapter designed to accommodate multiple downstream tasks.\\nWhile PRCA [69] add a pluggable reward-driven contextual\\nadapter to enhance performance on specific tasks. BGM [26]\\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\\nmodel in between. The bridge model aims to transform the\\nretrieved information into a format that LLMs can work with\\neffectively, allowing it to not only rerank but also dynami-\\ncally select passages for each query, and potentially employ\\nmore advanced strategies like repetition. Furthermore, PKG\\n10\\nintroduces an innovative method for integrating knowledge\\ninto white-box models via directive fine-tuning [75]. In this\\napproach, the retriever module is directly substituted to gen-\\nerate relevant documents according to a query. This method\\nassists in addressing the difficulties encountered during the\\nfine-tuning process and enhances model performance.\\nIV. GENERATION\\nAfter retrieval, it is not a good practice to directly input all\\nthe retrieved information to the LLM for answering questions.\\nFollowing will introduce adjustments from two perspectives:\\nadjusting the retrieved content and adjusting the LLM.\\nA. Context Curation\\nRedundant information can interfere with the final gener-\\nation of LLM, and overly long contexts can also lead LLM\\nto the “Lost in the middle” problem [98]. Like humans, LLM\\ntends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.\\n1) Reranking: Reranking fundamentally reorders document\\nchunks to highlight the most pertinent results first, effectively\\nreducing the overall document pool, severing a dual purpose\\nin information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many\\nrelevant documents as possible and concatenating them to form\\na lengthy retrieval prompt is beneficial. However, excessive\\ncontext can introduce more noise, diminishing the LLM’s\\nperception of key information .\\n(Long) LLMLingua [100], [101] utilize small language\\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\\ndetect and remove unimportant tokens, transforming it into\\na form that is challenging for humans to comprehend but\\nwell understood by LLMs. This approach presents a direct\\nand practical method for prompt compression, eliminating the\\nneed for additional training of LLMs while balancing language\\nintegrity and compression ratio. PRCA tackled this issue by\\ntraining an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data\\npoint consists of one positive sample and five negative sam-\\nples, and the encoder undergoes training using contrastive loss\\nthroughout this process [102] .\\nIn addition to compressing the context, reducing the num-\\nber of documents aslo helps improve the accuracy of the\\nmodel’s answers. Ma et al. [103] propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and SLMs.\\nIn this paradigm, SLMs serve as filters, while LLMs function\\nas reordering agents. The research shows that instructing\\nLLMs to rearrange challenging samples identified by SLMs\\nleads to significant improvements in various Information\\nExtraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the\\nLLM to filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [104], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nB. LLM Fine-tuning\\nTargeted fine-tuning based on the scenario and data char-\\nacteristics on LLMs can yield better results. This is also one\\nof the greatest advantages of using on-premise LLMs. When\\nLLMs lack data in a specific domain, additional knowledge can\\nbe provided to the LLM through fine-tuning. Huggingface’s\\nfine-tuning data can also be used as an initial step.\\nAnother benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-\\nticular style as instructed [37]. For retrieval tasks that engage\\nwith structured data, the SANTA framework [76] implements\\na tripartite training regimen to effectively encapsulate both\\nstructural and semantic nuances. The initial phase focuses on\\nthe retriever, where contrastive learning is harnessed to refine\\nthe query and document embeddings.\\nAligning LLM outputs with human or retriever preferences\\nthrough reinforcement learning is a potential approach. For\\ninstance, manually annotating the final generated answers\\nand then providing feedback through reinforcement learning.\\nIn addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to\\npowerful proprietary models or larger parameter open-source\\nmodels, a simple and effective method is to distill the more\\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\\nbe coordinated with fine-tuning of the retriever to align pref-\\nerences. A typical approach, such as RA-DIT [27], aligns the\\nscoring functions between Retriever and Generator using KL\\ndivergence.\\nV. AUGMENTATION PROCESS IN RAG\\nIn the domain of RAG, the standard practice often involves\\na singular (once) retrieval step followed by generation, which\\ncan lead to inefficiencies and sometimes is typically insuffi-\\ncient for complex problems demanding multi-step reasoning,\\nas it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.\\nA. Iterative Retrieval\\nIterative retrieval is a process where the knowledge base\\nis repeatedly searched based on the initial query and the text\\ngenerated so far, providing a more comprehensive knowledge\\n11\\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\\nbase for LLMs. This approach has been shown to enhance\\nthe robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-\\ntinuity and the accumulation of irrelevant information. ITER-\\nRETGEN [14] employs a synergistic approach that lever-\\nages “retrieval-enhanced generation” alongside “generation-\\nenhanced retrieval” for tasks that necessitate the reproduction\\nof specific information. The model harnesses the content\\nrequired to address the input task as a contextual basis for\\nretrieving pertinent knowledge, which in turn facilitates the\\ngeneration of improved responses in subsequent iterations.\\nB. Recursive Retrieval\\nRecursive retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.\\nThe process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\\nthe retrieval process and refines the CoT with the obtained\\nretrieval results. ToC [57] creates a clarification tree that\\nsystematically optimizes the ambiguous parts in the Query. It\\ncan be particularly useful in complex search scenarios where\\nthe user’s needs are not entirely clear from the outset or where\\nthe information sought is highly specialized or nuanced. The\\nrecursive nature of the process allows for continuous learning\\nand adaptation to the user’s requirements, often resulting in\\nimproved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as seen\\nin model agents like AutoGPT, Toolformer, and Graph-\\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\\nvides its retrieval process into distinct steps where LLMs\\nproactively use retrievers, apply Self-Ask techniques, and em-\\nploy few-shot prompts to initiate search queries. This proactive\\nstance allows LLMs to decide when to search for necessary\\ninformation, akin to how an agent utilizes tools.\\nWebGPT [110] integrates a reinforcement learning frame-\\nwork to train the GPT-3 model in autonomously using a\\nsearch engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby\\nexpanding GPT-3’s capabilities through the use of external\\nsearch engines. Flare automates timing retrieval by monitoring\\nthe confidence of the generation process, as indicated by the\\n12\\nprobability of generated terms [24]. When the probability falls\\nbelow a certain threshold would activates the retrieval system\\nto collect relevant information, thus optimizing the retrieval\\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\\nthe model to introspect its outputs. These tokens come in\\ntwo varieties: “retrieve” and “critic”. The model autonomously\\ndecides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional\\nclassifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the\\nmodel’s autonomous judgment capabilities in generating ac-\\ncurate responses.\\nVI. TASK AND EVALUATION\\nThe rapid advancement and growing adoption of RAG\\nin the field of NLP have propelled the evaluation of RAG\\nmodels to the forefront of research in the LLMs community.\\nThe primary objective of this evaluation is to comprehend\\nand optimize the performance of RAG models across diverse\\napplication scenarios.This chapter will mainly introduce the\\nmain downstream tasks of RAG, datasets, and how to evaluate\\nRAG systems.\\nA. Downstream Task\\nThe core task of RAG remains Question Answering (QA),\\nincluding\\ntraditional\\nsingle-hop/multi-hop\\nQA,\\nmultiple-\\nchoice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-\\nmation Extraction (IE), dialogue generation, code search, etc.\\nThe main downstream tasks of RAG and their corresponding\\ndatasets are summarized in Table II.\\nB. Evaluation Target\\nHistorically, RAG models assessments have centered on\\ntheir execution in specific downstream tasks. These evaluations\\nemploy established metrics suitable to the tasks at hand. For\\ninstance, question answering evaluations might rely on EM\\nand F1 scores [7], [45], [59], [72], whereas fact-checking\\ntasks often hinge on Accuracy as the primary metric [4],\\n[14], [42]. BLEU and ROUGE metrics are also commonly\\nused to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-\\nspecific metrics [160]. Despite this, there is a notable paucity\\nof research dedicated to evaluating the distinct characteristics\\nof RAG models.The main evaluation objectives include:\\nRetrieval Quality. Evaluating the retrieval quality is crucial\\nfor determining the effectiveness of the context sourced by\\nthe retriever component. Standard metrics from the domains\\nof search engines, recommendation systems, and information\\nretrieval systems are employed to measure the performance of\\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [161], [162].\\nGeneration Quality. The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation\\ncan be categorized based on the content’s objectives: unlabeled\\nand labeled content. For unlabeled content, the evaluation\\nencompasses the faithfulness, relevance, and non-harmfulness\\nof the generated answers. In contrast, for labeled content,\\nthe focus is on the accuracy of the information produced by\\nthe model [161]. Additionally, both retrieval and generation\\nquality assessments can be conducted through manual or\\nautomatic evaluation methods [29], [161], [163].\\nC. Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-\\nvance, answer faithfulness, and answer relevance. These qual-\\nity scores evaluate the efficiency of the RAG model from\\ndifferent perspectives in the process of information retrieval\\nand generation [164]–[166].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers\\nremain true to the retrieved context, maintaining consistency\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing\\nthe core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:\\nnoise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency in\\nsynthesizing information from multiple documents to address\\ncomplex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the\\nquality of generation.\\n13\\nTABLE II\\nDOWNSTREAM TASKS AND DATASETS OF RAG\\nTask\\nSub Task\\nDataset\\nMethod\\nQA\\nSingle-hop\\nNatural Qustion(NQ) [111]\\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\\n[20], [44], [72]\\nTriviaQA(TQA) [113]\\n[13], [30], [34], [45], [50], [64]\\n[4], [27], [59], [62], [112]\\n[22], [25], [43], [44], [71], [72]\\nSQuAD [114]\\n[20], [23], [30], [32], [45], [69], [112]\\nWeb Questions(WebQ) [115]\\n[3], [4], [13], [30], [50], [68]\\nPopQA [116]\\n[7], [25], [67]\\nMS MARCO [117]\\n[4], [40], [52]\\nMulti-hop\\nHotpotQA [118]\\n[23], [26], [31], [34], [47], [51], [61], [82]\\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\\n2WikiMultiHopQA [119]\\n[14], [24], [48], [59], [61], [91]\\nMuSiQue [120]\\n[14], [51], [61], [91]\\nLong-form QA\\nELI5 [121]\\n[27], [34], [43], [49], [51]\\nNarrativeQA(NQA) [122]\\n[45], [60], [63], [123]\\nASQA [124]\\n[24], [57]\\nQMSum(QM) [125]\\n[60], [123]\\nDomain QA\\nQasper [126]\\n[60], [63]\\nCOVID-QA [127]\\n[35], [46]\\nCMB [128],MMCU Medical [129]\\n[81]\\nMulti-Choice QA\\nQuALITY [130]\\n[60], [63]\\nARC [131]\\n[25], [67]\\nCommonsenseQA [132]\\n[58], [66]\\nGraph QA\\nGraphQA [84]\\n[84]\\nDialog\\nDialog Generation\\nWizard of Wikipedia (WoW) [133]\\n[13], [27], [34], [42]\\nPersonal Dialog\\nKBP [134]\\n[74], [135]\\nDuleMon [136]\\n[74]\\nTask-oriented Dialog\\nCamRest [137]\\n[78], [79]\\nRecommendation\\nAmazon(Toys,Sport,Beauty) [138]\\n[39], [40]\\nIE\\nEvent Argument Extraction\\nWikiEvent [139]\\n[13], [27], [37], [42]\\nRAMS [140]\\n[36], [37]\\nRelation Extraction\\nT-REx [141],ZsRE [142]\\n[27], [51]\\nReasoning\\nCommonsense Reasoning\\nHellaSwag [143]\\n[20], [66]\\nCoT Reasoning\\nCoT Reasoning [144]\\n[27]\\nComplex Reasoning\\nCSQA [145]\\n[55]\\nOthers\\nLanguage Understanding\\nMMLU [146]\\n[7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling\\nWikiText-103 [147]\\n[5], [29], [64], [71]\\nStrategyQA [148]\\n[14], [24], [48], [51], [55], [58]\\nFact Checking/Verification\\nFEVER [149]\\n[4], [13], [27], [34], [42], [50]\\nPubHealth [150]\\n[25], [67]\\nText Generation\\nBiography [151]\\n[67]\\nText Summarization\\nWikiASP [152]\\n[24]\\nXSum [153]\\n[17]\\nText Classification\\nVioLens [154]\\n[19]\\nTREC [155]\\n[33]\\nSentiment\\nSST-2 [156]\\n[20], [33], [38]\\nCode Search\\nCodeSearchNet [157]\\n[76]\\nRobustness Evaluation\\nNoMIRACL [56]\\n[56]\\nMath\\nGSM8K [158]\\n[73]\\nMachine Translation\\nJRC-Acquis [159]\\n[17]\\n14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance\\nFaithfulness\\nAnswer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nEM\\n✓\\nRecall\\n✓\\nPrecision\\n✓\\n✓\\nR-Rate\\n✓\\nCosine Similarity\\n✓\\nHit Rate\\n✓\\nMRR\\n✓\\nNDCG\\n✓\\nBLEU\\n✓\\n✓\\n✓\\nROUGE/ROUGE-L\\n✓\\n✓\\n✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these\\nmetrics, derived from related work, are traditional measures\\nand do not yet represent a mature or standardized approach for\\nquantifying RAG evaluation aspects. Custom metrics tailored\\nto the nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\nD. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD\\n[167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. DISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original\\nreferences for LLMs to help users verify the generated an-\\nswers. The entire retrieval and reasoning process is observable,\\nwhile generation solely relying on long context remains a\\nblack box. Conversely, the expansion of context provides new\\nopportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Determining the optimal integration of RAG and\\nfine-tuning whether sequential, alternating, or through end-to-\\nend joint training—and how to harness both parameterized\\n15\\nTABLE IV\\nSUMMARY OF EVALUATION FRAMEWORKS\\nEvaluation Framework\\nEvaluation Targets\\nEvaluation Aspects\\nQuantitative Metrics\\nRGB†\\nRetrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL†\\nGeneration Quality\\nCounterfactual Robustness\\nR-Rate (Reappearance Rate)\\nRAGAS‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\nCRUD†\\nRetrieval Quality\\nGeneration Quality\\nCreative Generation\\nKnowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific\\nfunctionalities into RAG and fine-tuned by the results of RAG\\nsystem. For example, CRAG [67] trains a lightweight retrieval\\nevaluator to assess the overall quality of the retrieved docu-\\nments for a query and triggers different knowledge retrieval\\nactions based on confidence levels.\\nD. Scaling laws of RAG\\nEnd-to-end RAG models and pre-trained models based\\non\\nRAG\\nare\\nstill\\none\\nof\\nthe\\nfocuses\\nof\\ncurrent\\nre-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for\\nLLMs, their applicability to RAG remains uncertain. Initial\\nstudies like RETRO++ [44] have begun to address this, yet the\\nparameter count in RAG models still lags behind that of LLMs.\\nThe possibility of an Inverse Scaling Law 10, where smaller\\nmodels outperform larger ones, is particularly intriguing and\\nmerits further investigation.\\nE. Production-Ready RAG\\nRAG’s practicality and alignment with engineering require-\\nments have facilitated its adoption. However, enhancing re-\\ntrieval efficiency, improving document recall in large knowl-\\nedge bases, and ensuring data security—such as preventing\\n10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra\\n12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/\\n16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video. The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by\\nincorporating external, offline strategies for voice-to-text con-\\nversion [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.\\nVid2Seq augments language models with specialized temporal\\nmarkers, facilitating the prediction of event boundaries and\\ntextual descriptions within a unified output sequence [181].\\nCode. RBPS [182] excels in small-scale learning tasks by\\nretrieving code examples that align with developers’ objectives\\nthrough encoding and frequency analysis. This approach has\\ndemonstrated efficacy in tasks such as test assertion genera-\\ntion and program repair. For structured knowledge, the CoK\\nmethod [106] first extracts facts pertinent to the input query\\nfrom a knowledge graph, then integrates these facts as hints\\nwithin the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.\\n17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep\\npace with its evolution. Ensuring accurate and representative\\nperformance assessments is crucial for fully capturing RAG’s\\ncontributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning.\\nPMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158, 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,”\\nin International conference on machine learning.\\nPMLR, 2022, pp.\\n2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nneural information processing systems, vol. 35, pp. 27 730–27 744,\\n2022.\\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[8] I.\\nILIN,\\n“Advanced\\nrag\\ntechniques:\\nan\\nil-\\nlustrated\\noverview,”\\nhttps://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\\n“Large language model based long-tail query rewriting in taobao\\nsearch,” arXiv preprint arXiv:2311.03758, 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117, 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496, 2022.\\n[12] V. Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\\nsityranker and lostinthemiddleranker,” https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\\n[13] W. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nand M. Jiang, “Generate rather than retrieve: Large language models\\nare strong context generators,” arXiv preprint arXiv:2209.10063, 2022.\\n[14] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294, 2023.\\n[15] X. Wang, Q. Yang, Y. Qiu, J. Liang, Q. He, Z. Gu, Y. Xiao,\\nand W. Wang, “Knowledgpt: Enhancing large language models with\\nretrieval and storage access on knowledge bases,” arXiv preprint\\narXiv:2308.11761, 2023.\\n[16] A.\\nH.\\nRaudaschl,\\n“Forget\\nrag,\\nthe\\nfuture\\nis\\nrag-fusion,”\\nhttps://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437, 2023.\\n[18] S. Wang, Y. Xu, Y. Fang, Y. Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple\\nand effective method by retrieving from training data,” arXiv preprint\\narXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint\\narXiv:2311.06595, 2023.\\n[20] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun,\\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\\nfor improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,\\n2023.\\n[21] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\\nK. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval\\nfrom 8 examples,” arXiv preprint arXiv:2209.11755, 2022.\\n[22] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, “Recitation-augmented\\nlanguage models,” arXiv preprint arXiv:2210.01296, 2022.\\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.\\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\narXiv preprint arXiv:2305.06983, 2023.\\n[25] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag:\\nLearning to retrieve, generate, and critique through self-reflection,”\\narXiv preprint arXiv:2310.11511, 2023.\\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\\n“Bridging the preference gap between retrievers and llms,” arXiv\\npreprint arXiv:2401.06954, 2024.\\n[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-\\naugmented dual instruction tuning,” arXiv preprint arXiv:2310.01352,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\\nretrieval? comparing knowledge injection in llms,” arXiv preprint\\narXiv:2312.05934, 2023.\\n[29] T. Lan, D. Cai, Y. Wang, H. Huang, and X.-L. Mao, “Copy is all\\nyou need,” in The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\\nuse?” arXiv preprint arXiv:2312.06648, 2023.\\n[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware\\nmulti-hop evidence retrieval,” arXiv preprint arXiv:2311.02616, 2023.\\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y. Li, and N. Cam-Tu,\\n“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503, 2023.\\n[33] Z. Guo, S. Cheng, Y. Wang, P. Li, and Y. Liu, “Prompt-guided re-\\ntrieval augmentation for non-knowledge-intensive tasks,” arXiv preprint\\narXiv:2305.17653, 2023.\\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\\nto filter context for retrieval-augmented generation,” arXiv preprint\\narXiv:2311.08377, 2023.\\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\\ndata augmentation for low-resource domain tasks,” arXiv preprint\\narXiv:2402.13482, 2024.\\n[36] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is not\\na good few-shot information extractor, but a good reranker for hard\\nsamples!” arXiv preprint arXiv:2303.08559, 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.\\n[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context\\nexamples for large language models,” arXiv preprint arXiv:2307.07164,\\n2023.\\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\\nL. Hong, Y. Tay, V. Q. Tran, J. Samost et al., “Recommender systems\\nwith generative retrieval,” arXiv preprint arXiv:2305.05065, 2023.\\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\\nY. Li, H. Lu et al., “Language models as semantic indexers,” arXiv\\npreprint arXiv:2310.07815, 2023.\\n[41] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\\nzaro, “Raven: In-context learning with retrieval augmented encoder-\\ndecoder language models,” arXiv preprint arXiv:2308.07922, 2023.\\n18\\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\\nO. Kuchaiev, B. Li, C. Xiao et al., “Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study,” arXiv preprint\\narXiv:2304.06762, 2023.\\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\\nzaro, “Instructretro: Instruction tuning post retrieval-augmented pre-\\ntraining,” arXiv preprint arXiv:2310.07713, 2023.\\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\\naugmented generation (rag) models for open domain question answer-\\ning,” Transactions of the Association for Computational Linguistics,\\nvol. 11, pp. 1–17, 2023.\\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\\ntrieval augmentation for long-form question answering,” arXiv preprint\\narXiv:2310.12150, 2023.\\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\\nEnhancing robustness in retrieval-augmented language models,” arXiv\\npreprint arXiv:2311.09210, 2023.\\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-\\nchain: Towards accurate, credible and traceable large language models\\nfor knowledgeintensive tasks,” CoRR, vol. abs/2304.14732, 2023.\\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682, 2023.\\n[53] J. L´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\\nscientific research,” arXiv preprint arXiv:2312.07559, 2023.\\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and\\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\\ning reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing, 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\\n“Nomiracl: Knowing when you don’t know for robust multilingual\\nretrieval-augmented generation,” arXiv preprint arXiv:2312.11361,\\n2023.\\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696, 2023.\\n[58] Y. Wang, P. Li, M. Sun, and Y. Liu, “Self-knowledge guided\\nretrieval augmentation for large language models,” arXiv preprint\\narXiv:2310.05002, 2023.\\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025,\\n2023.\\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509, 2022.\\n[62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\\nof large language models with retrieval augmentation,” arXiv preprint\\narXiv:2307.11019, 2023.\\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059, 2024.\\n[64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, and Y. Shoham, “In-context retrieval-augmented language\\nmodels,” arXiv preprint arXiv:2302.00083, 2023.\\n[65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\\nsample: Document-level event argument extraction via hybrid retrieval\\naugmentation,” in Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\n2023, pp. 293–306.\\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\\nzero-shot semi-parametric language models from multiple tasks,” arXiv\\npreprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884, 2024.\\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\\ngeneration and evidence retrieval,” arXiv preprint arXiv:2310.16568,\\n2023.\\n[69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:\\nFitting black-box large language models for retrieval question answer-\\ning via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\\nlanguage models are strong zero-shot query likelihood models for\\ndocument ranking,” arXiv preprint arXiv:2310.13243, 2023.\\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\\nlms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652, 2023.\\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\\ntionale memory for retrieval augmented generation,” arXiv preprint\\narXiv:2311.04177, 2023.\\n[74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi,\\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\\nretrieval-augmented generation for personalized dialogue systems,”\\narXiv preprint arXiv:2401.13256, 2024.\\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\\n“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757, 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, “Structure-\\naware language model pretraining improves dense retrieval on struc-\\ntured data,” arXiv preprint arXiv:2305.19912, 2023.\\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\\ngraph-augmented language models for knowledge-grounded dialogue\\ngeneration,” arXiv preprint arXiv:2305.18846, 2023.\\n[78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-\\ngeneration alignment for end-to-end task-oriented dialogue system,”\\narXiv preprint arXiv:2310.08877, 2023.\\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\\nknowledge retrieval for task-oriented dialogue systems,” arXiv preprint\\narXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.\\n[81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang,\\nH. Ding, X. Chu, J. Zhao et al., “Think and retrieval: A hypothesis\\nknowledge graph enhanced medical large language models,” arXiv\\npreprint arXiv:2312.15883, 2023.\\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\\nand interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun,\\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,” arXiv preprint\\narXiv:2402.07630, 2024.\\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\\nX. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language\\nand commands into one gpt,” arXiv preprint arXiv:2307.08674, 2023.\\n[86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, “Iseeq: Information\\nseeking question generation using dynamic meta-information retrieval\\nand knowledge graphs,” in Proceedings of the AAAI Conference on\\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.\\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch¨arli,\\nand D. Zhou, “Large language models can be easily distracted by\\nirrelevant context,” in International Conference on Machine Learning.\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R.\\nTeja,\\n“Evaluating\\nthe\\nideal\\nchunk\\nsize\\nfor\\na\\nrag\\nsystem\\nusing\\nllamaindex,”\\nhttps://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.\\n19\\n[89] Langchain, “Recursively split by character,” https://python.langchain.\\ncom/docs/modules/data connection/document transformers/recursive\\ntext splitter, 2023.\\n[90] S.\\nYang,\\n“Advanced\\nrag\\n01:\\nSmall-to-\\nbig\\nretrieval,”\\nhttps://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n[91] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730, 2023.\\n[92] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495, 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint\\narXiv:2309.12871, 2023.\\n[95] VoyageAI, “Voyage’s embedding models,” https://docs.voyageai.com/\\nembeddings/, 2023.\\n[96] BAAI,\\n“Flagembedding,”\\nhttps://github.com/FlagOpen/\\nFlagEmbedding, 2023.\\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y. Nie, “Retrieve anything\\nto augment large language models,” arXiv preprint arXiv:2310.07554,\\n2023.\\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\\nand P. Liang, “Lost in the middle: How language models use long\\ncontexts,” arXiv preprint arXiv:2307.03172, 2023.\\n[99] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524, 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track\\nand Government Track), J. Campbell, S. Larocca, J. Marciano,\\nK. Savenkov, and A. Yanishevsky, Eds.\\nOrlando, USA: Association\\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context\\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839,\\n2023.\\n[102] V. Karpukhin, B. O˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question\\nanswering,” arXiv preprint arXiv:2004.04906, 2020.\\n[103] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is\\nnot a good few-shot information extractor, but a good reranker for\\nhard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:257532405\\n[104] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092, 2023.\\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y. K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-\\nels with structured knowledge bases,” arXiv preprint arXiv:2305.13269,\\n2023.\\n[107] H.\\nYang,\\nS.\\nYue,\\nand\\nY.\\nHe,\\n“Auto-gpt\\nfor\\nonline\\ndecision\\nmaking:\\nBenchmarks\\nand\\nadditional\\nopinions,”\\narXiv\\npreprint\\narXiv:2306.02224, 2023.\\n[108] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\\n2023.\\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\\nsoning ability via prompt augmented by chatgpt,” arXiv preprint\\narXiv:2304.11116, 2023.\\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics, vol. 7, pp. 453–466,\\n2019.\\n[112] Y. Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y. Zhou,\\n“Exploring the integration strategies of retriever and large language\\nmodels,” arXiv preprint arXiv:2308.12574, 2023.\\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551, 2017.\\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions\\nfor\\nmachine\\ncomprehension\\nof\\ntext,”\\narXiv\\npreprint\\narXiv:1606.05250, 2016.\\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.\\n[118] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdi-\\nnov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.\\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a\\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,”\\narXiv preprint arXiv:2011.01060, 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics, vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190,\\n2019.\\n[122] T. Koˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y. Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\\nquestions meet long-form answers,” arXiv preprint arXiv:2204.06092,\\n2022.\\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\\nAwadallah, A. Celikyilmaz, Y. Liu, X. Qiu et al., “Qmsum: A new\\nbenchmark for query-based multi-domain meeting summarization,”\\narXiv preprint arXiv:2104.05938, 2021.\\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011, 2021.\\n[127] T. M¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID), 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\\nJ. Li, X. Wan, B. Wang et al., “Cmb: A comprehensive medical\\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833, 2023.\\n[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\\npreprint arXiv:2304.12986, 2023.\\n[130] R. Y. Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V. Pad-\\nmakumar, J. Ma, J. Thompson, H. He et al., “Quality: Question an-\\nswering with long input texts, yes!” arXiv preprint arXiv:2112.08608,\\n2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457, 2018.\\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937, 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241, 2018.\\n[134] H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source\\n20\\nplanner for personalized knowledge-grounded dialogue,” arXiv preprint\\narXiv:2310.08840, 2023.\\n[135] ——, “Large language models as source planner for personal-\\nized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,\\n2023.\\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, and S. Wang,\\n“Long time no see! open-domain conversation with long-term persona\\nmemory,” arXiv preprint arXiv:2203.05797, 2022.\\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\\nSu, S. Ultes, D. Vandyke, and S. Young, “Conditional generation\\nand snapshot learning in neural dialogue systems,” arXiv preprint\\narXiv:1606.03352, 2016.\\n[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution\\nof fashion trends with one-class collaborative filtering,” in proceedings\\nof the 25th international conference on world wide web, 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\\nby conditional generation,” arXiv preprint arXiv:2104.05919, 2021.\\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-\\nsentence argument linking,” arXiv preprint arXiv:1911.03766, 2019.\\n[141] H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\\nand E. Simperl, “T-rex: A large scale alignment of natural language\\nwith knowledge base triples,” in Proceedings of the Eleventh Inter-\\nnational Conference on Language Resources and Evaluation (LREC\\n2018), 2018.\\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\\ntraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\\n2017.\\n[143] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.\\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\\n“The cot collection: Improving zero-shot and few-shot learning of\\nlanguage models via chain-of-thought fine-tuning,” arXiv preprint\\narXiv:2305.14045, 2023.\\n[145] A. Saha, V. Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\\n“Complex sequential question answering: Towards learning to converse\\nover linked question answer pairs with a knowledge graph,” in Proceed-\\nings of the AAAI conference on artificial intelligence, vol. 32, no. 1,\\n2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300, 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843, 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926, 2020.\\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.\\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\\nand G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based\\nsummarization,” Transactions of the Association for Computational\\nLinguistics, vol. 9, pp. 211–225, 2021.\\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary! topic-aware convolutional neural networks for ex-\\ntreme summarization,” arXiv preprint arXiv:1808.08745, 2018.\\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\\nS. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms\\nof communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.\\n[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:\\nThe 19th International Conference on Computational Linguistics, 2002.\\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference on\\nempirical methods in natural language processing, 2013, pp. 1631–\\n1642.\\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436, 2019.\\n[158] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\\nand D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus\\nwith 20+ languages,” arXiv preprint cs/0609058, 2006.\\n[160] Y. Hoshi, D. Miyashita, Y. Ng, K. Tatsuno, Y. Morioka, O. Torii,\\nand J. Deguchi, “Ralle: A framework for developing and eval-\\nuating retrieval-augmented large language models,” arXiv preprint\\narXiv:2308.10633, 2023.\\n[161] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”\\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476, 2023.\\n[166] C.\\nJarvis\\nand\\nJ.\\nAllard,\\n“A\\nsurvey\\nof\\ntechniques\\nfor\\nmaximizing\\nllm\\nperformance,”\\nhttps://community.openai.\\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.\\n[168] Y. Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\\nX. Sun, “Recall: A benchmark for llms robustness against external\\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147, 2023.\\n[169] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025,\\n2023.\\n[171] C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.\\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\\nGonzalez, “Raft: Adapting language model to domain specific rag,”\\narXiv preprint arXiv:2403.10131, 2024.\\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-\\nsymbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning.\\nPMLR, 2022, pp.\\n468–485.\\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\\nmodal language modeling,” arXiv preprint arXiv:2211.12561, 2022.\\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\\nimage pre-training with frozen image encoders and large language\\nmodels,” arXiv preprint arXiv:2301.12597, 2023.\\n[178] W. Zhu, A. Yan, Y. Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y.\\nWang, “Visualize before you write: Imagination-guided open-ended\\ntext generation,” arXiv preprint arXiv:2210.03765, 2022.\\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\\nspokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external\\noff-policy speech-to-text mappings in contextual end-to-end automated\\nspeech recognition,” arXiv preprint arXiv:2301.02736, 2023.\\n21\\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\\nJ. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual\\nlanguage model for dense video captioning,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2023, pp. 10 714–10 726.\\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering (ICSE), 2023, pp.\\n2450–2462.\\n']...\n",
      "Similarity: [29.253265380859375, 29.731069564819336]\n",
      "--------------------------------------------------\n",
      "Document 1:\n",
      "Source: 2312.10997.pdf\n",
      "Content: ['Large Language Models: A Survey\\nShervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\\nRichard Socher, Xavier Amatriain, Jianfeng Gao\\nAbstract—Large Language Models (LLMs) have drawn a\\nlot of attention due to their strong performance on a wide\\nrange of natural language tasks, since the release of ChatGPT\\nin November 2022. LLMs’ ability of general-purpose language\\nunderstanding and generation is acquired by training billions of\\nmodel’s parameters on massive amounts of text data, as predicted\\nby scaling laws [1], [2]. The research area of LLMs, while very\\nrecent, is evolving rapidly in many different ways. In this paper,\\nwe review some of the most prominent LLMs, including three\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\\ncharacteristics, contributions and limitations. We also give an\\noverview of techniques developed to build, and augment LLMs.\\nWe then survey popular datasets prepared for LLM training,\\nfine-tuning, and evaluation, review widely used LLM evaluation\\nmetrics, and compare the performance of several popular LLMs\\non a set of representative benchmarks. Finally, we conclude\\nthe paper by discussing open challenges and future research\\ndirections.\\nI.\\nINTRODUCTION\\nLanguage modeling is a long-standing research topic, dat-\\ning back to the 1950s with Shannon’s application of informa-\\ntion theory to human language, where he measured how well\\nsimple n-gram language models predict or compress natural\\nlanguage text [3]. Since then, statistical language modeling\\nbecame fundamental to many natural language understanding\\nand generation tasks, ranging from speech recognition, ma-\\nchine translation, to information retrieval [4], [5], [6].\\nThe recent advances on transformer-based large language\\nmodels (LLMs), pretrained on Web-scale text corpora, signif-\\nicantly extended the capabilities of language models (LLMs).\\nFor example, OpenAI’s ChatGPT and GPT-4 can be used not\\nonly for natural language processing, but also as general task\\nsolvers to power Microsoft’s Co-Pilot systems, for instance,\\ncan follow human instructions of complex new tasks per-\\nforming multi-step reasoning when needed. LLMs are thus\\nbecoming the basic building block for the development of\\ngeneral-purpose AI agents or artificial general intelligence\\n(AGI).\\nAs the field of LLMs is moving fast, with new findings,\\nmodels and techniques being published in a matter of months\\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\\ntioners often find it challenging to figure out the best recipes\\nto build LLM-powered AI systems for their tasks. This paper\\ngives a timely survey of the recent advances on LLMs. We\\nhope this survey will prove a valuable and accessible resource\\nfor students, researchers and developers.\\nLLMs are large-scale, pre-trained, statistical language mod-\\nels based on neural networks. The recent success of LLMs is\\nan accumulation of decades of research and development of\\nlanguage models, which can be categorized into four waves\\nthat have different starting points and velocity: statistical lan-\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n −1 words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing, where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan-\\nguage models with recurrent neural networks [23] or trans-\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge\\nlanguage\\nmodels\\n(LLMs)\\nmainly\\nrefer\\nto\\ntransformer-based neural language models\\n1 that contain\\ntens to hundreds of billions of parameters, which are pre-\\ntrained on massive text data, such as PaLM [31], LLaMA\\n[32], and GPT-4 [33], as summarized in Table III. Compared\\n1Recently, several very promising non-transformer LLMs have been pro-\\nposed, such as the LLMs based on structured state space models [29], [30].\\nSee Section VII for more details.\\narXiv:2402.06196v2  [cs.CL]  20 Feb 2024\\nEmerging\\nBasic\\nAugmented\\nLLM Capabilities\\nReasoning\\nCoding\\nComprehension\\nMultilingual\\nTool\\nutilization\\nWorld\\nknowledge\\nInstruction\\nfollowing\\nIn-context\\nlearning\\nInteracting\\nwith users\\nSelf-improvement\\nMulti choice QA\\nWikipedia QA\\nXNLI\\nCrosslingual QA\\nCrosslingual Tasks\\nTranslation\\nReading Comprehension\\nMulti choice QA\\nBoolean QA\\nSimplification\\nSummarization\\nFunction Calling\\nAPI calling\\nLogical\\nSymbolic\\nCommon Sense\\nArithmetic\\nTurn based\\nCompletion\\nTask definition\\nFew-shot\\nSymbolic\\nreference\\nPos/Neg example\\nStep by step\\nsolving\\nTool planning\\nTask\\ndecomposition\\nVirtual acting\\nPhysical acting\\nKnowledge base\\nutilization\\nAssignment\\nplanning\\nSelf-cirtisim\\nSelf-refinement\\nFig. 1: LLM Capabilities.\\nto PLMs, LLMs are not only much larger in model size, but\\nalso exhibit stronger language understanding and generation\\nabilities, and more importantly, emergent abilities that are\\nnot present in smaller-scale language models. As illustrated\\nin Fig. 1, these emergent abilities include (1) in-context\\nlearning, where LLMs learn a new task from a small set\\nof examples presented in the prompt at inference time, (2)\\ninstruction following, where LLMs, after instruction tuning,\\ncan follow the instructions for new types of tasks without\\nusing explicit examples, and (3) multi-step reasoning, where\\nLLMs can solve a complex task by breaking down that task\\ninto intermediate reasoning steps as demonstrated in the\\nchain-of-thought prompt [34]. LLMs can also be augmented\\nby using external knowledge and tools [35], [36] so that\\nthey can effectively interact with users and environment [37],\\nand continually improve itself using feedback data collected\\nthrough interactions (e.g. via reinforcement learning with\\nhuman feedback (RLHF)).\\nThrough advanced usage and augmentation techniques,\\nLLMs can be deployed as so-called AI agents: artificial entities\\nthat sense their environment, make decisions, and take actions.\\nPrevious research has focused on developing agents for specific\\ntasks and domains. The emergent abilities demonstrated by\\nLLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with\\ndynamic environment. Therefore, LLM-based agents often\\nneed to augment LLMs to e.g., obtain updated information\\nfrom external knowledge bases, verify whether a system action\\nproduces the expected result, and cope with when things do\\nnot go as expected, etc. We will discuss in detail LLM-based\\nagents in Section IV.\\nIn the rest of this paper, Section II presents an overview of\\nstate of the art of LLMs, focusing on three LLM families (GPT,\\nLLaMA and PaLM) and other representative models. Section\\nIII discusses how LLMs are built. Section IV discusses how\\nLLMs are used, and augmented for real-world applications\\nSections V and VI review popular datasets and benchmarks for\\nevaluating LLMs, and summarize the reported LLM evaluation\\nresults. Finally, Section VII concludes the paper by summa-\\nrizing the challenges and future research directions.\\nII.\\nLARGE LANGUAGE MODELS\\nIn this section we start with a review of early pre-trained\\nneural language models as they are the base of LLMs, and\\nthen focus our discussion on three families of LLMs: GPT,\\nLlaMA, and PaLM. Table I provides an overview of some of\\nthese models and their characteristics.\\nA. Early Pre-trained Neural Language Models\\nLanguage modeling using neural networks was pioneered\\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\\nneural language models (NLMs) that are comparable to n-gram\\nmodels. Then, [14] successfully applied NLMs to machine\\ntranslation. The release of RNNLM (an open source NLM\\ntoolkit) by Mikolov [41], [42] helped significantly popularize\\nNLMs. Afterwards, NLMs based on recurrent neural networks\\n(RNNs) and their variants, such as long short-term memory\\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\\nused for many natural language applications including machine\\ntranslation, text generation and text classification [43].\\nThen, the invention of the Transformer architecture [44]\\nmarks another milestone in the development of NLMs. By\\napplying self-attention to compute in parallel for every word\\nin a sentence or document an “attention score” to model the\\ninfluence each word has on another, Transformers allow for\\nmuch more parallelization than RNNs, which makes it possible\\nto efficiently pre-train very big language models on large\\namounts of data on GPUs. These pre-trained language models\\n(PLMs) can be fine-tuned for many downstream tasks.\\nPaper Strcuture\\nEarly Pre-trained\\nLanguage Models\\nII\\nLarge Language Models\\nA\\nIII\\nHOW LLMS ARE BUILT\\nA\\nData Cleaning\\nB\\nLarge Language\\nModel Families\\nB\\nOther Representative\\nLLMs\\nC\\nDominant LLM\\nArchitectures\\nTokenizations\\nC\\nPositional Encoding\\nD\\nModel Pre-training\\nE\\nFine-tuning and\\nInstruction Tuning\\nF\\nAlignment\\nG\\nDecoding Strategies\\nH\\nI\\nHOW LLMS ARE USED AND AUGMENTED\\nA\\nB\\nLLM limitations\\nCost-Effective Training/Inference,\\nAdaptation & Compression\\nI\\nUsing LLMs: Prompt Design\\nand Engineering\\nC\\nAugmenting LLMs through\\nexternal knowledge - RAG\\nD\\nUsing External Tools\\nE\\nLLM Agents\\nV\\n\\xa0POPULAR DATASETS FOR LLMS\\nA\\nDatasets for Basic Tasks: language\\nmodeling/understanding/generation\\nB\\n\\xa0Datasets for Emergent: ICL, reasoning,\\ninstruction following\\nC\\nDatasets for Augmented: using\\nexternal knowledge/tools\\nVI\\n\\xa0PROMINENT LLMS’ PERFORMANCE\\nON BENCHMARKS\\nA\\nB\\nVII\\nCHALLENGES AND FUTURE DIRECTIONS\\nA\\nSmaller and more efficient\\nLanguage Models\\nLLMs’ Performance on Different Tasks\\nPopular Metrics for Evaluating LLMs\\nB\\nNew Post-attention\\nArchitectural Paradigms\\nC\\nMulti-modal Models\\nD\\nImproved LLM Usage and\\nAugmentation techniques\\nD\\nSecurity and\\nEthical/Responsible AI\\nFig. 2: The paper structure.\\nWe group early popular Transformer-based PLMs, based on\\ntheir neural architectures, into three main categories: encoder-\\nonly, decoder-only, and encoder-decoder models. Comprehen-\\nsive surveys of early PLMs are provided in [43], [28].\\n1) Encoder-only PLMs: As the name suggests, the encoder-\\nonly models only consist of an encoder network. These models\\nare originally developed for language understanding tasks,\\nsuch as text classification, where the models need to predict a\\nclass label for an input text. Representative encoder-only mod-\\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\\nDeBERTa, XLM, XLNet, UNILM, as to be described below.\\nBERT (Birectional Encoder Representations from Trans-\\nformers) [24] is one of the most widely used encoder-only\\nlanguage models. BERT consists of three modules: (1) an\\nembedding module that converts input text into a sequence\\nof embedding vectors, (2) a stack of Transformer encoders\\nthat converts embedding vectors into contextual representation\\nvectors, and (3) a fully connected layer that converts the\\nrepresentation vectors (at the final layer) to one-hot vectors.\\nBERT is pre-trained uses two objectives: masked language\\nmodeling (MLM) and next sentence prediction. The pre-trained\\nBERT model can be fine-tuned by adding a classifier layer\\nfor many language understanding tasks, ranging from text\\nTABLE I: High-level Overview of Popular Language Models\\nType\\nModel Name\\n#Parameters\\nRelease\\nBase Models\\nOpen\\nSource\\n#Tokens\\nTraining dataset\\nBERT\\n110M, 340M\\n2018\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nRoBERTa\\n355M\\n2019\\n-\\n✓\\n2.2T\\nBooksCorpus,\\nEnglish\\nWikipedia,\\nCC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-Only\\nALBERT\\n12M,\\n18M,\\n60M,\\n235M\\n2019\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nDeBERTa\\n-\\n2020\\n-\\n✓\\n-\\nBooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet\\n110M, 340M\\n2019\\n-\\n✓\\n32.89B\\nBooksCorpus, English Wikipedia, Giga5, Com-\\nmon Crawl, ClueWeb 2012-B\\nDecoder-only\\nGPT-1\\n120M\\n2018\\n-\\n✓\\n1.3B\\nBooksCorpus\\nGPT-2\\n1.5B\\n2019\\n-\\n✓\\n10B\\nReddit outbound\\nT5 (Base)\\n223M\\n2019\\n-\\n✓\\n156B\\nCommon Crawl\\nEncoder-Decoder\\nMT5 (Base)\\n300M\\n2020\\n-\\n✓\\n-\\nNew Common Crawl-based dataset in 101 lan-\\nguages (m Common Crawl)\\nBART (Base)\\n139M\\n2019\\n-\\n✓\\n-\\nCorrupting text\\nGPT-3\\n125M,\\n350M,\\n760M, 1.3B, 2.7B,\\n6.7B, 13B, 175B\\n2020\\n×\\n300B\\nCommon Crawl (filtered), WebText2, Books1,\\nBooks2, Wikipedia\\nGPT Family\\nCODEX\\n12B\\n2021\\nGPT\\n✓\\n-\\nPublic GitHub software repositories\\nWebGPT\\n760M, 13B, 175B\\n2021\\nGPT-3\\n×\\n-\\nELI5\\nGPT-4\\n1.76T\\n2023\\n-\\n×\\n13T\\n-\\nLLaMA1\\n7B, 13B, 33B, 65B\\n2023\\n-\\n✓\\n1T, 1.4T\\nOnline sources\\nLLaMA2\\n7B, 13B, 34B, 70B\\n2023\\n-\\n✓\\n2T\\nOnline sources\\nAlpaca\\n7B\\n2023\\nLLaMA1\\n✓\\n-\\nGPT-3.5\\nVicuna-13B\\n13B\\n2023\\nLLaMA1\\n✓\\n-\\nGPT-3.5\\nLLaMA Family\\nKoala\\n13B\\n2023\\nLLaMA\\n✓\\n-\\nDialogue data\\nMistral-7B\\n7.3B\\n2023\\n✓\\n-\\n-\\nCode Llama\\n34\\n2023\\nLLaMA2\\n✓\\n500B\\nPublicly available code\\nLongLLaMA\\n3B, 7B\\n2023\\nOpenLLaMA\\n✓\\n1T\\n-\\nLLaMA-Pro-8B\\n8.3B\\n2024\\nLLaMA2-7B\\n✓\\n80B\\nCode and math corpora\\nTinyLlama-1.1B\\n1.1B\\n2024\\nLLaMA1.1B\\n✓\\n3T\\nSlimPajama, Starcoderdata\\nPaLM\\n8B, 62B, 540B\\n2022\\n-\\n×\\n780B\\nWeb documents, books, Wikipedia, conversations,\\nGitHub code\\nU-PaLM\\n8B, 62B, 540B\\n2022\\n-\\n×\\n1.3B\\nWeb documents, books, Wikipedia, conversations,\\nGitHub code\\nPaLM Family\\nPaLM-2\\n340B\\n2023\\n-\\n✓\\n3.6T\\nWeb documents, books, code, mathematics, con-\\nversational data\\nMed-PaLM\\n540B\\n2022\\nPaLM\\n×\\n780B\\nHealthSearchQA, MedicationQA, LiveQA\\nMed-PaLM 2\\n-\\n2023\\nPaLM 2\\n×\\n-\\nMedQA, MedMCQA, HealthSearchQA, LiveQA,\\nMedicationQA\\nFLAN\\n137B\\n2021\\nLaMDA-PT\\n✓\\n-\\nWeb documents, code, dialog data, Wikipedia\\nGopher\\n280B\\n2021\\n-\\n×\\n300B\\nMassiveText\\nERNIE 4.0\\n10B\\n2023\\n-\\n×\\n4TB\\nChinese text\\nRetro\\n7.5B\\n2021\\n-\\n×\\n600B\\nMassiveText\\nLaMDA\\n137B\\n2022\\n-\\n×\\n168B\\npublic dialog data and web documents\\nChinChilla\\n70B\\n2022\\n-\\n×\\n1.4T\\nMassiveText\\nGalactia-120B\\n120B\\n2022\\n-\\n450B\\nOther Popular LLMs\\nCodeGen\\n16.1B\\n2022\\n-\\n✓\\n-\\nTHE PILE, BIGQUERY, BIGPYTHON\\nBLOOM\\n176B\\n2022\\n-\\n✓\\n366B\\nROOTS\\nZephyr\\n7.24B\\n2023\\nMistral-7B\\n✓\\n800B\\nSynthetic data\\nGrok-0\\n33B\\n2023\\n-\\n×\\n-\\nOnline source\\nORCA-2\\n13B\\n2023\\nLLaMA2\\n-\\n2001B\\n-\\nStartCoder\\n15.5B\\n2023\\n-\\n✓\\n35B\\nGitHub\\nMPT\\n7B\\n2023\\n-\\n✓\\n1T\\nRedPajama, m Common Crawl, S2ORC, Common\\nCrawl\\nMixtral-8x7B\\n46.7B\\n2023\\n-\\n✓\\n-\\nInstruction dataset\\nFalcon 180B\\n180B\\n2023\\n-\\n✓\\n3.5T\\nRefinedWeb\\nGemini\\n1.8B, 3.25B\\n2023\\n✓\\n-\\nWeb documents, books, and code, image data,\\naudio data, video data\\nDeepSeek-Coder\\n1.3B, 6.7B, 33B\\n2024\\n-\\n✓\\n2T\\nGitHub’s Markdown and StackExchange\\nDocLLM\\n1B,7B\\n2024\\n-\\n×\\n2T\\nIIT-CDIP Test Collection 1.0, DocBank\\nclassification, question answering to language inference. A\\nhigh-level overview of BERT framework is shown in Fig 3. As\\nBERT significantly improved state of the art on a wide range\\nof language understanding tasks when it was published, the AI\\ncommunity was inspired to develop many similar encoder-only\\nlanguage models based on BERT.\\nRoBERTa [25] significantly improves the robustness of\\nBERT using a set of model design choices and training strate-\\ngies, such as modifying a few key hyperparameters, removing\\nthe next-sentence pre-training objective and training with much\\nlarger mini-batches and learning rates. ALBERT [45] uses two\\nparameter-reduction techniques to lower memory consumption\\nand increase the training speed of BERT: (1) splitting the\\nembedding matrix into two smaller matrices, and (2) using\\nrepeating layers split among groups. DeBERTa (Decoding-\\nenhanced BERT with disentangled attention) [26] improves the\\nBERT and RoBERTa models using two novel techniques. The\\nfirst is the disentangled attention mechanism, where each word\\nis represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words\\nFig. 3: Overall pre-training and fine-tuning procedures for\\nBERT. Courtesy of [24]\\nare computed using disentangled matrices on their contents and\\nrelative positions, respectively. Second, an enhanced mask de-\\ncoder is used to incorporate absolute positions in the decoding\\nlayer to predict the masked tokens in model pre-training. In\\naddition, a novel virtual adversarial training method is used for\\nfine-tuning to improve models’ generalization. ELECTRA [46]\\nuses a new pre-training task, known as replaced token detection\\n(RTD), which is empirically proven to be more sample-efficient\\nthan MLM. Instead of masking the input, RTD corrupts it by\\nreplacing some tokens with plausible alternatives sampled from\\na small generator network. Then, instead of training a model\\nthat predicts the original identities of the corrupted tokens, a\\ndiscriminative model is trained to predict whether a token in\\nthe corrupted input was replaced by a generated sample or not.\\nRTD is more sample-efficient than MLM because the former\\nis defined over all input tokens rather than just the small subset\\nbeing masked out, as illustrated in Fig 4.\\nFig. 4: A comparison between replaced token detection and\\nmasked language modeling. Courtesy of [46].\\nXLMs [47] extended BERT to cross-lingual language\\nmodels using two methods: (1) a unsupervised method that\\nonly relies on monolingual data, and (2) a supervised method\\nthat leverages parallel data with a new cross-lingual language\\nmodel objective, as illustrated in Fig 5. XLMs had obtained\\nstate-of-the-art results on cross-lingual classification, unsuper-\\nvised and supervised machine translation, at the time they were\\nproposed.\\nThere are also encoder-only language models that leverage\\nthe advantages of auto-regressive (decoder) models for model\\ntraining and inference. Two examples are XLNet and UNILM.\\nXLNet [48] is based on Transformer-XL, pre-trained using a\\ngeneralized autoregressive method that enables learning bidi-\\nrectional contexts by maximizing the expected likelihood over\\nFig. 5: Cross-lingual language model pretraining. The MLM\\nobjective is similar to BERT, but with continuous streams\\nof text as opposed to sentence pairs. The TLM objective\\nextends MLM to pairs of parallel sentences. To predict a\\nmasked English word, the model can attend to both the English\\nsentence and its French translation, and is encouraged to align\\nEnglish and French representations. Courtesy of [47].\\nall permutations of the factorization order. UNILM (UNIfied\\npre-trained Language Model) [49] is pre-trained using three\\ntypes of language modeling tasks: unidirectional, bidirectional,\\nand sequence-to-sequence prediction. This is achieved by\\nemploying a shared Transformer network and utilizing specific\\nself-attention masks to control what context the prediction is\\nconditioned on, as illustrated in Fig 6. The pre-trained model\\ncan be fine-tuned for both natural language understanding and\\ngeneration tasks.\\nFig. 6: Overview of unified LM pre-training. The model\\nparameters are shared across the LM objectives (i.e., bidirec-\\ntional LM, unidirectional LM, and sequence-to-sequence LM).\\nCourtesy of [49].\\n2) Decoder-only PLMs: Two of the most widely used\\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\\nOpenAI. These models lay the foundation to more powerful\\nLLMs subsequently, i.e., GPT-3 and GPT-4.\\nGPT-1 [50] demonstrates for the first time that good\\nperformance over a wide range of natural language tasks can be\\nobtained by Generative Pre-Training (GPT) of a decoder-only\\nTransformer model on a diverse corpus of unlabeled text in a\\nself-supervised learning fashion (i.e., next word/token predic-\\ntion), followed by discriminative fine-tuning on each specific\\ndownstream task (with much fewer samples), as illustrated in\\nFig 7. GPT-1 paves the way for subsequent GPT models, with\\neach version improving upon the architecture and achieving\\nbetter performance on various language tasks.\\nFig. 7: High-level overview of GPT pretraining, and fine-tuning\\nsteps. Courtesy of OpenAI.\\nGPT-2 [51] shows that language models are able to learn\\nto perform specific natural language tasks without any explicit\\nsupervision when trained on a large WebText dataset consisting\\nof millions of webpages. The GPT-2 model follows the model\\ndesigns of GPT-1 with a few modifications: Layer normal-\\nization is moved to the input of each sub-block, additional\\nlayer normalization is added after the final self-attention block,\\ninitialization is modified to account for the accumulation on\\nthe residual path and scaling the weights of residual layers,\\nvocabulary size is expanded to 50,25, and context size is\\nincreased from 512 to 1024 tokens.\\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\\nalmost all NLP tasks can be cast as a sequence-to-sequence\\ngeneration task. Thus, an encoder-decoder language model, by\\ndesign, is a unified model in that it can perform all natural\\nlanguage understanding and generation tasks. Representative\\nencoder-decoder PLMs we will review below are T5, mT5,\\nMASS, and BART.\\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\\nwhere transfer learning is effectively exploited for NLP via an\\nintroduction of a unified framework in which all NLP tasks are\\ncast as a text-to-text generation task. mT5 [53] is a multilingual\\nvariant of T5, which is pre-trained on a new Common Crawl-\\nbased dataset consisting of texts in 101 languages.\\nMASS (MAsked Sequence to Sequence pre-training) [54]\\nadopts the encoder-decoder framework to reconstruct a sen-\\ntence fragment given the remaining part of the sentence. The\\nencoder takes a sentence with randomly masked fragment\\n(several consecutive tokens) as input, and the decoder predicts\\nthe masked fragment. In this way, MASS jointly trains the\\nencoder and decoder for language embedding and generation,\\nrespectively.\\nBART [55] uses a standard sequence-to-sequence transla-\\ntion model architecture. It is pre-trained by corrupting text with\\nan arbitrary noising function, and then learning to reconstruct\\nthe original text.\\nB. Large Language Model Families\\nLarge\\nlanguage\\nmodels\\n(LLMs)\\nmainly\\nrefer\\nto\\ntransformer-based\\nPLMs\\nthat\\ncontain\\ntens\\nto\\nhundreds\\nof billions of parameters. Compared to PLMs reviewed above,\\nLLMs are not only much larger in model size, but also exhibit\\nstronger language understanding and generation and emergent\\nabilities that are not present in smaller-scale models. In what\\nfollows, we review three LLM families: GPT, LLaMA, and\\nPaLM, as illustrated in Fig 8.\\n1) The GPT Family: Generative Pre-trained Transform-\\ners (GPT) are a family of decoder-only Transformer-based\\nlanguage models, developed by OpenAI. This family con-\\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\\nCODEX, and WebGPT. Although early GPT models, such as\\nGPT-1 and GPT-2, are open-source, recent models, such as\\nGPT-3 and GPT-4, are close-source and can only be accessed\\nvia APIs. GPT-1 and GPT-2 models have been discussed in\\nthe early PLM subsection. We start with GPT-3 below.\\nGPT-3 [56] is a pre-trained autoregressive language model\\nwith 175 billion parameters. GPT-3 is widely considered as\\nthe first LLM in that it not only is much larger than previous\\nPLMs, but also for the first time demonstrates emergent\\nabilities that are not observed in previous smaller PLMs. GPT-\\n3 shows the emergent ability of in-context learning, which\\nmeans GPT-3 can be applied to any downstream tasks without\\nany gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the\\nmodel. GPT-3 achieved strong performance on many NLP\\ntasks, including translation, question-answering, and the cloze\\ntasks, as well as several ones that require on-the-fly reasoning\\nor domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\\nperformance of GPT-3 as a function of the number of examples\\nin in-context prompts.\\nCODEX [57], released by OpenAI in March 2023, is a\\ngeneral-purpose programming model that can parse natural\\nlanguage and generate code in response. CODEX is a de-\\nscendant of GPT-3, fine-tuned for programming applications\\non code corpora collected from GitHub. CODEX powers\\nMicrosoft’s GitHub Copilot.\\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\\nanswer open-ended questions using a text-based web browser,\\nfacilitating users to search and navigate the web. Specifically,\\nWebGPT is trained in three steps. The first is for WebGPT\\nto learn to mimic human browsing behaviors using human\\ndemonstration data. Then, a reward function is learned to\\npredict human preferences. Finally, WebGPT is refined to\\noptimize the reward function via reinforcement learning and\\nrejection sampling.\\nTo enable LLMs to follow expected human instructions,\\nInstructGPT [59] is proposed to align language models with\\nuser intent on a wide range of tasks by fine-tuning with\\nhuman feedback. Starting with a set of labeler-written prompts\\nand prompts submitted through the OpenAI API, a dataset\\nof labeler demonstrations of the desired model behavior is\\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\\ndataset of human-ranked model outputs is collected to further\\nfine-tune the model using reinforcement learning. The method\\nis known Reinforcement Learning from Human Feedback\\nGPT Family\\nPaLM Family\\n\\xa0 \\xa0LLaMA 1/2 Family\\nGPT\\nGPT1\\nGPT2\\nGPT3\\nGPT4\\nGPT3.5 Turbo\\ntext-davinci\\ncode-davinci\\nCODEX\\nInstructGPT\\nWebGPT\\nGPT4 Vision\\nGPT4 Turbo\\nGorilla\\nMistral\\nVigogne\\nStable Beluga2\\nKoala\\nCode LLaMA\\nVicuna\\nAlpaca\\nBaize\\nLong LLaMA\\nGiraffe\\nGuanaco\\nTulu\\nWizardLM\\nMed-PaLM\\nPaLM-E\\nMed-PaLM2\\nFLAN-PaLM\\nU-PaLM\\nPaLM2\\nPaLM\\nFig. 8: Popular LLM Families.\\nFig. 9: GPT-3 shows that larger models make increasingly\\nefficient use of in-context information. It shows in-context\\nlearning performance on a simple task requiring the model to\\nremove random symbols from a word, both with and without\\na natural language task description. Courtesy of [56].\\n(RLHF), as shown in 10. The resultant InstructGPT models\\nhave shown improvements in truthfulness and reductions in\\ntoxic output generation while having minimal performance\\nregressions on public NLP datasets.\\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\\nThe most important milestone of LLM development is the\\nlaunch of ChatGPT (Chat Generative Pre-trained Transformer)\\n[60] on November 30, 2022. ChatGPT is chatbot that enables\\nusers to steer a conversation to complete a wide range of\\ntasks such as question answering, information seeking, text\\nsummarization, and more. ChatGPT is powered by GPT-3.5\\n(and later by GPT-4), a sibling model to InstructGPT, which\\nis trained to follow an instruction in a prompt and provide a\\ndetailed response.\\nGPT-4 [33] is the latest and most powerful LLM in the\\nGPT family. Launched in March, 2023, GPT-4 is a multi-\\nmodal LLM in that it can take image and text as inputs and\\nproduce text outputs. While still less capable than humans\\nin some of the most challenging real-world scenarios, GPT-4\\nexhibits human-level performance on various professional and\\nacademic benchmarks, including passing a simulated bar exam\\nwith a score around the top 10% of test takers, as shown in\\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\\npredict next tokens on large text corpora, and then fine-tuned\\nwith RLHF to align model behaviors with human-desired ones.\\n2) The LLaMA Family: LLaMA is a collection of founda-\\ntion language models, released by Meta. Unlike GPT models,\\nLLaMA models are open-source, i.e., model weights are\\nreleased to the research community under a noncommercial\\nlicense. Thus, the LLaMA family grows rapidly as these\\nmodels are widely used by many research groups to develop\\nbetter open-source LLMs to compete the closed-source ones or\\nto develop task-specific LLMs for mission-critical applications.\\nThe first set of LLaMA models [32] was released in Febru-\\nary 2023, ranging from 7B to 65B parameters. These models\\nare pre-trained on trillions of tokens, collected from publicly\\navailable datasets. LLaMA uses the transformer architecture of\\nGPT-3, with a few minor architectural modifications, including\\n(1) using a SwiGLU activation function instead of ReLU,\\n(2) using rotary positional embeddings instead of absolute\\npositional embedding, and (3) using root-mean-squared layer-\\nnormalization instead of standard layer-normalization. The\\nopen-source LLaMA-13B model outperforms the proprietary\\nGPT-3 (175B) model on most benchmarks, making it a good\\nbaseline for LLM research.\\nFig. 11: GPT-4 performance on academic and professional\\nexams, compared with GPT 3.5. Courtesy of [33].\\nIn July 2023, Meta, in partnership with Microsoft, released\\nthe LLaMA-2 collection [61], which include both foundation\\nlanguage models and Chat models finetuned for dialog, known\\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\\nto outperform other open-source models on many public\\nbenchmarks. Fig 12 shows the training process of LLaMA-2\\nChat. The process begins with pre-training LLaMA-2 using\\npublicly available online data. Then, an initial version of\\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\\nquently, the model is iteratively refined using RLHF, rejection\\nsampling and proximal policy optimization. In the RLHF stage,\\nthe accumulation of human feedback for revising the reward\\nmodel is crucial to prevent the reward model from being\\nchanged too much, which could hurt the stability of LLaMA\\nmodel training.\\nFig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\\n52K instruction-following demonstrations generated in the\\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\\nis very cost-effective for training, especially for academic\\nresearch. On the self-instruct evaluation set, Alpaca performs\\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\\nThe Vicuna team has developed a 13B chat model, Vicuna-\\n13B, by fine-tuning LLaMA on user-shared conversations\\ncollected from ShareGPT. Preliminary evaluation using GPT-\\n4 as a evaluator shows that Vicuna-13B achieves more than\\n90% quality of OpenAI’s ChatGPT, and Google’s Bard while\\noutperforming other models like LLaMA and Stanford Alpaca\\nin more than 90% of cases. 13 shows the relative response\\nquality of Vicuna and a few other well-known models by\\nGPT-4. Another advantage of Vicuna-13B is its relative limited\\ncomputational demand for model training. The training cost of\\nVicuna-13B is merely $300.\\nFig. 13: Relative Response Quality of Vicuna and a few other\\nwell-known models by GPT-4. Courtesy of Vicuna Team.\\nLike Alpaca and Vicuna, the Guanaco models [63] are also\\nfinetuned LLaMA models using instruction-following data. But\\nthe finetuning is done very efficiently using QLoRA such\\nthat finetuning a 65B parameter model can be done on a\\nsingle 48GB GPU. QLoRA back-propagates gradients through\\na frozen, 4-bit quantized pre-trained language model into Low\\nRank Adapters (LoRA). The best Guanaco model outperforms\\nall previously released models on the Vicuna benchmark,\\nreaching 99.3% of the performance level of ChatGPT while\\nonly requiring 24 hours of fine-tuning on a single GPU.\\nKoala [64] is yet another instruction-following language\\nmodel built on LLaMA, but with a specific focus on interaction\\ndata that include user inputs and responses generated by highly\\ncapable closed-source chat models such as ChatGPT. The\\nKoala-13B model performs competitively with state-of-the-art\\nchat models according to human evaluation based on real-\\nworld user prompts.\\nMistral-7B [65] is a 7B-parameter language model engi-\\nneered for superior performance and efficiency. Mistral-7B\\noutperforms the best open-source 13B model (LLaMA-2-13B)\\nacross all evaluated benchmarks, and the best open-source\\n34B model (LLaMA-34B) in reasoning, mathematics, and code\\ngeneration. This model leverages grouped-query attention for\\nfaster inference, coupled with sliding window attention to\\neffectively handle sequences of arbitrary length with a reduced\\ninference cost.\\nThe LLaMA family is growing rapidly, as more instruction-\\nfollowing models have been built on LLaMA or LLaMA-\\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],\\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\\nBeluga2 [72], just to name a few.\\n3) The PaLM Family: The PaLM (Pathways Language\\nModel) family are developed by Google. The first PaLM\\nmodel [31] was announced in April 2022 and remained private\\nuntil March 2023. It is a 540B parameter transformer-based\\nLLM. The model is pre-trained on a high-quality text corpus\\nconsisting of 780 billion tokens that comprise a wide range\\nof natural language tasks and use cases. PaLM is pre-trained\\non 6144 TPU v4 chips using the Pathways system, which\\nenables highly efficient training across multiple TPU Pods.\\nPaLM demonstrates continued benefits of scaling by achiev-\\ning state-of-the-art few-shot learning results on hundreds of\\nlanguage understanding and generation benchmarks. PaLM-\\n540B outperforms not only state-of-the-art fine-tuned models\\non a suite of multi-step reasoning tasks, but also on par with\\nhumans on the recently released BIG-bench benchmark.\\nThe U-PaLM models of 8B, 62B, and 540B scales are\\ncontinually trained on PaLM with UL2R, a method of continue\\ntraining LLMs on a few steps with UL2’s mixture-of-denoiser\\nobjective [73]. An approximately 2x computational savings\\nrate is reported.\\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\\nCompared to other instruction finetuning work mentioned\\nabove, Flan-PaLM’s finetuning is performed using a much\\nlarger number of tasks, larger model sizes, and chain-of-\\nthought data. As a result, Flan-PaLM substantially outperforms\\nprevious instruction-following models. For instance, Flan-\\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\\noutperforms PaLM-540B by a large margin (+9.4% on av-\\nerage). The finetuning data comprises 473 datasets, 146 task\\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\\ntask categories. Courtesy of [74].\\nPaLM-2 [75] is a more compute-efficient LLM with bet-\\nter multilingual and reasoning capabilities, compared to its\\npredecessor PaLM. PaLM-2 is trained using a mixture of\\nobjectives. Through extensive evaluations on English, multi-\\nlingual, and reasoning tasks, PaLM-2 significantly improves\\nthe model performance on downstream tasks across different\\nmodel sizes, while simultaneously exhibiting faster and more\\nefficient inference than PaLM.\\nMed-PaLM [76] is a domain-specific PaLM, and is de-\\nsigned to provide high-quality answers to medical questions.\\nMed-PaLM is finetuned on PaLM using instruction prompt\\ntuning, a parameter-efficient method for aligning LLMs to\\nnew domains using a few exemplars. Med-PaLM obtains very\\nencouraging results on many healthcare tasks, although it is\\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\\nPaLM via med-domain finetuning and ensemble prompting\\n[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\\ndataset (i.e., a benchmark combining six existing open ques-\\ntion answering datasets spanning professional medical exams,\\nresearch, and consumer queries), improving upon Med-PaLM\\nby over 19% and setting a new state-of-the-art.\\nC. Other Representative LLMs\\nIn addition to the models discussed in the previous sub-\\nsections, there are other popular LLMs which do not belong\\nto those three model families, yet they have achieved great\\nperformance and have pushed the LLMs field forward. We\\nbriefly describe these LLMs in this subsection.\\nFLAN: In [78], Wei et al. explored a simple method for\\nimproving the zero-shot learning abilities of language models.\\nThey showed that instruction tuning language models on a\\ncollection of datasets described via instructions substantially\\nimproves zero-shot performance on unseen tasks. They take\\na 137B parameter pretrained language model and instruction\\ntune it on over 60 NLP datasets verbalized via natural language\\ninstruction templates. They call this instruction-tuned model\\nFLAN. Fig 15 provides a comparison of instruction tuning\\nwith pretrain–finetune and prompting.\\nFig.\\n15:\\ncomparison\\nof\\ninstruction\\ntuning\\nwith\\npre-\\ntrain–finetune and prompting. Courtesy of [78].\\nGopher: In [79], Rae et al. presented an analysis of\\nTransformer-based language model performance across a wide\\nrange of model scales — from models with tens of millions of\\nparameters up to a 280 billion parameter model called Gopher.\\nThese models were evaluated on 152 diverse tasks, achieving\\nstate-of-the-art performance across the majority. The number\\nof layers, the key/value size, and other hyper-parameters of\\ndifferent model sizes are shown in Fig 16.\\nFig. 16: Model architecture details of Gopher with different\\nnumber of parameters. Courtesy of [78].\\nT0: In [80], Sanh et al. developed T0, a system for easily\\nmapping any natural language tasks into a human-readable\\nprompted form. They converted a large set of supervised\\ndatasets, each with multiple prompts with diverse wording.\\nThese prompted datasets allow for benchmarking the ability\\nof a model to perform completely held-out tasks. Then, a\\nT0 encoder-decoder model is developed to consume textual\\ninputs and produces target responses. The model is trained on\\na multitask mixture of NLP datasets partitioned into different\\ntasks.\\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\\nwork named ERNIE 3.0 for pre-training large-scale knowledge\\nenhanced models. It fuses auto-regressive network and auto-\\nencoding network, so that the trained model can be easily tai-\\nlored for both natural language understanding and generation\\ntasks using zero-shot learning, few-shot learning or fine-tuning.\\nThey have trained ERNIE 3.0 with 10 billion parameters\\non a 4TB corpus consisting of plain texts and a large-scale\\nknowledge graph. Fig 17 illustrates the model architecture of\\nErnie 3.0.\\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\\nof [81].\\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\\nlanguage models by conditioning on document chunks re-\\ntrieved from a large corpus, based on local similarity with pre-\\nceding tokens. Using a 2-trillion-token database, the Retrieval-\\nEnhanced Transformer (Retro) obtains comparable perfor-\\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\\n25% fewer parameters. As shown in Fig 18, Retro combines\\na frozen Bert retriever, a differentiable encoder and a chunked\\ncross-attention mechanism to predict tokens based on an order\\nof magnitude more data than what is typically consumed\\nduring training.\\nGLaM: In [84], Du et al. proposed a family of LLMs\\nnamed GLaM (Generalist Language Model), which use a\\nsparsely activated mixture-of-experts architecture to scale the\\nmodel capacity while also incurring substantially less training\\ncost compared to dense variants. The largest GLaM has 1.2\\ntrillion parameters, which is approximately 7x larger than GPT-\\n3. It consumes only 1/3 of the energy used to train GPT-3 and\\nrequires half of the computation flops for inference, while still\\nachieving better overall zero, one and few-shot performance\\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\\nof GLAM.\\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\\nfamily of Transformer-based neural language models special-\\nized for dialog, which have up to 137B parameters and are\\npre-trained on 1.56T words of public dialog data and web text.\\nFig. 18: Retro architecture. Left: simplified version where a\\nsequence of length n = 12 is split into l = 3 chunks of size\\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\\n5 tokens each. The retrieval pathway is shown on top. Right:\\nDetails of the interactions in the CCA operator. Causality is\\nmaintained as neighbours of the first chunk only affect the last\\ntoken of the first chunk and tokens from the second chunk.\\nCourtesy of [82].\\nFig. 19: GLaM model architecture. Each MoE layer (the\\nbottom block) is interleaved with a Transformer layer (the\\nupper block). Courtesy of [84].\\nThey showed that fine-tuning with annotated data and enabling\\nthe model to consult external knowledge sources can lead to\\nsignificant improvements towards the two key challenges of\\nsafety and factual grounding.\\nOPT: In [86], Zhang et al. presented Open Pre-trained\\nTransformers (OPT), a suite of decoder-only pre-trained trans-\\nformers ranging from 125M to 175B parameters, which they\\nshare with researchers. The OPT models’ parameters are\\nshown in 20\\nFig. 20: Different OPT Models’ architecture details. Courtesy\\nof [86].\\nChinchilla: In [2], Hoffmann et al. investigated the optimal\\nmodel size and number of tokens for training a transformer\\nlanguage model under a given compute budget. By training\\nover 400 language models ranging from 70 million to over\\n16 billion parameters on 5 to 500 billion tokens, they found\\nthat for compute-optimal training, the model size and the\\nnumber of training tokens should be scaled equally: for every\\ndoubling of model size the number of training tokens should\\nalso be doubled. They tested this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the\\nsame compute budget as Gopher but with 70B parameters and\\n4% more more data.\\nGalactica: In [87], Taylor et al. introduced Galactica, a\\nlarge language model that can store, combine and reason about\\nscientific knowledge. They trained on a large scientific corpus\\nof papers, reference material, knowledge bases and many other\\nsources. Galactica performed well on reasoning, outperforming\\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\\nCodeGen: In [88], Nijkamp et al. trained and released\\na family of large language models up to 16.1B parameters,\\ncalled CODEGEN, on natural language and programming\\nlanguage data, and open sourced the training library JAX-\\nFORMER. They showed the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-\\nthe-art on zero-shot Python code generation on HumanEval.\\nThey further investigated the multi-step paradigm for program\\nsynthesis, where a single program is factorized into multi-\\nple prompts specifying sub-problems. They also constructed\\nan open benchmark, Multi-Turn Programming Benchmark\\n(MTPB), consisting of 115 diverse problem sets that are\\nfactorized into multi-turn prompts.\\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\\ntilingual large-scale sequence-to-sequence (seq2seq) models,\\npre-trained on a mixture of denoising and Causal Language\\nModeling (CLM) tasks, are more efficient few-shot learners\\nthan decoder-only models on various task. They trained a\\n20 billion parameter multilingual seq2seq model called Alexa\\nTeacher Model (AlexaTM 20B) and showed that it achieves\\nstate-of-the-art (SOTA) performance on 1-shot summarization\\ntasks, outperforming a much larger 540B PaLM decoder\\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\\nlayers, 32 attention heads, and dmodel = 4096.\\nSparrow: In [90], Glaese et al. presented Sparrow, an\\ninformation-seeking dialogue agent trained to be more helpful,\\ncorrect, and harmless compared to prompted language model\\nbaselines. They used reinforcement learning from human feed-\\nback to train their models with two new additions to help\\nhuman raters judge agent behaviour. The high-level pipeline\\nof Sparrow model is shown in Fig 21.\\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\\na large language model pretrained on general natural language\\ndata and further trained on technical content, to tackle previous\\nLLM struggle with quantitative reasoning (such as solving\\nmathematics, science, and engineering problems).\\nMoD: In [92], Tay et al. presented a generalized and\\nunified perspective for self-supervision in NLP and show how\\ndifferent pre-training objectives can be cast as one another\\nand how interpolating between different objectives can be\\nFig. 21: Sparrow pipeline relies on human participation to\\ncontinually expand a training set. Courtesy of [90].\\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\\ntraining objective that combines diverse pre-training paradigms\\ntogether. This framework is known as Unifying Language\\nLearning (UL2). An overview of UL2 pretraining paradigm\\nis shown in Fig 21.\\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\\nof [92].\\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\\nparameter open-access language model designed and built\\nthanks to a collaboration of hundreds of researchers. BLOOM\\nis a decoder-only Transformer language model trained on the\\nROOTS corpus, a dataset comprising hundreds of sources in\\n46 natural and 13 programming languages (59 in total). An\\noverview of BLOOM architecture is shown in Fig 23.\\nFig. 23: An overview of BLOOM architecture. Courtesy of\\n[93].\\nGLM: In [94], Zeng et al. introduced GLM-130B, a\\nbilingual (English and Chinese) pre-trained language model\\nwith 130 billion parameters. It was an attempt to open-source\\na 100B-scale model at least as good as GPT-3 (davinci) and\\nunveil how models of such a scale can be successfully pre-\\ntrained.\\nPythia: In [95], Biderman et al. introduced Pythia, a suite\\nof 16 LLMs all trained on public data seen in the exact same\\norder and ranging in size from 70M to 12B parameters. We\\nprovide public access to 154 checkpoints for each one of the\\n16 models, alongside tools to download and reconstruct their\\nexact training dataloaders for further study.\\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\\nparameter model that learns to imitate the reasoning process\\nof large foundation models. Orca learns from rich signals\\nfrom GPT-4 including explanation traces; step-by-step thought\\nprocesses; and other complex instructions, guided by teacher\\nassistance from ChatGPT.\\nStarCoder: In [97], Li et al. introduced StarCoder and\\nStarCoderBase. They are 15.5B parameter models with 8K\\ncontext length, infilling capabilities and fast large-batch in-\\nference enabled by multi-query attention. StarCoderBase is\\ntrained on one trillion tokens sourced from The Stack, a\\nlarge collection of permissively licensed GitHub repositories\\nwith inspection tools and an opt-out process. They fine-tuned\\nStarCoderBase on 35B Python tokens, resulting in the creation\\nof StarCoder. They performed the most comprehensive evalu-\\nation of Code LLMs to date and showed that StarCoderBase\\noutperforms every open Code LLM that supports multiple pro-\\ngramming languages and matches or outperforms the OpenAI\\ncode-cushman-001 model.\\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\\na Multimodal Large Language Model (MLLM) that can per-\\nceive general modalities, learn in context (i.e., few-shot), and\\nfollow instructions (i.e. zero-shot). Specifically, they trained\\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\\nincluding arbitrarily interleaved text and images, image-caption\\npairs, and text data. Experimental results show that KOSMOS-\\n1 achieves impressive performance on (i) language understand-\\ning, generation, and even OCR-free NLP (directly fed with\\ndocument images), (ii) perception-language tasks, including\\nmultimodal dialogue, image captioning, visual question an-\\nswering, and (iii) vision tasks, such as image recognition with\\ndescriptions (specifying classification via text instructions).\\nGemini: In [99], Gemini team introduced a new family of\\nmultimodal models, that exhibit promising capabilities across\\nimage, audio, video, and text understanding. Gemini family\\nincludes three versions: Ultra for highly-complex tasks, Pro\\nfor enhanced performance and deployability at scale, and Nano\\nfor on-device applications. Gemini architecture is built on top\\nof Transformer decoders, and is trained to support 32k context\\nlength (via using efficient attention mechanisms).\\nSome of the other popular LLM frameworks (or techniques\\nused for efficient developments of LLMs) includes Inner-\\nMonologue [100], Megatron-Turing NLG [101], LongFormer\\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],\\nFuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\\n[122].\\nFig 24 provides an overview of some of the most repre-\\nsentative LLM frameworks, and the relevant works that have\\ncontributed to the success of LLMs and helped to push the\\nlimits of LLMs.\\nIII.\\nHOW LLMS ARE BUILT\\nIn this section, we first review the popular architectures\\nused for LLMs, and then discuss data and modeling techniques\\nranging from data preparation, tokenization, to pre-training,\\ninstruction tuning, and alignment.\\nOnce the model architecture is chosen, the major steps\\ninvolved in training an LLM includes: data preparation (col-\\nlection, cleaning, deduping, etc.), tokenization, model pre-\\ntraining (in a self-supervised learning fashion), instruction\\ntuning, and alignment. We will explain each of them in a\\nseparate subsection below. These steps are also illustrated in\\nFig 25.\\nA. Dominant LLM Architectures\\nThe most widely used LLM architectures are encoder-only,\\ndecoder-only, and encoder-decoder. Most of them are based on\\nTransformer (as the building block). Therefore we also review\\nthe Transformer architecture here.\\n1) Transformer: in a ground-breaking work [44], Vaswani\\net al. proposed the Transformer framework, which was orig-\\ninally designed for effective parallel computing using GPUs.\\nThe heart of Transformer is the (self-)attention mechanism,\\nwhich can capture long-term contextual information much\\nmore effectively using GPUs than the recurrence and convo-\\nlution mechanisms. Fig 26 provides a high-level overview of\\ntransformer work. In this section we provide an overview of the\\nmain elements and variants, see [44], [123] for more details.\\nThe Transformer language model architecture, originally\\nproposed for machine translation, consists of an encoder and\\na decoder. The encoder is composed of a stack of N = 6\\nidentical Transformer layers. Each layer has two sub-layers.\\nThe first one is a multi-head self-attention layer, and the other\\none is a simple position-wise fully connected feed-forward\\nnetwork. The decoder is composed of a stack of 6 identical\\nlayers. In addition to the two sub-layers in each encoder layer,\\nthe decoder has a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. The attention\\nfunction can be described as mapping a query and a set of key-\\nvalue pairs to an output, where the query, keys, values, and\\noutput are all vectors. The output is computed as a weighted\\nsum of the values, where the weight assigned to each value\\nis computed by a compatibility function of the query with the\\ncorresponding key. Instead of performing a single attention\\nfunction with dmodel dimensional keys, values and queries,\\nit is found to be beneficial to linearly project the queries,\\nkeys and values h with different, learned linear projections to\\ndk, dk and dv dimensions, respectively. Positional encoding is\\nincorporated to fuse information about the relative or absolute\\nposition of the tokens in the sequence.\\nFig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣shows entities that serve\\nnot only as models but also as approaches. ♦shows only approaches.\\n2) Encoder-Only: For this family, at each stage, the atten-\\ntion layers can access all the words in the initial sentence.\\nThe pre-training of these models usually consist of some-\\nhow corrupting a given sentence (for instance, by masking\\nrandom words in it) and tasking the model with finding or\\nreconstructing the initial sentence. Encoder models are great\\nfor tasks requiring an understanding of the full sequence,\\nsuch as sentence classification, named entity recognition, and\\nextractive question answering. One prominent encoder only\\nmodel is BERT (Bidirectional Encoder Representations from\\nTransformers), proposed in [24].\\n3) Decoder-Only: For these models, at each stage, for any\\nword, the attention layers can only access the words positioned\\nbefore that in the sentence. These models are also sometimes\\ncalled auto-regressive models. The pretraining of these models\\nis usually formulated as predicting the next word (or token)\\nin the sequence. The decoder-only models are best suited for\\ntasks involving text generation. GPT models are prominent\\nexample of this model category.\\n4) Encoder-Decoder: These models use both encoder and\\ndecoder, and are sometimes called sequence-to-sequence mod-\\nels. At each stage, the attention layers of the encoder can access\\nall the words in the initial sentence, whereas the attention\\nlayers of the decoder only accesses the words positioned before\\na given word in the input. These models are usually pre-\\ntrained using the objectives of encoder or decoder models, but\\nusually involve something a bit more complex. For instance,\\nsome models are pretrained by replacing random spans of text\\n(that can contain several words) with a single mask special\\nword, and the objective is then to predict the text that this\\nmask word replaces. Encoder-decoder models are best suited\\nfor tasks about generating new sentences conditioned on a\\ngiven input, such as summarization, translation, or generative\\nquestion answering.\\nB. Data Cleaning\\nData quality is crucial to the performance of language\\nmodels trained on them. Data cleaning techniques such as\\nfiltering, deduplication, are shown to have a big impact on\\nthe model performance.\\nAs an example, in Falcon40B [124], Penedo et al. showed\\nthat properly filtered and deduplicated web data alone can lead\\nto powerful models; even significantly outperforming models\\nfrom the state-of-the-art trained on The Pile. Despite extensive\\nfiltering, they were able to obtain five trillion tokens from\\nCommonCrawl. They also released an extract of 600 billion\\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\\neters language models trained on it. 27 shows the Refinement\\nprocess of CommonCrawl data by this work.\\n1) Data Filtering: Data filtering aims to enhance the qual-\\nity of training data and the effectiveness of the trained LLMs.\\nCommon data filtering techniques include:\\nRemoving Noise: refers to eliminating irrelevant or noisy\\ndata that might impact the model’s ability to generalize well.\\nAs an example, one can think of removing false information\\nfrom the training data, to lower the chance of model generating\\nfalse responses. Two mainstream approaches for quality filter-\\ning includes: classifier-based, and heuristic-based frameworks.\\nHow LLMs Are Built?\\nData Cleaning\\nTokenizations\\nBytePairEncoding\\nWordPieceEncoding\\nSentencePieceEncoding\\nPositional Encoding\\nAbsolute Positional Embeddings\\nRelative Positional Embeddings\\nRotary Position Embeddings\\nRelative Positional Bias\\nModel Pre-training\\nMasked Language Modeling\\nCausal Language Modeling\\nNext Sentence Prediction\\nMixture of Experts\\nFine-tuning and Instruction Tuning\\nAlignment\\nSupervised learning\\nReinforcement Learning from Human Feedback\\nDirect Preference Optimization\\nKahneman-Tversky Optimization\\nDecoding Strategies\\nGreedy Search\\nBeam Search\\nTop-k Sampling\\nTop-p Sampling\\nCost-Effective Training/Inference,\\nAdaptation & Compression\\nOptimized Training\\nZero Redundancy Optimizer\\nReceptance Weighted Key Value\\nLow-Rank Adaption\\nKnowledge Distillation\\nQuantization\\nData Filtering\\nRemoving Noise\\nHandling Outliers\\nAddressing Imbalances\\nText Preprocessing\\nDeduplication\\nLLM Architectures\\nEncoder-Only\\nDecoder-Only\\nEncoder-Decoder\\n...\\nSupervised Fine-tuning\\nGeneral Fine-tuning\\nMulti-turn Instructions\\nInstruction Following\\nFig. 25: This figure shows different components of LLMs.\\nFig. 26: High-level overview of transformer work. Courtesy of\\n[44].\\nFig. 27: Subsequent stages of Macrodata Refinement remove\\nnearly 90% of the documents originally in CommonCrawl.\\nCourtesy of [124].\\nHandling Outliers: Identifying and handling outliers or\\nanomalies in the data to prevent them from disproportionately\\ninfluencing the model.\\nAddressing Imbalances: Balancing the distribution of\\nclasses or categories in the dataset to avoid biases and ensure\\nfair representation. This is specially useful for responsible\\nmodel training and evaluation.\\nText Preprocessing: Cleaning and standardizing text data\\nby removing stop words, punctuation, or other elements that\\nmay not contribute significantly to the model’s learning.\\nDealing with Ambiguities: Resolving or excluding am-\\nbiguous or contradictory data that might confuse the model\\nduring training. This can help the model to provide more\\ndefinite and reliable answers.\\n2) Deduplication: De-duplication refers to the process of\\nremoving duplicate instances or repeated occurrences of the\\nsame data in a dataset. Duplicate data points can introduce\\nbiases in the model training process and reduce the diversity, as\\nthe model may learn from the same examples multiple times,\\npotentially leading to overfitting on those particular instances.\\nSome works [125] have shown that de-duplication improves\\nmodels’ ability to generalize to new, unseen data.\\nThe de-duplication process is particularly important when\\ndealing with large datasets, as duplicates can unintentionally\\ninflate the importance of certain patterns or characteristics.\\nThis is especially relevant in NLP tasks, where diverse and\\nrepresentative training data is crucial for building robust lan-\\nguage models.\\nThe specific de-duplication method can vary based on\\nthe nature of the data and the requirements of the particular\\nlanguage model being trained. It may involve comparing entire\\ndata points or specific features to identify and eliminate du-\\nplicates. At the document level, existing works mainly rely on\\nthe overlap ratio of high-level features (e.g. n-grams overlap)\\nbetween documents to detect duplicate samples.\\nC. Tokenizations\\nTokenization referes to the process of converting a se-\\nquence of text into smaller parts, known as tokens. While\\nthe simplest tokenization tool simply chops text into tokens\\nbased on white space, most tokenization tools rely on a word\\ndictionary. However, out-of-vocabulary (OOV) is a problem\\nin this case because the tokenizer only knows words in its\\ndictionary. To increase the coverage of dictionaries, popular\\ntokenizers used for LLMs are based on sub-words, which can\\nbe combined to form a large number of words, including the\\nwords unseen in training data or words in different languages.\\nIn what follows, we describe three popular tokenizers.\\n1) BytePairEncoding: BytePairEncoding is originally a\\ntype of data compression algorithm that uses frequent patterns\\nat byte level to compress the data. By definition, this algorithm\\nmainly tries to keep the frequent words in their original form\\nand break down ones that are not common. This simple\\nparadigm keeps the vocabulary not very large, but also good\\nenough to represent common words at the same time. Also\\nmorphological forms of the frequent words can be represented\\nvery well if suffix or prefix is also commonly presented in the\\ntraining data of the algorithm.\\n2) WordPieceEncoding: This algorithm is mainly used for\\nvery well-known models such as BERT and Electra. At the\\nbeginning of training, the algorithm takes all the alphabet from\\nthe training data to make sure that nothing will be left as UNK\\nor unknown from the training dataset. This case happens when\\nthe model is given an input that can not be tokenized by the\\ntokenizer. It mostly happens in cases where some characters are\\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\\nmaximize the likelihood of putting all the tokens in vocabulary\\nbased on their frequency.\\n3) SentencePieceEncoding: Although both tokenizers de-\\nscribed before are strong and have many advantages compared\\nto white-space tokenization, they still take assumption of\\nwords being always separated by white-space as granted. This\\nassumption is not always true, in fact in some languages, words\\ncan be corrupted by many noisy elements such as unwanted\\nspaces or even invented words. SentencePieceEncoding tries\\nto address this issue.\\nD. Positional Encoding\\n1) Absolute Positional Embeddings: (APE) [44] has been\\nused in the original Transformer model to preserve the infor-\\nmation of sequence order. Therefore, the positional information\\nof words is added to the input embeddings at the bottom of\\nboth the encoder and decoder stacks. There are various options\\nfor positional encodings, either learned or fixed. In the vanilla\\nTransformer, sine and cosine functions are employed for this\\npurpose. The main drawback of using APE in Transformers\\nis the restriction to a certain number of tokens. Additionally,\\nAPE fails to account for the relative distances between tokens.\\n2) Relative Positional Embeddings: (RPE) [126] involves\\nextending self-attention to take into account the pairwise links\\nbetween input elements. RPE is added to the model at two\\nlevels: first as an additional component to the keys, and\\nsubsequently as a sub-component of the values matrix. This\\napproach looks at the input as a fully-connected graph with\\nlabels and directed edges. In the case of linear sequences, edges\\ncan capture information about the relative position differences\\nbetween input elements. A clipping distance, represented as k\\n2 ≤k ≤n −4, specifies the maximum limit on relative lo-\\ncations. This allows the model to make reasonable predictions\\nfor sequence lengths that are not part of the training data.\\n3) Rotary Position Embeddings: Rotary Positional Em-\\nbedding (RoPE) [127] tackles problems with existing ap-\\nproaches. Learned absolute positional encodings can lack gen-\\neralizability and meaningfulness, particularly when sentences\\nare short. Moreover, current methods like T5’s positional\\nembedding face challenges with constructing a full attention\\nmatrix between positions. RoPE uses a rotation matrix to\\nencode the absolute position of words and simultaneously in-\\ncludes explicit relative position details in self-attention. RoPE\\nbrings useful features like flexibility with sentence lengths, a\\ndecrease in word dependency as relative distances increase,\\nand the ability to improve linear self-attention with relative\\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\\nLLaMA are among models that take advantage of RoPE in\\ntheir architectures.\\n4) Relative Positional Bias: The concept behind this type\\nof positional embedding is to facilitate extrapolation during\\ninference for sequences longer than those encountered in train-\\ning. In [128] Press et al. proposed Attention with Linear Biases\\n(ALiBi). Instead of simply adding positional embeddings to\\nword embeddings, they introduced a bias to the attention scores\\nof query-key pairs, imposing a penalty proportional to their\\ndistance. In the BLOOM model, ALiBi is leveraged.\\nE. Model Pre-training\\nPre-training is the very first step in large language model\\ntraining pipeline, and it helps LLMs to acquire fundamental\\nlanguage understanding capabilities, which can be useful in a\\nwide range of language related tasks. During pre-training, the\\nLLM is trained on a massive amount of (usually) unlabeled\\ntexts, usually in a self-supervised manner. There are different\\napproaches used for pre-training like next sentence prediction\\n[24], two most common ones include, next token prediction\\n(autoregressive language modeling), and masked language\\nmodeling.\\nIn Autoregressive Language Modeling framework, given\\na sequence of n tokens x1, ..., xn, the model tries to predict\\nnext token xn+1 (and sometimes next sequence of tokens) in\\nan auto-regressive fashion. One popular loss function in this\\ncase is the log-likelihood of predicted tokens as shown in Eq\\n2\\nLALM(x) =\\nN\\nX\\ni=1\\np(xi+n|xi, ..., xi+n−1)\\n(1)\\nGiven the auto-regressive nature of this framework, the\\ndecoder-only models are naturally better suited to learn how\\nto accomplish these task.\\nIn Masked Language Modeling, some words are masked\\nin a sequence and the model is trained to predict the masked\\nwords based on the surrounding context. Sometimes people\\nrefer to this approach as denoising autoencoding, too. If we\\ndenote the masked/corrupted samples in the sequence x, as ˜x,\\nthen the training objective of this approach can be written as:\\nLMLM(x) =\\nN\\nX\\ni=1\\np(˜x|x\\\\˜x)\\n(2)\\nAnd more recently, Mixture of Experts (MoE) [130],\\n[131] have become very popular in LLM space too. MoEs\\nenable models to be pre-trained with much less compute,\\nwhich means one can dramatically scale up the model or\\ndataset size with the same compute budget as a dense model.\\nMoE consists of two main elements: Sparse MoE layers,\\nwhich are used instead of dense feed-forward network (FFN)\\nlayers, and have a certain number of “experts” (e.g. 8), in\\nwhich each expert is a neural network. In practice, the experts\\nare FFNs, but they can also be more complex networks. A gate\\nnetwork or router, that determines which tokens are sent to\\nwhich expert. It is worth noting that, one can send a token\\nto more than one expert. How to route a token to an expert\\nis one of the big decisions when working with MoEs - the\\nrouter is composed of learned parameters and is pretrained at\\nthe same time as the rest of the network. Fig 29 provides an\\nillustration of a Switch Transformer encoder block, which are\\nused in MoE.\\nF. Fine-tuning and Instruction Tuning\\nEarly language models such as BERT trained using self-\\nsupervision as explained in section III-E were not able to\\nperform specific tasks. In order for the foundation model to be\\nuseful it needed to be fine-tuned to a specific task with labeled\\ndata (so-called supervised fine-tuning or SFT for short). For\\nexample, in the original BERT paper [24], the model was fine-\\ntuned to 11 different tasks. While more recent LLMs no longer\\nrequire fine-tuning to be used, they can still benefit from task\\nor data-specific fine-tuning. For example, OpenAI reports that\\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\\nwhen fine-tuned with task specific data 2.\\nFine-tuning does not need to be performed to a single\\ntask though, and there are different approaches to multi-task\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\nor more tasks is known to improve results and reduce the\\ncomplexity of prompt engineering, and it can serve as an\\n2https://platform.openai.com/docs/guides/fine-tuning\\n(a) Absolute Positional Embeddings [129]\\n(b) Relative Positional Embeddings\\n(c) Rotary Positional Embedding [127]\\n(d) Relative Positional Bias [128]\\nFig. 28: Various positional encodings are employed in LLMs.\\nFig. 29: : Illustration of a Switch Transformer encoder block.\\nThey replaced the dense feed forward network (FFN) layer\\npresent in the Transformer with a sparse Switch FFN layer\\n(light blue). . Courtesy of [131].\\nalternative to retrieval augmented generation. Furthermore,\\nthere are other reasons why it might be advisable to fine-tune.\\nFor example, one might want to fine-tune to expose the model\\nto new or proprietary data that it has not been exposed to\\nduring pre-training.\\nAn important reason to fine-tune LLMs is to align the\\nresponses to the expectations humans will have when providing\\ninstructions through prompts. This is the so-called instruction\\ntuning [133]. We dive into the details of how to design\\nand engineer prompts in section IV-B, but in the context\\nof instruction tuning, it is important to understand that the\\ninstruction is a prompt that specifies the task that the LLM\\nshould accomplish. Instruction tuning datasets such as Natural\\nInstructions [134] include not only the task definition but other\\ncomponents such as positive/negative examples or things to\\navoid.\\nThe specific approach and instruction datasets used to\\ninstruction-tune an LLM varies, but, generally speaking, in-\\nstruction tuned models outperform their original foundation\\nmodels they are based on. For example, InstructGPT [59]\\noutperforms GPT-3 on most benchmarks. The same is true\\nfor Alpaca [62] when compared to LLaMA.\\nSelf-Instruct [135], proposed by Wang et al. is also a\\npopular approach along this line, in which they introduced a\\nframework for improving the instruction-following capabilities\\nof pre-trained language models by bootstrapping their own\\ngenerations. Their pipeline generates instructions, input, and\\noutput samples from a language model, then filters invalid or\\nsimilar ones before using them to fine tune the original model.\\nG. Alignment\\nAI Alignment is the process of steering AI systems towards\\nhuman goals, preferences, and principles. LLMs, pre-trained\\nfor word prediction, often exhibit unintended behaviors. For\\nexample, they might generate contents that are toxic, harmful,\\nmisleading and biased.\\nInstruction tuning, discussed above, gets LLMs a step\\ncloser to being aligned. However, in many cases, it is important\\nto include further steps to improve the alignment of the model\\nand avoid unintended behaviors 3. We review the most popular\\n3According to very recent research by Ethayarajh et al. [136], further\\nalignment besides SFT mainly improves models of at least 7B parameters.\\nFor smaller models, SFT is sufficient.\\napproaches to alignment in this subsection.\\nRLHF (reinforcement learning from human feedback) and\\nRLAIF (reinforcement learning from AI feedback) are two\\npopular approaches. RLHF uses a reward model to learn\\nalignment from human feedback. This reward model, after\\nbeing tuned, is able to rate different outputs and score them\\naccording to their alignment preferences given by humans. The\\nreward model gives feedback to the original LLM and this\\nfeedback is used to tune the LLM further [137]. Reinforcement\\nlearning from AI feedback on the other hand, directly connects\\na pretrained and well-aligned model to the LLM and helps it\\nto learn from larger and more aligned models [138].\\nIn another recent work (known as DPO) [139], Rafailov\\net al. discussed that RLHF is a complex and often unstable\\nprocedure, and tried to address this with a new approach. They\\nleveraged a mapping between reward functions and optimal\\npolicies to show that this constrained reward maximization\\nproblem can be optimized exactly with a single stage of policy\\ntraining, essentially solving a classification problem on the\\nhuman preference data. The resulting algorithm, which they\\ncalled Direct Preference Optimization (DPO), is stable, per-\\nformant, and computationally lightweight, eliminating the need\\nfor fitting a reward model, sampling from the LM during fine-\\ntuning, or performing significant hyperparameter tuning. They\\nobserved that fine-tuning with DPO exceeds RLHF’s ability to\\ncontrol sentiment of generations and improves response quality\\nin summarization. Fig 30 shows the high-level comparison\\nbetween DPO vs RLHF.\\nFig. 30: DPO optimizes for human preferences while avoiding\\nreinforcement learning. Existing methods for fine-tuning lan-\\nguage models with human feedback first fit a reward model\\nto a dataset of prompts and human preferences over pairs of\\nresponses, and then use RL to find a policy that maximizes\\nthe learned reward. In contrast, DPO directly optimizes for\\nthe policy best satisfying the preferences with a simple classi-\\nfication objective, without an explicit reward function or RL.\\nCourtesy of [139].\\nEven more recently Ethayarajh et al. proposed a new align-\\nment approach called the Kahneman-Tversky Optimization\\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\\ndoes not require paired preference data (x, yw, yl), and it\\nonly needs (x,y) and knowledge of whether y is desirable or\\nundesirable. KTO-aligned models are shown to be good or\\nbetter than DPO-aligned models at scales from 1B to 30B,\\ndespite not using paired preferences. KTO is also far easier to\\nuse in the real world than preference optimization methods, as\\nthe kind of data it needs is far more abundant. As an example,\\nevery retail company has a lot of customer interaction data and\\nwhether that interaction was successful (e.g., purchase made)\\nor unsuccessful (e.g., no purchase made). However, They have\\nlittle to no counterfactual data (i.e., what would have made\\nan unsuccessful customer interaction yl into a successful one\\nyw). Fig 31 shows a high-level comparison between KTO and\\nother alignment approaches discussed above.\\nFig. 31: LLM alignment involves supervised finetuning fol-\\nlowed by optimizing a human-centered loss (HALO). How-\\never, the paired preferences that existing approaches need are\\nhard-to-obtain. In contrast, KTO uses a far more abundant\\nkind of data, making it much easier to use in the real world.\\nCourtesy of [136].\\nH. Decoding Strategies\\nDecoding refers to the process of text generation using pre-\\ntrained LLMs. Given an input prompt, the tokenizer translates\\neach token in the input text into a corresponding token ID.\\nThen, the language model uses these token IDs as input and\\npredicts the next most likely token (or a sequence of tokens).\\nFinally, the model generates logits, which are converted to\\nprobabilities using a softmax function. Different decoding\\nstrategies have been proposed. Some of the most popular ones\\nare greedy search, beam search, as well as different sample\\ntechniques such as top-K, top-P (Nucleus sampling).\\n1) Greedy Search: Greedy search takes the most probable\\ntoken at each step as the next token in the sequence, discarding\\nall other potential options. As you can imagine, this is a simple\\napproach and can loose a lot of temporal consistency and\\ncoherency. It only considers the most probable token at each\\nstep, without considering the overall effect on the sequence.\\nThis property makes it fast, but it also means that it can miss\\nout on better sequences that might have appeared with slightly\\nless probable next tokens.\\n2) Beam Search: Unlike greedy search that only considers\\nthe next most probable token, beam search takes into account\\nthe N most likely tokens, where N denotes the number of\\nbeams. This procedure is repeated until a predefined maxi-\\nmum sequence length is reached or an end-of-sequence token\\nappears. At this point, the sequence of tokens (AKA “beam”)\\nwith the highest overall score is chosen as the output. For\\nexample for beam size of 2 and maximum length of 5,\\nthe beam search needs to keep track of 25 = 32 possible\\nsequences. So it is more computationally intensive than greedy\\nsearch.\\n3) Top-k Sampling: Top-k sampling is a technique that\\nuses the probability distribution generated by the language\\nmodel to select a token randomly from the k most likely\\noptions.\\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)=\\n12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\\nand the model outputs A 60% of the time, and B, 40% of\\nthe time. This approach ensures that we prioritize the most\\nprobable tokens while introducing an element of randomness\\nin the selection process.\\nThe randomness is usually introduced via the concept of\\ntemperature. The temperature T is a parameter that ranges from\\n0 to 1, which affects the probabilities generated by the softmax\\nfunction, making the most likely tokens more influential. In\\npractice, it simply consists of dividing the input logits by\\ntemperature value:\\nsoftmax(xi) =\\nexi/T\\nP\\nj exj/T\\n(3)\\nA low temperature setting significantly alters the proba-\\nbility distribution (and is commonly used in text generation\\nto control the level of “creativity” in the generated output),\\nwhile a large temperature prioritizes the tokens with higher\\nprobabilities. Top-k is a creative way of sampling, and can be\\nused along with beam search. The sequence chosen by top-\\nk sampling may not be the sequence with highest probability\\nin beam search. But it’s important to remember that highest\\nscores do not always lead to more realistic or meaningful\\nsequences.\\n4) Top-p Sampling: Top-p sampling, also known as Nu-\\ncleus sampling, takes a slightly different approach from top-k\\nsampling. Instead of selecting the top k most probable tokens,\\nnucleus sampling chooses a cutoff value p such that the sum of\\nthe probabilities of the selected tokens exceeds p. This forms\\na “nucleus” of tokens from which to randomly choose the next\\ntoken. In other words, in top-p sampling the language model\\nexamines the most probable tokens in descending order and\\nkeeps adding them to the list until the sum of probabilities\\nsurpasses the threshold p. As you can imagine, this could be\\nbetter specially for scenarios in which top-k tokens do not have\\na large probability mass. Unlike top-k sampling, the number\\nof tokens included in the nucleus sampling is not fixed. This\\nvariability often results in a more diverse and creative output,\\nmaking nucleus sampling popular for text generation related\\ntasks.\\nI.\\nCost-Effective Training/Inference/Adaptation/Compression\\nIn this part, we review some of the popular approaches\\nused for more cost-friendly (and compute-friendly) training\\nand usage of LLMs.\\n1) Optimized Training: There are many frameworks de-\\nveloped for optimized training of LLMs, here we introduce\\nsome of the prominent ones.\\nZeRO:\\nIn [140], Rajbhandari et al. developed a novel\\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\\nmemory, vastly improving training speed of LLMs while\\nincreasing the model size that can be efficiently trained. ZeRO\\neliminates memory redundancies in data- and model-parallel\\ntraining while retaining low communication volume and high\\ncomputational granularity, allowing one to scale the model\\nsize proportional to the number of devices with sustained high\\nefficiency.\\nRWKV: In [141], Peng et al. proposed a novel model\\narchitecture, Receptance Weighted Key Value (RWKV), that\\ncombines the efficient parallelizable training of Transformers\\nwith the efficient inference of RNNs. Their approach leverages\\na linear attention mechanism and allows them to formulate the\\nmodel as either a Transformer or an RNN, which parallelizes\\ncomputations during training and maintains constant compu-\\ntational and memory complexity during inference, leading to\\nthe first non-transformer architecture to be scaled to tens of\\nbillions of parameters. RWKV architecture is shown in Fig\\n32. The Time Complexity comparison of RWKV with different\\nFig. 32: RWKV architecture. Courtesy of [141].\\nTransformers are provided in Fig 33.\\nFig. 33: Time Complexity comparison of RWKV with different\\nTransformers. Here T denotes the sequence length, d the\\nfeature dimension, and c is MEGA’s chunk size of quadratic\\nattention. Courtesy of [141].\\n2) Low-Rank Adaption (LoRA): Low-Rank Adaptation is\\na popular and lightweight training technique that significantly\\nreduces the number of trainable parameters, and is based\\non a crucial insight that the difference between the fine-\\ntuned weights for a specialized task and the initial pre-trained\\nweights often exhibits “low intrinsic rank” - meaning that\\nit can be approximated well by a low rank matrix [142].\\nFig. 34: An illustration of LoRA reparametrizan. Only A and\\nB trained during this process. Courtesy of [142].\\nTraining with LoRA is much faster, memory-efficient, and\\nproduces smaller model weights (a few hundred MBs), that are\\neasier to store and share. One property of low-rank matrices\\nis that they can be represented as the product of two smaller\\nmatrices. This realization leads to the hypothesis that this delta\\nbetween fine-tuned weights and initial pre-trained weights can\\nbe represented as the matrix product of two much smaller\\nmatrices. By focusing on updating these two smaller matrices\\nrather than the entire original weight matrix, computational\\nefficiency can be substantially improved.\\nSpecifically, for a pre-trained weight matrix W0 ∈Rd×k,\\nLoRA constrains its update by representing the latter with\\na low-rank decomposition W0 + ∆W = W0 + BA, where\\nB ∈Rd×r , A ∈Rr×k, and the rank r ≪min(d, k). During\\ntraining, W0 is frozen and does not receive gradient updates,\\nwhile A and B contain trainable parameters. It is worth\\nmentioning that both W0 and ∆W = BA are multiplied with\\nthe same input, and their respective output vectors are summed\\ncoordinate-wise. For h = W0x, their modified forward pass\\nyields: h = W0x + ∆Wx = W0x + BAx. Usually a random\\nGaussian initialization is used for A, and zero initialization\\nfor B, so ∆W = BA is zero at the beginning of training.\\nThey then scale ∆Wx by αr, where α is a constant in r. This\\nreparametrization is illustrated in Figure 34\\nIt is worth mentioning that LoRA can be applied to any a\\nsubset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architec-\\nture, there are four weight matrices in the self-attention module\\n(Wq , Wk, Wv , Wo), and two in the MLP module. Most of\\nthe time, LoRA is focused on adapting the attention weights\\nonly for downstream tasks, and freezes the MLP modules, so\\nthey are not trained in downstream tasks both for simplicity\\nand parameter-efficiency.\\n3) Knowledge Distillation: Knowledge distillation is the\\nprocess of learning from a larger model [143]. Earlier days of\\nbest-performing models release have proven that this approach\\nis very useful even if it is used in an API distillation approach.\\nIt is also referred to as an approach to distill the knowledge of\\nnot a single model but in fact multiple models into a smaller\\none. Creating smaller models by this approach yields smaller\\nmodel sizes that can be used even on edge devices. Knowledge\\ndistillation as shown in Fig 35, illustrates a general setup of\\nthis training scheme.\\nFig. 35: A generic knowledge distillation framework with\\nstudent and teacher (Courtesy of [144]).\\nKnowledge can be transferred by different forms of learn-\\ning: response distillation, feature distillation, and API distilla-\\ntion. Response distillation is concerned only with the outputs\\nof the teacher model and tries to teach the student model\\nhow to exactly or at least similarly perform (in the sense of\\nprediction) as the teacher. Feature distillation not only uses\\nthe last layer but also intermediate layers as well to create a\\nbetter inner representation for the student model. This helps the\\nsmaller model to have a similar representation as the teacher\\nmodel.\\nAPI distillation is the process of using an API (typically\\nfrom an LLM provider such as OpenAI) to train smaller\\nmodels. In the case of LLMs, it is used to train the model\\nfrom the direct output of the larger model which makes it very\\nsimilar to response distillation. Many concerns are raised by\\nthis type of distillation because in cases where the model itself\\nis not openly available, a (usually) paid API is exposed for end\\nusers. On the other hand, while users pay for each call, how to\\nuse the predictions is limited, for example, OpenAI prohibits\\nusage of its API to create LLMs that later will be used to\\ncompete with it. The main value in such case is training data.\\n4) Quantization: deep learning in its core, is a set of\\nmathematical functions applied to matrices, with a specific\\nprecision for model weights. Reducing the precision of the\\nweights can be used to reduce the size of the model and also\\nmake it faster. As an example, Float-32 operations compared\\nto Int-8 operations are slower. This process, which is called\\nquantization, can be applied in different phases. Main ap-\\nproaches for model quantization can be categorized as: post\\ntraining quantization and quantization-aware training. Post-\\ntraining quantization is concerned with quantized trained mod-\\nels in two well-known methods: dynamic and static. Dynamic\\npost-training quantization computes the range of quantization\\non the runtime and is slower compared to static. Quantization-\\naware training adds quantization criteria into training, and\\na quantized model is trained and optimized during training\\nprocess. This approach ensures that the end model will have\\ngood performance and also does not need to be quantized after\\ntraining.\\nIV.\\nHOW LLMS ARE USED AND AUGMENTED\\nOnce the LLMs are trained, we can use them to generate\\ndesired outputs for a variety of tasks. LLMs can be used\\ndirectly through basic prompting. However, in order to exploit\\ntheir full potential or to address some of the shortcomings,\\nwe need to augment the models through some external means.\\nIn this section we first provide a brief overview of the main\\nshortcoming of LLMs, with a deeper look at the issue of\\nhallucination. We then describe how prompting and some aug-\\nmentation approaches can not only address those limitations\\nbut also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n•\\nThey don’t have state/memory. LLMs on their own\\ncannot remember even what was sent to them in the\\nprevious prompt. That is an important limitation for\\nmany of the uses cases that require some form of state.\\n•\\nThey are stochastic/probabilistic. If you send the same\\nprompt to an LLM several times, you are likely to get\\ndifferent responses. While there are parameters, and\\nin particular the temperature, to limit the variability\\nin the response, this is an inherent property of their\\ntraining that can create issues.\\n•\\nThey have stale information and, on their own, don’t\\nhave access to external data. An LLM on its own does\\nnot even know about the current time or day and does\\nnot have access to any information that was not present\\nin its training set.\\n•\\nThey are generally very large. This means that many\\ncostly GPU machines are needed for training and\\nserving. In some cases, largest models have poor\\nSLAs, particularly in terms of latency.\\n•\\nThey hallucinate. LLMs do not have a notion of\\n”truth” and they have usually been trained on a mix\\nof good and bad content. They can produce very\\nplausible but untruthful answers.\\nWhile the previous limitations can all become important\\nfor some applications, it is worth for us to dive a bit into the\\nlast one, hallucinations, since it has gathered a lot of interest\\nover the past few months and it has also sparked many of the\\nprompt approaches and LLM augmentation methods we will\\nlater describe.\\nHallucination: In the realm of Large Language Models\\n(LLMs), the phenomenon of ”hallucinations” has garnered\\nsignificant attention. Defined in the literature, notably in the\\n”Survey of Hallucination in Natural Language Generation”\\npaper [145], hallucination in an LLM is characterized as\\n”the generation of content that is nonsensical or unfaithful\\nto the provided source.” This terminology, although rooted in\\npsychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:\\n1)\\nIntrinsic Hallucinations: These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2)\\nExtrinsic Hallucinations: These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.\\nThe definition of ’source’ in LLM contexts varies with the\\ntask. In dialogue-based tasks, it refers to ’world knowledge’,\\nwhereas in text summarization, it pertains to the input text\\nitself. This distinction plays a crucial role in evaluating and\\ninterpreting hallucinations. The impact of hallucinations is also\\nhighly context-dependent. For instance, in creative endeavors\\nlike poem writing, hallucinations might be deemed acceptable\\nor even beneficial.\\nLLMs, trained on diverse datasets including the internet,\\nbooks, and Wikipedia, generate text based on probabilistic\\nmodels without an inherent understanding of truth or falsity.\\nRecent advancements like instruct tuning and Reinforcement\\nLearning from Human Feedback (RLHF) have attempted to\\nsteer LLMs towards more factual outputs, but the fundamental\\nprobabilistic nature and its inherent limitations remain. A\\nrecent study, “Sources of Hallucination by Large Language\\nModels on Inference Tasks” [146], highlights two key aspects\\ncontributing to hallucinations in LLMs: the veracity prior and\\nthe relative frequency heuristic, underscoring the complexities\\ninherent in LLM training and output generation.\\nEffective automated measurement of hallucinations in\\nLLMs requires a combination of statistical and model-based\\nmetrics.\\nStatistical Metrics:\\n•\\nMetrics like ROUGE [147] and BLEU [148] are com-\\nmon for assessing text similarity, focusing on intrinsic\\nhallucinations.\\n•\\nAdvanced metrics such as PARENT [149], PARENT-\\nT [150], and Knowledge F1 [151] are utilized when\\nstructured knowledge sources are available. These\\nmetrics, while effective, have limitations in capturing\\nsyntactic and semantic nuances.\\nModel-Based Metrics:\\n•\\nIE-Based Metrics: Utilize Information Extraction\\nmodels to simplify knowledge into relational tuples,\\nthen compare these with the source.\\n•\\nQA-Based Metrics: Assess the overlap between gen-\\nerated content and the source through a question-\\nanswering framework (see [152]).\\n•\\nNLI-Based Metrics: Use Natural Language Inference\\ndatasets to evaluate the truthfulness of a generated\\nhypothesis based on a given premise (see [153]).\\n•\\nFaithfulness Classification Metrics: Offer a refined\\nassessment by creating task-specific datasets for a\\nnuanced evaluation (see [154]).\\nDespite advances in automated metrics, human judgment\\nremains a vital piece. It typically involves two methodologies:\\nB) Augmenting LLMs through\\nexternal knowledge - RAG\\nHow LLMs Are Used and Augmented\\nC) Using External Tools\\nD) LLM Agents\\nFunctionality of an LLM-based agent\\nTool Access and Utilization\\nDecision Making\\nPrompt engineering techniques for agents\\nReasoning without Observation\\nReason and Act\\nDialog-Enabled Resolving Agents\\na) RAG-aware prompting techniques\\na) Tool-aware prompting techniques\\nA)\\xa0LLM limitations\\nHallucination\\nHallucination Quantification\\nAutomated metrics\\nHuman judgment\\nStatistical Metrics\\nModel-Based Metrics\\nScoring\\nComparative Analysis\\nIE-Based Metrics\\nQA-Based Metrics\\nNLI-Based Metrics\\nB)\\xa0Using LLMs\\n\\xa0Prompt Design and Engineering\\n1) Chain of Thought\\nZero-Shot CoT\\nManual CoT\\n5) Expert Prompting\\n6) Chains\\n2) Tree of Thought\\n7) Rails\\nTopical Rails\\nFact-Checking Rails\\nJailbreaking Rails\\n8) Automatic Prompt Engineering\\nPrompt Generation\\nPrompt Scoring\\nRefinement and Iteration\\n3)\\xa0Self-Consistency\\n4) Reflection\\nComponents of a RAG\\nRetrieval\\xa0\\nGeneration\\xa0\\nAugmentation\\nRAG Tools\\nLangChain\\xa0\\nLlamaIndex\\nHayStack\\nMeltano\\nCohere Coral\\nFlowise AI\\nFig. 36: How LLMs Are Used and Augmented.\\n1)\\nScoring: Human evaluators rate the level of halluci-\\nnation within a predefined scale.\\n2)\\nComparative Analysis: Evaluators compare gener-\\nated content against baseline or ground-truth refer-\\nences, adding an essential layer of subjective assess-\\nment.\\nFactScore [155] is a recent example of a metric that can be\\nused both for human and model-based evaluation. The metric\\nbreaks an LLM generation into “atomic facts”. The final score\\nis computed as the sum of the accuracy of each atomic fact,\\ngiving each of them equal weight. Accuracy is a binary number\\nthat simply states whether the atomic fact is supported by the\\nsource. The authors implement different automation strategies\\nthat use LLMs to estimate this metric.\\nFinally, mitigating hallucinations in LLMs is a multifaceted\\nchallenge, requiring tailored strategies to suit various applica-\\ntions. Those include:\\n•\\nProduct Design and User Interaction Strategies such\\nas use case design, structuring the input/output, or\\nproviding mechanisms for user feedback.\\n•\\nData Management and Continuous Improvement.\\nMaintaining and analyzing a tracking set of hallucina-\\ntions is essential for ongoing model improvement.\\n•\\nPrompt Engineering and Metaprompt Design. Many\\nof the advanced prompt techniques described in IV-B\\nsuch as Retrieval Augmented Generation directly ad-\\ndress hallucination risks.\\n•\\nModel Selection and Configuration for Hallucination\\nMitigation. For exemple, larger models with lower\\ntemperature settings usually perform better. Also,\\ntechniques such as RLHF or domain-sepcific fine-\\ntuning can mitigate hallucination risks.\\nB. Using LLMs: Prompt Design and Engineering\\nA prompt in generative AI models is the textual input\\nprovided by users to guide the model’s output. This could\\nrange from simple questions to detailed descriptions or specific\\ntasks. Prompts generally consist of instructions, questions,\\ninput data, and examples. In practice, to elicit a desired\\nresponse from an AI model, a prompt must contain either\\ninstructions or questions, with other elements being optional.\\nAdvanced prompts involve more complex structures, such as\\n”chain of thought” prompting, where the model is guided to\\nfollow a logical reasoning process to arrive at an answer.\\nPrompt engineering is a rapidly evolving discipline that\\nshapes the interactions and outputs of LLMs and other gen-\\nerative AI models. The essence of prompt engineering lies in\\ncrafting the optimal prompt to achieve a specific goal with\\na generative model. This process is not only about instructing\\nthe model but also involves some understanding of the model’s\\ncapabilities and limitations, and the context within which it\\noperates.\\nPrompt engineering transcends the mere construction of\\nprompts; it requires a blend of domain knowledge, understand-\\ning of the AI model, and a methodical approach to tailor\\nprompts for different contexts. This might involve creating\\ntemplates that can be programmatically modified based on a\\ngiven dataset or context. For example, generating personalized\\nresponses based on user data might use a template that is\\ndynamically filled with relevant user information.\\nFurthermore, prompt engineering is an iterative and ex-\\nploratory process, akin to traditional machine learning prac-\\ntices such as model evaluation or hyperparameter tuning. The\\nrapid growth of this field suggests its potential to revolutionize\\ncertain aspects of machine learning, moving beyond traditional\\nmethods like feature or architecture engineering. On the other\\nhand, traditional engineering practices such as version con-\\ntrol and regression testing need to be adapted to this new\\nparadigm just like they were adapted to other machine learning\\napproaches [156].\\nIn the following paragraphs we detail some of the most\\ninteresting and popular prompt engineering approaches.\\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\\ntechnique, initially described in the paper “Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models”[34]\\nby Google researchers, represents a pivotal advancement in\\nprompt engineering for Large Language Models (LLMs).\\nThis approach hinges on the understanding that LLMs, while\\nproficient in token prediction, are not inherently designed for\\nexplicit reasoning. CoT addresses this by guiding the model\\nthrough essential reasoning steps.\\nCoT is based on making the implicit reasoning process of\\nLLMs explicit. By outlining the steps required for reasoning,\\nthe model is directed closer to a logical and reasoned output,\\nespecially in scenarios demanding more than simple informa-\\ntion retrieval or pattern recognition.\\nCoT prompting manifests in two primary forms:\\n1)\\nZero-Shot CoT: This form involves instructing the\\nLLM to “think step by step”, prompting it to de-\\nconstruct the problem and articulate each stage of\\nreasoning.\\n2)\\nManual CoT: A more complex variant, it requires\\nproviding step-by-step reasoning examples as tem-\\nplates for the model. While yielding more effective\\nresults, it poses challenges in scalability and mainte-\\nnance.\\nManual CoT is more effective than zero-shot. However,\\nthe effectiveness of this example-based CoT depends on the\\nchoice of diverse examples, and constructing prompts with\\nsuch examples of step by step reasoning by hand is hard and\\nerror prone. That is where automatic CoT [157] comes into\\nplay.\\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\\n[158] prompting technique is inspired by the concept of\\nconsidering various alternative solutions or thought processes\\nbefore converging on the most plausible one. ToT is based\\non the idea of branching out into multiple ”thought trees”\\nwhere each branch represents a different line of reasoning.\\nThis method allows the LLM to explore various possibilities\\nand hypotheses, much like human cognitive processes where\\nmultiple scenarios are considered before determining the most\\nlikely one.\\nA critical aspect of ToT is the evaluation of these reasoning\\npaths. As the LLM generates different branches of thought,\\neach is assessed for its validity and relevance to the query.\\nThis process involves real-time analysis and comparison of\\nthe branches, leading to a selection of the most coherent and\\nlogical outcome.\\nToT is particularly useful in complex problem-solving\\nscenarios where a single line of reasoning might not suffice.\\nIt allows LLMs to mimic a more human-like problem-solving\\napproach, considering a range of possibilities before arriving\\nat a conclusion. This technique enhances the model’s ability\\nto handle ambiguity, complexity, and nuanced tasks, making it\\na valuable tool in advanced AI applications.\\n3) Self-Consistency:\\nSelf-Consistency [159] utilizes an\\nensemble-based method, where the LLM is prompted to gen-\\nerate multiple responses to the same query. The consistency\\namong these responses serves as an indicator of their accuracy\\nand reliability.\\nThe Self-Consistency approach is grounded in the principle\\nthat if an LLM generates multiple, similar responses to the\\nsame prompt, it is more likely that the response is accurate.\\nThis method involves asking the LLM to tackle a query mul-\\ntiple times, each time analyzing the response for consistency.\\nThis technique is especially useful in scenarios where factual\\naccuracy and precision are paramount.\\nThe consistency of responses can be measured using vari-\\nous methods. One common approach is to analyze the overlap\\nin the content of the responses. Other methods may include\\ncomparing the semantic similarity of responses or employing\\nmore sophisticated techniques like BERT-scores or n-gram\\noverlaps. These measures help in quantifying the level of\\nagreement among the responses generated by the LLM.\\nSelf-Consistency has significant applications in fields\\nwhere the veracity of information is critical. It is particularly\\nrelevant in scenarios like fact-checking, where ensuring the\\naccuracy of information provided by AI models is essential.\\nBy employing this technique, prompt engineers can enhance\\nthe trustworthiness of LLMs, making them more reliable for\\ntasks that require high levels of factual accuracy.\\n4) Reflection: Reflection [160] involves prompting LLMs\\nto assess and potentially revise their own outputs based on\\nreasoning about the correctness and coherence of their re-\\nsponses. The concept of Reflection centers on the ability of\\nLLMs to engage in a form of self-evaluation. After generating\\nan initial response, the model is prompted to reflect on its\\nown output, considering factors like factual accuracy, logical\\nconsistency, and relevance. This introspective process can lead\\nto the generation of revised or improved responses.\\nA key aspect of Reflection is the LLM’s capacity for\\nself-editing. By evaluating its initial response, the model can\\nidentify potential errors or areas of improvement. This iterative\\nprocess of generation, reflection, and revision enables the LLM\\nto refine its output, enhancing the overall quality and reliability\\nof its responses.\\n5) Expert Prompting: Expert Prompting [161] enhances the\\ncapabilities of Large Language Models (LLMs) by simulating\\nthe responses of experts in various fields. This method involves\\nprompting the LLMs to assume the role of an expert and re-\\nspond accordingly, providing high-quality, informed answers.\\nA key strategy within Expert Prompting is the multi-expert\\napproach. The LLM is prompted to consider responses from\\nmultiple expert perspectives, which are then synthesized to\\nform a comprehensive and well-rounded answer. This tech-\\nnique not only enhances the depth of the response but also\\nincorporates a range of viewpoints, reflecting a more holistic\\nunderstanding of the subject matter.\\n6) Chains: Chains refer to the method of linking multiple\\ncomponents in a sequence to handle complex tasks with Large\\nLanguage Models (LLMs). This approach involves creating a\\nseries of interconnected steps or processes, each contributing\\nto the final outcome. The concept of Chains is based on\\nthe idea of constructing a workflow where different stages\\nor components are sequentially arranged. Each component in\\na Chain performs a specific function, and the output of one\\nserves as the input for the next. This end-to-end arrangement\\nallows for more complex and nuanced processing, as each\\nstage can be tailored to handle a specific aspect of the task.\\nChains can vary in complexity and structure, depending on\\nthe requirements. In “PromptChainer: Chaining Large Lan-\\nguage Model Prompts through Visual Programming” [162],\\nthe authors not only describe the main challenges in designing\\nchains, but also describe a visual tool to support those tasks.\\n7) Rails: Rails in advanced prompt engineering refer to\\na method of guiding and controlling the output of Large\\nLanguage Models (LLMs) through predefined rules or tem-\\nplates. This approach is designed to ensure that the model’s\\nresponses adhere to certain standards or criteria, enhancing the\\nrelevance, safety, and accuracy of the output. The concept of\\nRails involves setting up a framework or a set of guidelines\\nthat the LLM must follow while generating responses. These\\nguidelines are typically defined using a modeling language or\\ntemplates known as Canonical Forms, which standardize the\\nway natural language sentences are structured and delivered.\\nRails can be designed for various purposes, depending on\\nthe specific needs of the application:\\n•\\nTopical Rails: Ensure that the LLM sticks to a\\nparticular topic or domain.\\n•\\nFact-Checking Rails: Aimed at minimizing the gen-\\neration of false or misleading information.\\n•\\nJailbreaking Rails: Prevent the LLM from generating\\nresponses that attempt to bypass its own operational\\nconstraints or guidelines.\\n8) Automatic\\nPrompt\\nEngineering\\n(APE):\\nAutomatic\\nPrompt Engineering (APE) [163] focuses on automating the\\nprocess of prompt creation for Large Language Models\\n(LLMs). APE seeks to streamline and optimize the prompt\\ndesign process, leveraging the capabilities of LLMs themselves\\nto generate and evaluate prompts. APE involves using LLMs\\nin a self-referential manner where the model is employed\\nto generate, score, and refine prompts. This recursive use of\\nLLMs enables the creation of high-quality prompts that are\\nmore likely to elicit the desired response or outcome.\\nThe methodology of APE can be broken down into several\\nkey steps:\\n•\\nPrompt Generation: The LLM generates a range of\\npotential prompts based on a given task or objective.\\n•\\nPrompt Scoring: Each generated prompt is then\\nevaluated for its effectiveness, often using criteria\\nlike clarity, specificity, and likelihood of eliciting the\\ndesired response.\\n•\\nRefinement and Iteration: Based on these evalua-\\ntions, prompts can be refined and iterated upon, further\\nenhancing their quality and effectiveness.\\nC. Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a\\nknowledge graph, see figure 38 ). The relevant information is\\nthen added to the original prompt and fed to the LLM in order\\nfor the model to generate the final response. A RAG system\\nincludes three important components: Retrieval, Generation,\\nAugmentation [165].\\na) RAG-aware prompting techniques: Because of the\\nimportance of RAG to build advanced LLM systems, several\\nRAG-aware prompting techniques have been developed re-\\ncently. One such technique is Forward-looking Active Retrieval\\nAugmented Generation (FLARE)\\nForward-looking Active Retrieval Augmented Generation\\n(FLARE) [168] enhances the capabilities of Large Language\\nModels (LLMs) by iteratively combining prediction and in-\\nformation retrieval. FLARE represents an evolution in the\\nuse of retrieval-augmented generation, aimed at improving the\\naccuracy and relevance of LLM responses.\\nFLARE involves an iterative process where the LLM\\nactively predicts upcoming content and uses these predictions\\nas queries to retrieve relevant information. This method con-\\ntrasts with traditional retrieval-augmented models that typically\\nretrieve information once and then proceed with generation. In\\nFLARE, this process is dynamic and ongoing throughout the\\ngeneration phase. In FLARE, each sentence or segment gener-\\nated by the LLM is evaluated for confidence. If the confidence\\nlevel is below a certain threshold, the model uses the generated\\ncontent as a query to retrieve relevant information, which is\\nthen used to regenerate or refine the sentence. This iterative\\nFig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\\nFig. 38: This is one example of synthesizing the KG as a\\nretriever with LLMs [167].\\nprocess ensures that each part of the response is informed by\\nthe most relevant and current information available.\\nFor more details on RAG framework and its relevant works,\\nwe refer the readers to this survey of retrieval augmented\\ngenerations [165].\\nD. Using External Tools\\nRetrieving information from an external knowledge source\\nas described above is only one of the potential ways to augment\\nan LLM. More generally, an LLM can access any number\\nof external tools (e.g. an API to a service) to augment its\\nfunctionality. In that regards, RAG can be seen as a specific\\ninstance of the broader category of the so called ”tools”.\\nTools in this context are external functions or services that\\nLLMs can utilize. These tools extend the range of tasks an\\nLLM can perform, from basic information retrieval to complex\\ninteractions with external databases or APIs.\\nIn the paper ”Toolformer: Language Models Can Teach\\nThemselves to Use Tools” [169], the authors go beyond simple\\ntool usage by training an LLM to decide what tool to use\\nwhen, and even what parameters the API needs. Tools include\\ntwo different search engines, or a calculator. In the following\\nexamples, the LLM decides to call an external Q&A tool,\\na calculator, and a Wikipedia Search Engine More recently,\\nresearchers at Berkeley have trained a new LLM called Gorilla\\n[67] that beats GPT-4 at the use of APIs, a specific but quite\\ngeneral tool.\\na) Tool-aware prompting techniques: Similarly to what\\nwas described with RAG, several tool-aware prompting ap-\\nproaches have been developed to make usage of tools more\\nscalable. A popular technique is the so called Automatic Multi-\\nstep Reasoning and Tool-use (ART).\\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\\nis a prompt engineering technique that combines automated\\nchain of thought prompting with the use of external tools.\\nART represents a convergence of multiple prompt engineering\\nstrategies, enhancing the ability of Large Language Models\\n(LLMs) to handle complex tasks that require both reasoning\\nand interaction with external data sources or tools.\\nART involves a systematic approach where, given a task\\nand input, the system first identifies similar tasks from a task\\nlibrary. These tasks are then used as examples in the prompt,\\nguiding the LLM on how to approach and execute the current\\ntask. This method is particularly effective when tasks require a\\ncombination of internal reasoning and external data processing\\nor retrieval.\\nE. LLM Agents\\nThe idea of AI agents has been well-explored in the history\\nof AI. An agent is typically an autonomous entity that can\\nperceive the environment using its sensors, make a judgment\\nbased on the state it currently is, and accordingly act based on\\nthe actions that are available to it.\\nIn the context of LLMs, an agent refers to a system based\\non a specialized instantiation of an (augmented) LLM that\\nis capable of performing specific tasks autonomously. These\\nagents are designed to interact with users and environment to\\nmake decisions based on the input and the intended goal of\\nthe interaction. Agents are based on LLMs equipped with the\\nability to access and use tools, and to make decisions based on\\nthe given input. They are designed to handle tasks that require\\na degree of autonomy and decision-making, typically beyond\\nsimple response generation.\\nThe functionalities of a generic LLM-based agent include:\\n•\\nTool Access and Utilization: Agents have the capabil-\\nity to access external tools and services, and to utilize\\nthese resources effectively to accomplish tasks.\\n•\\nDecision Making: They can make decisions based on\\nthe input, context, and the tools available to them,\\noften employing complex reasoning processes.\\nAs an example, an LLM that has access to a function (or\\nan API) such as weather API, can answer any question related\\nto the weather of the specific place. In other words, it can use\\nAPIs to solve problems. Furthermore, if that LLM has access\\nto an API that allows to make purchases, a purchasing agent\\ncan be built to not only have capabilities to read information\\nfrom the external world, but also act on it [171].\\nFig. 40 shows another example of LLM-based agents for\\nconversational information seeking [36], where an LLM is\\naugmented with a set of plug-and-play modules, including\\na working memory that tracks the dialog state, a policy that\\nmakes an execution plan for the task and selects next system\\naction, an action executor that performs an action selected by\\nthe policy (consolidating evidence from external knowledge,\\nor prompting the LLM to generate responses), and a utility\\nthat accesses the alignment of the LLM’s responses with user\\nexpectations or specific business requirements, and generate\\nfeedback to improve agent performance.\\nFor more details on LLM-based AI agents see recent survey\\n[172], [173], [174].\\na) Prompt engineering techniques for agents:\\nLike\\nRAG and Tools, prompt engineering techniques that specif-\\nically address the needs of LLM-based agents have been\\ndeveloped. Three such examples are Reasoning without Ob-\\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\\nEnabled Resolving Agents (DERA).\\nReasoning without Observation (ReWOO) [175] aims to\\ndecouple reasoning from direct observations. ReWOO operates\\nby enabling LLMs to formulate comprehensive reasoning plans\\nor meta-plans without immediate reliance on external data\\nor tools. This approach allows the agent to create a struc-\\ntured framework for reasoning that can be executed once the\\nnecessary data or observations are available. In ReWOO, the\\nLLM initially develops a plan (a series of steps) that outlines\\nhow to approach and solve a given problem. This meta-\\nplanning phase is crucial as it sets the stage for the agent to\\nprocess information once it becomes available. The execution\\nphase then involves integrating actual data or observations into\\nthe pre-specified plan, leading to coherent and contextually\\nrelevant responses. ReWOO offers significant advantages in\\nterms of token efficiency and robustness to tool failure. It\\nenables LLMs to handle tasks where immediate access to\\nexternal data is not available, relying instead on a well-\\nstructured reasoning framework. This method is particularly\\nadvantageous in scenarios where data retrieval is costly, slow,\\nor uncertain, allowing the LLM-based agent to maintain a high\\nlevel of performance and reliability.\\nReason and Act (ReAct)[176] prompts LLMs to generate\\nnot only verbal reasoning but also actionable steps, thus\\nenhancing the model’s dynamic problem-solving capabilities.\\nReAct is grounded in the principle of integrating reasoning\\nwith action. In this approach, the LLM is prompted to alternate\\nbetween generating reasoning traces (explanations) and taking\\nactions (steps or commands) in an interleaved manner. This\\napproach allows the model to dynamically reason about a prob-\\nlem, and propose and take concrete actions simultaneously.\\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\\ncialized AI agents that can engage in dialogue, resolve queries,\\nand make decisions based on interactive exchanges. DERA\\nis developed based on the idea of utilizing multiple agents\\nwithin a dialog context, each with specific roles and functions.\\nThese agents can include Researchers, who gather and analyze\\ninformation, and Deciders, who make final judgments based\\non the information provided. This division of roles allows for\\na well-organized and efficient approach to problem-solving\\nand decision-making. DERA is particularly advantageous in\\nscenarios requiring complex decision-making and problem-\\nsolving, such as those in medical diagnostics or customer ser-\\nvice. The collaborative and interactive nature of DERA agents\\nallows them to handle intricate queries with a level of depth\\nand nuance that single-agent systems might struggle with.\\nMoreover, this approach aligns well with human decision-\\nmaking processes, making AI reasoning more relatable and\\ntrustworthy.\\nV.\\nPOPULAR DATASETS FOR LLMS\\nLarge language models exhibit promising accomplish-\\nments, but the main question that arises is how effectively\\nthey function and how their performance can be assessed in\\nspecific tasks or applications.\\nThe evaluation of LLMs poses particular challenges due\\nto the evolving landscape of their applications. The original\\nintent behind developing LLMs was to boost the performance\\nof NLP tasks such as translation, summarization, question-\\nanswering, and so on [178]. However, it is evident today\\nthat these models are finding utility across diverse domains\\nincluding code generation and finance. Moreover, the eval-\\nuation of LLMs encompasses several critical considerations\\nsuch as fairness and bias, fact-checking, and reasoning. In\\nthis section, we outline the commonly used benchmarks for\\nassessing LLMs. These benchmarks are categorized based on\\ntraining or evaluating the LLM Capabilities.\\nA. Datasets\\nfor\\nBasic\\nTasks:\\nlanguage\\nmodel-\\ning/understanding/generation\\nThis section provides an overview of the benchmarks and\\ndatasets suited to evaluate the basic abilities of LLMs.\\n•\\nNatural Questions [179] is a QA dataset that consists\\nof real anonymized, aggregated queries submitted to\\nthe Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5 search results, and annotates a\\nlong answer (typically a paragraph) and a short answer\\nFig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\\nFig. 40: A LLM-based agent for conversational information\\nseeking. Courtesy of [36].\\n(one or more entities) if present on the page, or marks\\nnull if no long/short answer is present.\\n•\\nMMLU [180] is intended to evaluate the knowl-\\nedge gained in zero-shot and few-shot scenarios. That\\nmeans that MMLU assesses both the general knowl-\\nedge and problem-solving ability of a model. It covers\\n57 subjects in STEM, humanities, social sciences,\\nand other areas. The benchmark varies in complexity,\\nranging from elementary to advanced professional.\\nIt is worth mentioning that the main contribution of\\nthis dataset is for multi-task language understanding,\\nquestion answering, and arithmetic reasoning.\\n•\\nMBPP [181] stands for “Mostly Basic Python Prob-\\nlems” and provides a benchmark for evaluating the\\nperformance of models designed for code generation.\\nThe benchmark encompasses 974 short Python pro-\\ngrams including a wide range of topics, including\\nfundamental programming concepts and standard li-\\nbrary usage, and more. Each challenge comprises a\\ntask description, a code solution, and three automated\\ntest cases.\\n•\\nHumanEval [182] is a dataset for code generation\\ntask. This dataset consists of 164 hand-crafted pro-\\ngramming challenges. Each challenge is accompanied\\nby a function signature, docstring, code body, and mul-\\ntiple unit tests. The main intuition behind developing\\nthis dataset is to guarantee the exclusion of its contents\\nfrom training datasets for code generation models.\\n•\\nAPPS [183] is designed for code generation task\\nfocusing on the Python programming language. The\\nAPPS dataset contains a collection of 232, 444 Python\\nprograms. Each program in the dataset has an average\\nof 18 lines of Python code. Additionally, APPS offers\\naccess to a repository of 10, 000 unique programming\\nexercises, each with text-based problem descriptions.\\nThe final aspect to highlight is that the it includes test\\ncases.\\n•\\nWikiSQL [184] is crafted for code generation task and\\nit has 87,726 carefully labeled pairs of SQL queries\\nand corresponding natural language questions from\\nWikipedia tables. The SQL queries comprise three\\nsubsets: test sets (17, 284 examples), development\\n(9, 145 examples), and training (61, 297 examples).\\n•\\nTriviaQA [185] is designed for QA task. This\\ndataset\\ncomprises\\nmore\\nthan\\n650, 000\\nquestion-\\nanswer-evidence triples. There are 95, 000 question-\\nanswer pairs in this dataset, each authored by trivia en-\\nthusiasts and supported by an average of six indepen-\\ndently sourced evidence documents. These documents\\nare automatically acquired from Wikipedia or broader\\nweb search results. The dataset is categorized into\\ntwo segments, including those with authentic answers\\nfrom Wikipedia and web domains, and verified sets\\nembody the accurately answered questions along with\\ntheir associated documents from both Wikipedia and\\nonline.\\nFig. 41: Dataset applications.\\n•\\nRACE [186] suits for reading comprehension task.\\nThis dataset is based on English tests completed by\\nChinese students from middle school and high school,\\naged 12 to 18, and it contains roughly 28, 000 texts\\nand 100, 000 questions rigorously prepared by human\\nspecialists, primarily English instructors. This dataset\\ncontains a wide range of subjects that were purpose-\\nfully chosen to assess students’ comprehension and\\nreasoning abilities. This dataset is available in three\\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\\nM refers to the middle school examinations, whereas\\nRACE-H denotes the high school tests. Finally, RACE\\nis the synthesis of RACE-M and RACE-H.\\n•\\nSQuAD [187] stands for “Stanford Question Answer-\\ning Dataset” and is a crowdsourced reading compre-\\nhension dataset based on Wikipedia articles. It has\\napproximately 100, 000 question-answer pairs con-\\nnected to more than 500 articles. The answers to\\nthese questions are typically text fragments or spans\\ntaken from the corresponding reading passages. The\\nquestions may be unanswerable in some cases. The\\ndataset is divided into three sets: an 80% training set,\\na 10% development set, and a 10% hidden test set.\\nFig. 42: Datasets licensed under different licenses.\\n•\\nBoolQ [188] is a yes/no question-answering dataset\\nwhere the goal is reading comprehension task. BoolQ\\nincludes 15, 942 examples. Each example is a triplet\\nthat includes a question, a relevant paragraph, and\\nthe solution. Although the main intuition behind\\nthis dataset is for reading comprehension, it can be\\nused for reasoning, natural language inference, and\\nquestion-answering tasks.\\n•\\nMultiRC [189] is another dataset that fits reading\\ncomprehension task. MultiRC contains brief para-\\ngraphs as well as multi-sentence questions that can\\nbe answered using the information in the paragraph.\\nThe paragraphs in this dataset come from a variety\\nof sources, including news, fiction, historical texts,\\nWikipedia articles, discussions on society and law,\\nelementary school science textbooks, and 9/11 re-\\nports. Each question has many response choices, with\\none or more of them being correct. Answering the\\nquestions requires reasoning across several sentences.\\nMultiRC dataset encompasses around 6, 000 multi-\\nsentence questions gathered from over 800 paragraphs.\\nOn average, each question offers about two valid\\nanswer alternatives out of a total of five.\\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\\nfollowing\\nThis section centers on the benchmarks and datasets em-\\nployed to evaluate the emergent abilities of LLMs.\\n•\\nGSM8K [190] is designed to evaluate the model’s\\nability for multi-step mathematical reasoning. GSM8K\\nincludes 8.5K linguistically diverse grade school math\\nword problems written by humans. The dataset is split\\ninto two sets: a training set with 7.5K problems,\\nand a test set with 1K problems. These problems\\nneed 2 to 8 steps to be solved. Solutions mainly\\nare a series of elementary calculations using basic\\narithmetic operations.\\n•\\nMATH [191] enables to assess how well models can\\nsolve math problems. MATH dataset hast 12, 500\\nproblems from high school math competitions. Each\\nproblem in the dataset has a step-by-step solution and\\na final answer enclosed in a box. The problems cover\\na wide range of topics and have different levels of\\ncomplexity. There are seven subjects in total. Further-\\nmore, the difficulty of each problem is rated based\\non the AoPS standards on a scale from ′1′ to ′5′. A\\n′1′ shows the easiest problems in a subject, while ′5′\\nrepresents the most difficult. In terms of formatting,\\nall problems and solutions are presented using LATEX\\nand the Asymptote vector graphics language.\\n•\\nHellaSwag [192] is designed to assess commonsense\\nreasoning in LLMs. This benchmark includes 70, 000\\nmultiple-choice questions. Each question is derived\\nfrom one of two domains: ActivityNet or WikiHow,\\nand presents four answer choices regarding what\\nmight happen in the following situation. The correct\\nanswer provides an actual statement describing the\\nupcoming event, but the three wrong answers are\\ncreated to confuse machines.\\n•\\nAI2 Reasoning Challenge (ARC) [193] is used\\nfor commonsense reasoning. This benchmark encom-\\npasses 7, 787 science examination questions. These\\nquestions are in English, and most of them are set\\nup in a multiple-choice format. The questions have\\nbeen divided into two groups: a Challenge Set with\\n2, 590 difficult questions and an Easy Set with 5,197\\nquestions. Each collection has also been pre-divided\\ninto Train, Development, and Test subsets.\\n•\\nPIQA [194] is intended to evaluate the language\\nrepresentations on their knowledge of physical com-\\nmonsense. In this dataset, the focus is on everyday\\nsituations with a preference for uncommon solutions.\\nThe central task is a multiple-choice question answer-\\ning, where a question (q) is provided along with two\\npotential solutions (s1, s2). Then, the best solution is\\nchosen by whether a model or a human. For each\\nquestion, only one of the solutions is the correct\\nanswer.\\n•\\nSIQA [195] provides a framework for evaluating mod-\\nels’ ability for commonsense reasoning about social\\nsituations. SIQA dataset has 38, 000 multiple-choice\\nquestions designed to assess emotional and social\\nintelligence in everyday circumstances. This dataset\\ncovers a wide variety of social scenarios. In SIQA,\\nthe potential answers is a mixture of human-selected\\nresponses and machine-generated ones that have been\\nfiltered through adversarial processes.\\n•\\nOpenBookQA (OBQA) [196] is a new kind of\\nquestion-answering dataset where answering its ques-\\ntions requires additional common and commonsense\\nknowledge not contained in the book and rich text\\ncomprehension. This dataset includes around 6,000\\nmultiple-choice questions. Each question is linked to\\none core fact, as well as an additional collection\\nof over 6000 facts. The questions were developed\\nusing a multi-stage crowdsourcing and expert filter-\\ning procedure. OpenBookQA questions are difficult\\nbecause they need multi-hop reasoning with limited\\nbackground.\\n•\\nTruthfulQA [197] is designed specifically to eval-\\nuate the truthfulness of language models in gen-\\nerating answers to questions. This dataset includes\\n817 questions, written by authors, from 38 different\\ncategories, including health, law, finance, and politics.\\nThese questions are purposefully designed to chal-\\nlenge human responders, as they may contain common\\nmisunderstandings that lead to incorrect answers.\\n•\\nOPT-IML Bench [103] is a comprehensive bench-\\nmark for Instruction Meta-Learning. It covers 2000\\nNLP tasks from 8 existing benchmarks. The OPT-IML\\nBench consists of a training set with 17.9 M examples,\\na dev set with 145K samples, and a test set with 321K\\nsamples.\\nC. Datasets for Augmented: using external knowledge/tools\\nThis section focuses on datasets designed for the aug-\\nmented abilities of LLMs.\\n•\\nHotpotQA [198] is designed to cover a diverse and\\nexplainable question-answering dataset that necessi-\\ntates multi-hop reasoning. This dataset is derived from\\nthe English Wikipedia. It consists of roughly 113, 000\\nquestions. Each question in the dataset comes with\\ntwo paragraphs, called gold paragraphs, from two\\nWikipedia articles. Also, there is a list of sentences\\nin those paragraphs that crowdworkers have picked as\\nimportant for answering the question.\\n•\\nToolQA [199] is a question answering benchmark\\nto evaluate LLMs’ ability to use external tools for\\nanswering questions.\\n•\\nGPT4Tools serves as an instructional dataset, gener-\\nated by instructing advanced teachers (such as Chat-\\nGPT), with instructions conditioned on visual content\\nand tool descriptions. This process results in the\\ngeneration of instructions related to the use of tools.\\nThere are three versions of this dataset. The first\\nversion comprises 71,000 instruction-following data\\npoints utilized to fine-tune the GPT4Tools model. The\\nnext version consists of manually cleaned instruction\\ndata used for validation, covering instructions related\\nto the tools from the first version. The last version is\\ncleaned instruction data used for testing and includes\\ninstructions related to some tools that are not present\\nin the first version.\\nVI.\\nPROMINENT LLMS’ PERFORMANCE ON\\nBENCHMARKS\\nIn this section we first provide an overview of some of\\npopular metrics used for evaluating the performance of LLMs\\nunder different scenarios. We then look at the performance\\nof prominent large language models on some of the popular\\ndatasets and benchmarks.\\nA. Popular Metrics for Evaluating LLMs\\nEvaluating the performance of generative language models\\ndepends on the underlying task they are going to be used for.\\nTasks that are mostly about selecting a choice out of given\\nones (such as sentiment analysis), can be seen as simple as\\nclassification and their performance can be evaluated using\\nclassification metrics. Metrics such as accuracy, precision,\\nrecall, F1, etc are applicable in this case. It is also important to\\nnote that the answers generated by the model for specific tasks\\nsuch as multi-choice question answering are always either True\\nor False. If the answer is not in a set of options, it can be seen\\nas False as well.\\nHowever, some tasks that are purely open-ended text gener-\\nation cannot be evaluated in the same way as for categorization.\\nDifferent metrics are required for the specific purpose of the\\nevaluation. Code generation is a very different case in open-\\nended generative evaluations. The generated code must pass\\nthe test suite but on the other hand, it is also important\\nto understand if a model is capable of generating different\\nTABLE II: LLM Datasets Overview.\\nBenchmark Name\\nEvaluation Metric\\nLeaderboard\\nSource\\npaperswithcode\\nHumanEval\\nPASS@k\\nLink\\nLink\\nLink\\nMBPP\\nPASS@k, Accuracy\\n-\\nLink\\nLink\\nAPPS\\nPASS@k, Accuracy\\n-\\nLink\\nLink\\nWikiSQL\\nAccuracy\\n-\\nLink\\nLink\\nCoNaLa\\nBLEU\\nLink\\nLink\\nCodeParrot\\nPASS@k\\n-\\nLink\\n-\\nHellaSwag\\nAccuracy\\nLink\\nLink\\nLink\\nAI2\\nReasoning\\nChallenge (ARC)\\nAccuracy\\nLink\\nLink\\nLink\\nBoolQ\\nAccuracy\\n-\\nLink\\nLink\\nMultiRC\\nF1-score, Accuracy\\n-\\nLink\\nLink\\nCNN/Daily Mail [200]\\nAccuracy\\n-\\nLink\\n-\\nSQuAD\\nF1-score, EM\\nLink\\nLink\\nLink\\nRACE\\nAccuracy\\n-\\nLink\\nLink\\nCNN/Daily Mail [201]\\nROUGE\\n-\\nLink\\nLink\\nDrop\\nF1-score, EM\\nLink\\nLink\\nLink\\nQuAC\\nF1-score, HEQ-Q, HEQ-D\\nLink\\nLink\\nLink\\nTriviaQA\\nEM, F1-score, Accuracy\\nLink\\nLink\\nLink\\nNatural Questions\\nEM, F1-score, Accuracy\\nLink\\nLink\\nLink\\nStrategyQA\\nAccuracy, Recall@10, SARI\\nLink\\nLink\\nLink\\nCoQA\\nF1-score\\nLink\\nLink\\nLink\\nXSum\\nROUGE\\n-\\nLink\\nLink\\nSAMSum\\nROUGE\\n-\\n-\\nLink\\nWikiSum\\nROUGE\\n-\\nLink\\n-\\nDialogSum\\nROUGE\\n-\\nLink\\nLink\\nTruthfulQA\\nMC1 , MC2, % true, % info, BLEURT\\nLink\\nLink\\nLink\\nMMLU\\nAccuracy\\nLink\\nLink\\nLink\\nGSM8K\\nAccuracy\\nLink\\nLink\\nLink\\nPIQA\\nAccuracy\\nLink\\nLink\\nLink\\nSIQA\\nAccuracy\\nLink\\nLink\\nLink\\nOpenBookQA (OBQA)\\nAccuracy\\nLink\\nLink\\nLink\\nHotpotQA\\nEM, F1-score, Joint EM, Joint F1-score,\\nLink\\nLink\\nLink\\nMATH\\nAccuracy\\n-\\nLink\\nLink\\nCommonsenseQA\\nAccuracy\\nLink\\nLink\\nLink\\nNatural Instructions\\nROUGE-L, Human\\nLink\\nLink\\nLink\\nBIG-bench\\nAccuracy, Average\\n-\\nLink\\nLink\\nToolTalk\\nSuccess rate, Precision, Recall, Incorrect\\naction rate, Percent of failing error types\\n-\\nLink\\nLink\\nMetaTool\\nAccuracy, Precision, Recall, F1-score\\n-\\nLink\\nLink\\nGPT4Tools\\nSuccessful Rate of Thought, Successful\\nRate of Action, Successful Rate of Ar-\\nguments, Success Rate\\n-\\nLink\\nLink\\nAPI-Bank\\nCorrectness, ROUGE, Error(API Hallu-\\ncination, Has Exception, Invalid Input\\nParameters, False API Call Format, API\\nCall, Miss Input Parameters)\\n-\\nLink\\nLink\\nAlpaca-CoT\\n-\\n-\\nLink\\nLink\\nsolutions as a code, what is the probability of selecting the\\ncorrect one among them. Pass@k is a very good metric in this\\ncase. It works in this manner that given a problem, different\\nsolutions as code are generated. They are tested for correctness\\nusing different functionality tests. Afterward, from generated\\nn solutions, and the respective c number of them being correct\\nequation 4 provides the final value.\\npass@k :=\\nE\\nProblems\\n\"\\n1 −\\n\\x00n−c\\nk\\n\\x01\\n\\x00n\\nk\\n\\x01\\n#\\n(4)\\nExact match (EM) is another metric that is mostly con-\\ncerned with exact matches from (pre-defined) answers. It\\ncounts a prediction as correct if it exactly matches one of\\nmore than one desired reference text token by token. In some\\ncases, it can be the same as accuracy and the equation 5 shows\\nthe mathematical definition. Here M is total number of correct\\nanswers and N is the total number of questions [202].\\nEM = M\\nN\\n(5)\\nHuman equivalence score (HEQ) on the other hand, is an\\nalternative to F1 score [203]. HEQ-Q represents the precision\\nof individual questions, wherein an answer is deemed correct\\nif the model’s F1 score surpasses the average human F1 score.\\nLikewise, HEQ-D denotes the precision of each dialogue; it is\\ndeemed accurate when all questions within the dialogue meet\\nthe criteria of HEQ [182].\\nEvaluation of other generative tasks such as machine trans-\\nlation are based on metrics such as Rouge and BLEU. These\\nscores work well when there is a reference text as ground\\ntruth (such as translation) and a hypothesis that is generated\\nby the generative model, in our case the LLM. These scores\\nare mostly used for cases where the goal is to detect the\\nsimilarity of the answer and ground truth in a computation\\nmanner. In a computation manner, it meant that nothing more\\nthan N-Grams would be used. However, metrics such as BERT-\\nScore are also good for these cases but they are also heavily\\nTABLE III: LLM categories and respective definitions.\\nClassification\\nCategory\\nDescription\\nSize\\nSmall\\nNumber of parameters ≤1B\\nMedium\\n1B < Number of parameters ≤10B\\nLarge\\n10B < Number of parameters ≤100B\\nVery Large\\n100B < Number of parameters\\nType\\nFoundation model\\nPretrained language model\\nInstruction model\\nPretrained and instruction fine-tuned language model\\nChat model\\nPretrained, instruction fine-tuned, and chat fine-tuned language model\\nOrigin\\nOriginal model\\nAn original model released with either Foundation, Instruction, or Chat model\\nTuned model\\nFine-tuned version of an original model\\nAvailability\\nPublicly available\\nModel and weights are available due to request to without request\\nPublicly unavailable\\nModel and weights are not publicly available\\nTABLE IV: Different LLM categorization.\\nModel\\nSize\\n#Params (B)\\nType\\nAvailability\\nOrigin\\nDavinci-002\\nVery Large\\n175\\nInstruction\\nUnavailable\\nTuned\\nDavinci-003\\nVery Large\\n175\\nInstruction\\nUnavailable\\nTuned\\nGPT 3.5-turbo\\nLarge\\n20\\nChat\\nUnavailable\\nTuned\\nFalcon 7B\\nMedium\\n7\\nFoundation\\nPublic\\nOriginal\\nAlpaca\\nLarge\\n13\\nChat\\nPublic\\nTuned\\nPythia 7B\\nMedium\\n7\\nFoundation\\nPublic\\nOriginal\\nPythia 12B\\nLarge\\n12\\nFoundation\\nPublic\\nOriginal\\nLLAMA 7B\\nMedium\\n7\\nChat\\nPublic\\nOriginal\\nLLAMA 2 7B\\nMedium\\n7\\nChat\\nPublic\\nTuned\\nLLAMA 2 7B\\nMedium\\n7\\nFoundation\\nPublic\\nOriginal\\nVicuna 13B\\nLarge\\n13\\nFoundation\\nPublic\\nTuned\\nVicuna 7B\\nMedium\\n7\\nFoundation\\nPublic\\nTuned\\nClaude\\nLarge\\n93\\nChat\\nUnavailable\\nOriginal\\nClaude 2\\nVery Large\\n137\\nChat\\nUnavailable\\nOriginal\\nerroneous because another model is used to judge. Still, even\\ntoday, evaluating purely generated content is very hard and\\nno completely fitting metric is not found, metrics are either\\nlooking for simplistic features such as N-Gram, SkipGram,\\netc, or they are models with unknown accuracy and preciseness\\n[204].\\nGenerative evaluation metrics are also another type of eval-\\nuation metric for LLMs that use another LLM for evaluating\\nthe answer. However, depending on the task itself, evaluation\\ncan be possible in this way or not. Another dependency\\nthat makes generative evaluation error-prone is reliance on\\nthe prompt itself. RAGAS is one of the good examples that\\nincorporate the usage of generative evaluation.\\nVarious benchmarks and leaderboards have been proposed\\nto address the most challenging question in the world of\\nlarge language models: Which one is better? However not\\na simple answer can address this question. The answer de-\\npends on various aspects of large language models. Section V\\nshows the categorical presentation of different tasks and the\\nmost important datasets in each category. We will follow the\\nsame categorization and provide a comparison based on each\\ncategory. After providing comparison for each category, we\\nwill provide a broad overview of aggregated performance by\\naveraging the reported performance metric on different tasks.\\nEvaluating different LLMs can be seen also from different\\nperspectives. For example, a LLM with a drastically fewer\\nnumber of parameters is not completely comparable to one\\nwith a larger number of parameters. From this perspective, we\\nwill categorize LLMs in four categories as well: small (less\\nthan or equal to 1 billion parameters), medium (between 1 and\\n10 billion), large (between 10 and 100 billion), and very large\\n(more than 100 billion). Another classification for the LLMs\\nwe use is their primary use case. We consider each LLM to\\nbe either: Foundation model (pretrained language model with\\nno instruction fine-tuning and chat fine-tuning), Instruction\\nmodel (pretrained language model with only instruction fine-\\ntuning), and Chat model (pretrained language model with\\ninstruction and chat fine-tuning). Apart from all the catego-\\nrization described, another category is required to distinguish\\nbetween original models and tuned ones. Original models are\\nthose that have been released as a foundation model or a fine-\\ntuned one. Tuned models are those that grasped the original\\nmodel and tuned it with different datasets or even different\\ntraining approaches. It is also good to note that original models\\nare usually foundation models that have been fine-tuned on\\nspecific datasets or even different approaches. Availability of\\nthe model weights regardless of the license is another category\\nin our classification. Models that have their weights publicly\\navailable (even through request) are noted as Public models\\nwhile others are noted as Private. Table III shows all of these\\ndefinitions and abbreviations used in the rest of the article.\\nFigure 43 illustrate these visually.\\nAccording to the provided categorizations, we can catego-\\nrize and label each notable LLM as shown in table IV. As can\\nbe seen from this table, models categorized as very large are\\nalso unavailable as well.\\nB. LLMs’ Performance on Different Tasks\\nCommonsense reasoning is one of the important capabili-\\nties each model can obtain. This capability denotes the ability\\nof the model to use prior knowledge in combination with\\nreasoning skills. In the case of HellaSwag for example, finding\\nthe continuation of text is challenging because the given text\\ncontains a partial part of the story while the given choices\\nas continuation are tricky to select, and without having prior\\nLarge\\nLanguage\\nModels\\nParameters\\nAvailability\\nOriginality\\nType\\nSmall LM\\n# of params <1B\\nMedium LM\\n1B < # of params <10B\\nLarge LM\\n10B < # of params <100B\\nVery Large LM\\n100B < # of params\\nTuned\\nFine tuning\\nOriginal\\nPublic\\nPrivate\\nFoundation\\nInstruction\\nChat\\nFine tuned models that are originally\\nbased on original models.\\nExample: Alpaca (based on LLaMA)\\nOriginal models that are not fine\\ntuned or based on any other\\npretrained model.\\nExample: LLaMA\\nModel weights are publicly released\\nand is available.\\nExample: LLaMA\\nModel weights are NOT publicly\\nreleased and is NOT available.\\nExample: GPT-4\\nPretrained model with no instruction\\nor chat fine-tuning.\\nExample: MPT-7B\\nPretrained model that is\\nalso fine-tuned on\\ninstruction following.\\nExample: MPT-7B-instruct\\nPretrained model that is\\nalso fine-tuned on chat.\\nExample: MPT-7B-chat\\nFig. 43: LLM categorizations.\\nknowledge about the world it is not possible. This specific kind\\nof reasoning deserves high attention because it is related to\\nutilizing previous knowledge with open text-described scenes\\nor facts. As can be seen from table V not just Unavailable\\nmodels but also Public ones can achieve good results on\\nvarious tests.\\nTABLE V: Commonsense reasoning comparison.\\nModel\\nOBQA\\nHellaSwag\\nDavinci-003\\n51\\n83.4\\nFalcon 7B\\n44.4\\n76.3\\nAlpaca\\n43.4\\n73.9\\nPythia 7B\\n37.2\\n64\\nPythia 12B\\n43.2\\n68.1\\nLLAMA 7B\\n42.4\\n73\\nDolly 6B\\n41.2\\n67.6\\nDolly 12B\\n40.4\\n71\\nAlpaca 7B\\n43.4\\n73.9\\nAlpaca Lora 7B\\n42.6\\n74\\nGPT-J 6.7B\\n38.2\\n66.2\\nLLama 7B\\n42.4\\n73\\nLLama 13B\\n42.2\\n76.2\\nPythia 6.7B\\n37.2\\n64\\nPythia 12B\\n38\\n67.3\\nStableLM Tuned\\n33.4\\n53.6\\nKoala 13B\\n42.8\\n72.6\\nMosaic mpt-7B\\n42.6\\n76.3\\nLLAMA 2 70B\\n-\\n87.33\\nLLAMA 65B\\n-\\n86.09\\nFalcon 40B\\n-\\n85.3\\nFalcon 180B\\n-\\n88.86\\nMPT Instruct 30B\\n-\\n84.31\\nMPT Instruct 7B\\n-\\n77.91\\nYi 6B\\n-\\n76.42\\nYi 34B\\n-\\n85.69\\nGPT-4\\n-\\n95.3\\nGemini Ultra\\n-\\n87.8\\nFrom the results presented in Table V it is clear that GPT-4\\nachieves best results for HellaSwag while Davinci-003 is best\\nmodel for OBQA. It is also good to note that results for OBQA\\nare not reported for all of the models and possibly davinci-003\\nis not the best model achieving highest results on OBQA.\\nNot all models report their performance on all datasets, and\\nbecause of that, the number of models for which performance\\nis reported in different tables varies.\\nTABLE VI: Symbolic reasoning comparison.\\nModel\\nCobjects\\nPenguins\\nGPT-NeoX\\n26\\n33.56\\nOPT 66B\\n31.2\\n28.08\\nBloomberg GPT\\n34.8\\n37.67\\nBLOOM 176B\\n36.8\\n40.41\\nPaLM 540B\\n38\\n44.5\\nGopher-280B\\n49.2\\n40.6\\nChinchilla-70B\\n59.7\\n48.7\\nPaLM 2\\n61.2\\n65.8\\nWorld knowledge is mostly about general knowledge ques-\\ntions, for example, in Wikifact dataset questions such as ”Who\\nis the author of a specific well-known book” can be found and\\nreferences are also provided. Table VII shows the results.\\nTABLE VII: World knowledge comparison.\\nModel\\nTriviaQA\\nNaturalQ\\nWebQ\\nARC\\nBLOOM\\n-\\n-\\n-\\n32.9\\nBLOOM 176B\\n-\\n-\\n-\\n50.85\\nBloomberg GPT\\n-\\n-\\n-\\n48.63\\nChinchilla\\n-\\n35.5\\n-\\n-\\nCodex + REPLUG\\n76.8\\n44.7\\n-\\n-\\nGAL 120B\\n-\\n-\\n-\\n67.9\\nGLaM 62B/64E\\n75.8\\n32.5\\n15.5\\n50.3\\nGopher\\n-\\n28.2\\n-\\n-\\nGPT-3 175B\\n71.2\\n29.9\\n41.5\\n85.2\\nGPT-4\\n-\\n-\\n-\\n96.4\\nGPT-NeoX\\n-\\n-\\n-\\n45.39\\nLLaMA 13B\\n-\\n-\\n-\\n52.7\\nLLaMA 2 70B\\n85\\n33\\n-\\n-\\nLLaMA 33B\\n-\\n24.9\\n-\\n57.8\\nLLaMA 65B\\n72.6\\n39.9\\n-\\n-\\nLLaMA 7B\\n-\\n-\\n-\\n47.6\\nMistral 7B\\n69.9\\n28.8\\n-\\n55.5\\nNeo-6B\\n-\\n13.7\\n-\\n-\\nOPT\\n-\\n-\\n-\\n31.1\\nOPT 66B\\n-\\n-\\n-\\n44.54\\nOPT-175B\\n-\\n-\\n-\\n43.94\\nOPT-175B\\n-\\n-\\n-\\n25.6\\nPaLM 2-L\\n86.1\\n37.5\\n28.2\\n95.1\\nPaLM 2-M\\n81.7\\n32\\n26.9\\n64.9\\nPaLM 2-S\\n75.2\\n25.3\\n21.8\\n59.6\\nPaLM-540B\\n81.4\\n39.6\\n43.5\\n87.1\\nphi-1.5-web 1.3B\\n-\\n-\\n-\\n44.9\\nSparseGPT\\n-\\n-\\n-\\n38.99\\nSparseGPT\\n-\\n-\\n-\\n39.85\\nSparseGPT\\n-\\n-\\n-\\n41.3\\nFor some specific use-case models, it is highly demanded to\\nhave coding and code-generation capability. Table VIII shows\\nthe results of different models on coding capability.\\nTABLE VIII: Coding capability comparison.\\nModel\\nHumanEval\\nGemini Ultra\\n74.4\\nGemini Pro\\n67.7\\nGPT-4\\n67\\nWizardCoder 15B\\n57.3\\nphi-1 1.3B\\n50.6\\nCode Llama\\n48.8\\nGPT-3.5\\n48.1\\nOctoCoder\\n46.2\\nphi-1-small\\n45\\nPaLM 2-S\\n37.6\\nInstructCodeT5+ 16B\\n35\\nMistral 7B\\n30.5\\nLLaMA 2\\n29.9\\nphi-1-base\\n29\\nCodex-12B\\n28.81\\nPaLM 540B\\n26.2\\nCodeT5+ 2B\\n24.2\\nLLaMA 65B\\n23.7\\nLLaMA 33B\\n21.7\\nPaLM 62B\\n15.9\\nLLaMA 13B\\n15.8\\nLaMDA 137B\\n14\\nMIM-350M\\n13.7\\nLLaMA 7B\\n10.5\\nPaLM 8B\\n3.6\\nArithmetic reasoning is another challenging reasoning ca-\\npability to achieve. GSM8K for example contains grade school\\nmathematical questions with respect to their answers. Table IX\\nprovides an insight for different model comparisons.\\nTABLE IX: Arithmetic reasoning comparison.\\nModel\\nGSM8k\\nMATH\\nGemini Ultra\\n94.4\\n53.2\\nGPT-4\\n87.1\\n42.5\\nGemini Pro\\n86.5\\n32.6\\nToRA 70B\\n84.3\\n49.7\\nMathCoder-L-70B\\n83.9\\n-\\nMetaMath 70B\\n82.3\\n26\\nMuggleMATH 70B\\n82.3\\n-\\nMathCoder-CL-34B\\n81.7\\n45.2\\nToRA-Code 34B\\n80.7\\n50.8\\nMetaMath-Mistral-7B\\n77.7\\n-\\nArithmo2-Mistral-7B\\n76.4\\n-\\nToRA-Code 13B\\n75.8\\n48.1\\nArithmo-Mistral-7B\\n74.7\\n-\\nMathCoder-CL-13B\\n74.1\\n35.9\\nMuggleMATH 13B\\n74\\n-\\nCodeT5+\\n73.8\\n-\\nKwaiYiiMath 13B\\n73.3\\n-\\nToRA-Code 7B\\n72.6\\n44.6\\nMathCoder-L-13B\\n72.6\\n29.9\\nMetaMath 13B\\n71\\n22.5\\nLLaMA 65B\\n69.7\\n10.6\\nMuggleMATH 7B\\n68.4\\n-\\nMathCoder-CL-7B\\n67.8\\n23.3\\nMetaMath 7B\\n66.4\\n19.4\\nRFT 70B\\n64.8\\n-\\nMathCoder-L-7B\\n64.2\\n-\\nOrca 2-13B\\n59.14\\n-\\nU-PaLM\\n58.5\\n-\\nPaLM-540B\\n58.1\\n8.8\\nLLaMA 2 70B\\n56.8\\n-\\nRFT 13B\\n55.3\\n-\\nLLaMA 33B\\n53.1\\n7.1\\nMistral 7B\\n52.2\\n13.1\\nRFT 7B\\n51.2\\n-\\nLLaMA 65B\\n50.9\\n20.5\\nOrca 2-7B\\n47.23\\n-\\nText-davinci-002\\n40.7\\n19.1\\nLLaMA 33B\\n35.6\\n3.9\\nGPT-Neo-2.7B\\n19.5\\n-\\nLLaMA 7B\\n18.1\\n2.9\\nPaLM 540B\\n17.9\\n8.8\\nLLaMA 13B\\n17.8\\n3.9\\nLLaMA 7B\\n11\\n2.9\\nGPT-Neo-125M\\n7.5\\n-\\nPaLM 8B\\n4.1\\n1.5\\nGPT-2\\n-\\n5.4\\nGPT-3 175B\\n-\\n5.2\\nPaLM 62B\\n-\\n4.4\\nGPT-3-13B\\n-\\n3\\nLLaMA 7B\\n11\\n2.9\\nPaLM 8B\\n-\\n1.5\\nLarge language models in some cases are hallucinating an-\\nswers simply because they are next-token prediction machines.\\nHallucination is one of the important factors in measuring\\nhow much a large language model is trustworthy and reliable.\\nMeasuring hallucination on the other hand is also not easy as it\\nseems because each fact can be written in different styles and\\neven the smallest changes in writing make it hard to detect.\\nIt is fair to assume if any particular LLM is more capable\\nto detect hallucination of false information in text, it is also\\nmore trustworthy. HaluEval is one of the datasets that aims to\\nmeasure hallucination in this field [205]. Evaluation can also be\\nperformed by another model judging the response with regard\\nto the actual answer [206]. Table X shows the evaluation of\\ndifferent models based on these datasets.\\nVII.\\nCHALLENGES AND FUTURE DIRECTIONS\\nAs we have seen in the previous sections, large language\\nmodels have achieved impressive results in the past 1-2 years.\\nTABLE X: Hallucination evaluation\\nModel\\nHHEM\\nHaluEval QA\\nHaluEval Dialogue\\nHaluEval Sum.\\nHaluEval General\\nGPT 4\\n97\\n-\\n-\\n-\\n-\\nGPT 4 Turbo\\n97\\n-\\n-\\n-\\n-\\nGPT 3.5 Turbo\\n96.5\\n62.59\\n72.4\\n58.53\\n79.44\\nDavinci002\\n-\\n60.05\\n60.81\\n47.77\\n80.42\\nDavinci003\\n-\\n49.65\\n68.37\\n48.07\\n80.4\\nGPT-3\\n-\\n49.21\\n50.02\\n51.23\\n72.72\\nGoogle Gemini Pro\\n95.2\\n-\\n-\\n-\\n-\\nLlama 2 70B\\n94.9\\n-\\n-\\n-\\n-\\nLlama 2 7B\\n94.4\\n49.6\\n43.99\\n49.55\\n20.46\\nLlama 2 13B\\n94.1\\n-\\n-\\n-\\n-\\nCohere-Chat\\n92.5\\n-\\n-\\n-\\n-\\nCohere\\n91.5\\n-\\n-\\n-\\n-\\nClaude 2\\n91.5\\n69.78\\n64.73\\n57.75\\n75\\nClaude 1\\n67.6\\n64.83\\n53.76\\n73.88\\nMicrosoft Phi 2\\n91.5\\n-\\n-\\n-\\n-\\nGoogle Palm 2 (beta)\\n91.4\\n-\\n-\\n-\\n-\\nMixtral 8x7B\\n90.7\\n-\\n-\\n-\\n-\\nAmazon Titan Express\\n90.6\\n-\\n-\\n-\\n-\\nMistral 7B\\n90.6\\n-\\n-\\n-\\n-\\nGoogle Palm 2 Chat (beta)\\n90\\n-\\n-\\n-\\n-\\nGoogle Palm 2\\n87.9\\n-\\n-\\n-\\n-\\nGoogle Palm 2 Chat\\n72.8\\n-\\n-\\n-\\n-\\nChatGLM\\n-\\n47.93\\n44.41\\n48.57\\n30.92\\nFalcon\\n-\\n39.66\\n29.08\\n42.71\\n18.98\\nVicuna\\n-\\n60.34\\n46.35\\n45.62\\n19.48\\nAlpaca\\n-\\n6.68\\n17.55\\n20.63\\n9.54\\nAt the same time this is still a new and extremely active\\nresearch area where the pace of innovation is increasing rather\\nthan slowing down. As in any other evolving area though, there\\nare still numerous challenges ahead. Here we briefly mention\\nsome of the challenges and main active areas which are known\\nso far. It is worth noting that LLM challenges are discussed\\nin details in a work by Kaddour et al. [207].\\nA. Smaller and more efficient Language Models\\nThis is a survey on large language models, and there\\nhas been an initial push towards ”larger is better” that has\\nclearly been rewarded with ever larger models like GPT-\\n4 getting better accuracy and performance in benchmarks.\\nHowever, those large models are costly and inefficient in\\nseveral dimensions (e.g. high latency). In response to all of\\nthis, there is a current research trend to come up with Small\\nLanguage Models (SLMs) as a cost-effective alternative to\\nLLMs, particularly when used on specific tasks that might not\\nrequire the full generality of larger models. Prominent works\\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\\nfrom Microsoft.\\nMore generally, we should expect many research efforts in\\nthis area of how to train smaller and more efficient models.\\nTechniques such as parameter-efficient fine-tuning (PEFT),\\nteacher/student, and other forms of distillation – see section\\nIII-I – will continue to be used to build a smaller model out\\nof larger ones.\\nB. New Post-attention Architectural Paradigms\\nTransformer blocks have been a crucial and constant part of\\nmost of current LLM frameworks, and it’s a big question mark\\nhow much longer this architecture will be in vogue, and what\\nwill be the next big architectural break-through in the field of\\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\\nmany architectures go in and out of fashion, including LSTM,\\nGRU, seq2seq, but Transformers have been the dominant\\napproach since its inception. As described earlier, attention is\\nthe main mechanism driving transformers. More recently, there\\nhas been promising research in alternative approaches that are\\nbeing labelled as post-attention.\\nAn important class of such class of post-attention models\\nare the so called State Space Models (SSMs). While the notion\\nof State Space Models has a long history in machine learning,\\nit should be noted that in the context of language models, SSM\\nis usually used in reference to the newer Structure State Space\\nModel architecture or S4 for short (see Gu et al. [29]). Some\\nrecent models in this category are Mamba [30], Hyena [210],\\nand Striped Hyena [211].\\nWhile all of those models are very competitive in terms of\\nperformance in leaderboards and efficiency, they also address\\nan important challenge in more traditional attention-based\\narchitectures: the lack of support for larger context windows.\\nHaving a good answer to many prompts requires context.\\nFor example, the response to ”Recommend some good movies\\nfor me” requires a lot of context about ”me” as well as what\\nmovies are available and which ones I have not watched.\\nContext length is especially important for RAG, where large\\nportions of text might be retrieved and injected into the prompt\\nfor generation (see section IV-C.\\nThe longer the context length, the more tokens we can\\nsqueeze into the context. The more information the model has\\naccess to, the better its response will be. But on the other\\nhand, with very long context, it would be hard for the model\\nto remember everything and efficiently process all the informa-\\ntion. Attention-based models are highly inefficient for longer\\ncontexts and that is why we should expect more research in\\ndifferent mechanisms that enable processing longer contexts\\nand generally come up with more efficient architectures.\\nThat being said, new architectures might not only propose\\nalternatives for the attention mechanism but rather rethink the\\nwhole Transformer architecture. As an early example of this,\\nMonarch Mixer [212] proposes a new architecture that uses\\nthe same sub-quadratic primitive that achieves high hardware\\nefficiency on GPUs – Monarch matrices – along both sequence\\nlength and model dimension.\\nOn the other end of the spectrum, it is worth mentioning\\nthat there are some attention-compatible architectural mecha-\\nnisms that have been recently gaining steam and proving their\\nvalue in creating better and more powerful LLMs. Probably\\nthe best example of such mechanism is Mixture of Experts\\n(MoE). MoEs have been around in machine learning for years,\\neven before the Deep Learning Era [213], but they have been\\ngaining popularity since then, and particularly in the context\\nof Transformer models and LLMs.\\nIn LLMs, MoEs allow to train an extremely large model\\nthan is then only partially instantiated during inference\\nwhen some of the experts are turned off wherever the gat-\\ning/weighting function has a low weight assigned to them. As\\nan example, the GLaM model has 1.2 trillion parameters, but\\nduring inference only 2 out of the 64 experts are used [84].\\nMoEs are nowadays an important component of the so-\\ncalled frontier LLMs (i.e. the most advanced and capable\\nmodels). GPT-4 itself is rumored to be based on a MoE\\narchitecture, and some of the best performing LLMs such as\\nMixtral [117], are basically an MoE version of pre-existing\\nLLMs.\\nFinally, it is important to note that MoEs can be used as a\\ncomponent of any architecture regardless of whether it is based\\non attention or not. In fact, MoEs have also been applied to\\nSSM-based LLMs like Mamba citepioro2024moemamba. We\\nshould continue to see MoE-driven improvements in the future\\nregardless of the underlying architecture.\\nC. Multi-modal Models\\nFuture LLMs are expected to be multi-modal and handle\\na variety of data types, such as text, images, and videos,\\naudio, in a unified manner. This opens up possibilities for\\nmore diverse applications in fields like question answering,\\ncontent generation, creative arts, and healthcare, robotics, and\\nbeyond. There are already several prominent multi-modal\\nLLMs out there, including: LLAVA [214], LLAVA-Plus [215],\\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\\nexpected to be continued. Evaluation of these models also is a\\nnew research topic, especially conversational generative vision\\nmodels [217]. Multi-modal LLMs can unlock huge potentials\\nin a variety of tasks, and there has already been a descent\\nprogress in this direction, which needs a dedicated paper to\\ndiscuss all its details.\\nD. Improved LLM Usage and Augmentation techniques\\nAs we described in sectionIV, many of the shortcomings\\nand limitations of LLMs such as hallucination can be ad-\\ndressed through advanced prompt engineering, use of tools,\\nor other augmentation techniques. We should expect not only\\ncontinued, but accelerated research in this area. It is worth\\nmentioning that, in the specific case of software engineering,\\nsome works ([218]) tried to automatically eliminate this issue\\nfrom the overall software engineering workflow\\nLLM-based systems are already starting to replace ma-\\nchine learning systems that were until recently using other\\napproaches. As a clear example of this, LLMs are now being\\ndeployed to better understand people preference and interests,\\nand provide more personalized interactions, whether in cus-\\ntomer service, content recommendation, or other applications.\\nThis involves better understanding of user preferences, and\\nanalyzing their past interactions and using them as the context.\\nWe will continue to see research in the application and usage\\nof LLMs for not only personalization and recommendations,\\nbut many other application areas using other machine learning\\ntechniques.\\nFinally, another important area of research we expect to\\ngather increased attention is that of LLM-based agents and\\nmulti-agent systems [172], [173], [174]. The development of\\nLLM systems with access to external tools and decision-\\nmaking capabilities is both exciting and challenging. We will\\nsee continued research and progress in this important area that\\nsome argue could lead to Artificial General Intelligence (AGI).\\nE. Security and Ethical/Responsible AI\\nEnsuring the robustness and security of LLMs against\\nadversarial attacks and other vulnerabilities is a critical area\\nof research [219]. As LLMs are increasingly deployed in real-\\nworld applications, they need to be protected from potential\\nthreats, to prevent them being used to manipulate people or\\nspread mis-information.\\nAddressing ethical concerns and biases in LLMs is another\\nactive area of research. Efforts are being made to ensure that\\nLLMs are fair, unbiased, and capable of handling sensitive\\ninformation responsibly. As LLMs are being used more and\\nmore by a large number of people on a daily basis, making\\nsure they are unbiased and behave responsibly is crucial.\\nVIII.\\nCONCLUSION\\nThis paper present a survey of LLMs developed in the\\npast few years. We first provide an overview of early pre-\\ntrained language models (e.g., as BERT), then review three\\npopular LLM families (GPT, LLaMA, PaLM), and other\\nrepresentative LLMs. We then survey methods and techniques\\nof building, augmenting, and using LLMs. We review popular\\nLLM datasets and benchmarks, and compare performance of\\na set of prominent models on public benchmarks. Finally, we\\npresent open challenges and future research directions.\\nREFERENCES\\n[1]\\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\\n[2]\\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\\net al., “Training compute-optimal large language models,” arXiv\\npreprint arXiv:2203.15556, 2022.\\n[3]\\nC. E. Shannon, “Prediction and entropy of printed english,” Bell system\\ntechnical journal, vol. 30, no. 1, pp. 50–64, 1951.\\n[4]\\nF. Jelinek, Statistical methods for speech recognition.\\nMIT press,\\n1998.\\n[5]\\nC. Manning and H. Schutze, Foundations of statistical natural lan-\\nguage processing.\\nMIT press, 1999.\\n[6]\\nC. D. Manning, An introduction to information retrieval.\\nCambridge\\nuniversity press, 2009.\\n[7]\\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\\nB. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\\nmodels,” arXiv preprint arXiv:2303.18223, 2023.\\n[8]\\nC. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He et al., “A comprehensive survey on pretrained foundation mod-\\nels: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419,\\n2023.\\n[9]\\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\\ntrain, prompt, and predict: A systematic survey of prompting methods\\nin natural language processing,” ACM Computing Surveys, vol. 55,\\nno. 9, pp. 1–35, 2023.\\n[10]\\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint\\narXiv:2301.00234, 2022.\\n[11]\\nJ. Huang and K. C.-C. Chang, “Towards reasoning in large language\\nmodels: A survey,” arXiv preprint arXiv:2212.10403, 2022.\\n[12]\\nS. F. Chen and J. Goodman, “An empirical study of smoothing\\ntechniques for language modeling,” Computer Speech & Language,\\nvol. 13, no. 4, pp. 359–394, 1999.\\n[13]\\nY. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\\nlanguage model,” Advances in neural information processing systems,\\nvol. 13, 2000.\\n[14]\\nH. Schwenk, D. D´echelotte, and J.-L. Gauvain, “Continuous space\\nlanguage models for statistical machine translation,” in Proceedings\\nof the COLING/ACL 2006 Main Conference Poster Sessions, 2006,\\npp. 723–730.\\n[15]\\nT. Mikolov, M. Karafi´at, L. Burget, J. Cernock`y, and S. Khudanpur,\\n“Recurrent neural network based language model.” in Interspeech,\\nvol. 2, no. 3.\\nMakuhari, 2010, pp. 1045–1048.\\n[16]\\nA. Graves, “Generating sequences with recurrent neural networks,”\\narXiv preprint arXiv:1308.0850, 2013.\\n[17]\\nP.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning\\ndeep structured semantic models for web search using clickthrough\\ndata,” in Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management, 2013, pp. 2333–2338.\\n[18]\\nJ. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\\nConversational Information Retrieval. Springer Nature, 2023, vol. 44.\\n[19]\\nI. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\\nwith neural networks,” Advances in neural information processing\\nsystems, vol. 27, 2014.\\n[20]\\nK. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On\\nthe properties of neural machine translation: Encoder-decoder ap-\\nproaches,” arXiv preprint arXiv:1409.1259, 2014.\\n[21]\\nH. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll´ar,\\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to\\nvisual concepts and back,” in Proceedings of the IEEE conference\\non computer vision and pattern recognition, 2015, pp. 1473–1482.\\n[22]\\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:\\nA neural image caption generator,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2015, pp.\\n3156–3164.\\n[23]\\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations. corr\\nabs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.\\n[24]\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805, 2018.\\n[25]\\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\\n[26]\\nP. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\\nwith disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.\\n[27]\\nX. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao,\\nA. Zhang, L. Zhang et al., “Pre-trained models: Past, present and\\nfuture,” AI Open, vol. 2, pp. 225–250, 2021.\\n[28]\\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\\nmodels for natural language processing: A survey,” Science China\\nTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\\n[29]\\nA. Gu, K. Goel, and C. R´e, “Efficiently modeling long sequences with\\nstructured state spaces,” 2022.\\n[30]\\nA. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\\n[31]\\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,\\n“Palm: Scaling language modeling with pathways,” arXiv preprint\\narXiv:2204.02311, 2022.\\n[32]\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\\nOpen and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971, 2023.\\n[33]\\nOpenAI,\\n“GPT-4\\nTechnical\\nReport,”\\nhttps://arxiv.org/pdf/2303.\\n08774v3.pdf, 2023.\\n[34]\\nJ.\\nWei,\\nX.\\nWang,\\nD.\\nSchuurmans,\\nM.\\nBosma,\\nb.\\nichter,\\nF.\\nXia,\\nE.\\nChi,\\nQ.\\nV.\\nLe,\\nand\\nD.\\nZhou,\\n“Chain-of-thought\\nprompting\\nelicits\\nreasoning\\nin\\nlarge\\nlanguage\\nmodels,”\\nin\\nAdvances in Neural Information Processing Systems, S. Koyejo,\\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35.\\nCurran Associates, Inc., 2022, pp. 24 824–24 837.\\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\n[35]\\nG. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al., “Augmented language models: a survey,” arXiv preprint\\narXiv:2302.07842, 2023.\\n[36]\\nB. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang,\\nL. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try\\nagain: Improving large language models with external knowledge and\\nautomated feedback,” arXiv preprint arXiv:2302.12813, 2023.\\n[37]\\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\\n“React: Synergizing reasoning and acting in language models,” arXiv\\npreprint arXiv:2210.03629, 2022.\\n[38]\\nD. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal\\nrepresentations by error propagation,” 1985.\\n[39]\\nJ. L. Elman, “Finding structure in time,” Cognitive science, vol. 14,\\nno. 2, pp. 179–211, 1990.\\n[40]\\nM. V. Mahoney, “Fast text compression with neural networks.” in\\nFLAIRS conference, 2000, pp. 230–234.\\n[41]\\nT. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y, “Strate-\\ngies for training large scale neural network language models,” in 2011\\nIEEE Workshop on Automatic Speech Recognition & Understanding.\\nIEEE, 2011, pp. 196–201.\\n[42]\\ntmikolov.\\nrnnlm.\\n[Online].\\nAvailable:\\nhttps://www.fit.vutbr.cz/\\n∼imikolov/rnnlm/\\n[43]\\nS. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, “Deep learning–based text classification: a comprehensive\\nreview,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40,\\n2021.\\n[44]\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\\nAdvances in neural information processing systems, vol. 30, 2017.\\n[45]\\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language represen-\\ntations,” arXiv preprint arXiv:1909.11942, 2019.\\n[46]\\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-\\ntraining text encoders as discriminators rather than generators,” arXiv\\npreprint arXiv:2003.10555, 2020.\\n[47]\\nG. Lample and A. Conneau, “Cross-lingual language model pretrain-\\ning,” arXiv preprint arXiv:1901.07291, 2019.\\n[48]\\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\\nQ. V. Le, “Xlnet: Generalized autoregressive pretraining for language\\nunderstanding,” Advances in neural information processing systems,\\nvol. 32, 2019.\\n[49]\\nL. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\\nnatural language understanding and generation,” Advances in neural\\ninformation processing systems, vol. 32, 2019.\\n[50]\\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv-\\ning language understanding by generative pre-training,” 2018.\\n[51]\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[52]\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\\nwith a unified text-to-text transformer,” The Journal of Machine\\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\\n[53]\\nL. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\\n[54]\\nK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked\\nsequence to sequence pre-training for language generation,” arXiv\\npreprint arXiv:1905.02450, 2019.\\n[55]\\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,” arXiv preprint arXiv:1910.13461, 2019.\\n[56]\\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems, vol. 33, pp. 1877–1901, 2020.\\n[57]\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\\nplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al.,\\n“Evaluating large language models trained on code,” arXiv preprint\\narXiv:2107.03374, 2021.\\n[58]\\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[59]\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nNeural Information Processing Systems, vol. 35, pp. 27 730–27 744,\\n2022.\\n[60]\\nOpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\\n//openai.com/blog/chatgpt\\n[61]\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\\n2: Open foundation and fine-tuned chat models,” arXiv preprint\\narXiv:2307.09288, 2023.\\n[62]\\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, “Alpaca: A strong, replicable instruction-\\nfollowing model,” Stanford Center for Research on Foundation Mod-\\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6,\\np. 7, 2023.\\n[63]\\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-\\nficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\\n2023.\\n[64]\\nX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nand D. Song, “Koala: A dialogue model for academic research,” Blog\\npost, April, vol. 1, 2023.\\n[65]\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\\n“Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\\n[66]\\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,\\nJ. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models\\nfor code,” arXiv preprint arXiv:2308.12950, 2023.\\n[67]\\nS. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large\\nlanguage model connected with massive apis,” 2023.\\n[68]\\nA. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\\nS. Naidu, “Giraffe: Adventures in expanding context lengths in llms,”\\narXiv preprint arXiv:2308.10882, 2023.\\n[69]\\nB. Huang, “Vigogne: French instruction-following and chat models,”\\nhttps://github.com/bofenghuang/vigogne, 2023.\\n[70]\\nY. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can\\ncamels go? exploring the state of instruction tuning on open resources,”\\narXiv preprint arXiv:2306.04751, 2023.\\n[71]\\nS. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski,\\nand P. Miło´s, “Focused transformer: Contrastive training for context\\nscaling,” arXiv preprint arXiv:2307.03170, 2023.\\n[72]\\nD.\\nMahan,\\nR.\\nCarlow,\\nL.\\nCastricato,\\nN.\\nCooper,\\nand\\nC.\\nLaforte,\\n“Stable\\nbeluga\\nmodels.”\\n[Online].\\nAvailable:\\n[https://huggingface.co/stabilityai/StableBeluga2](https://\\nhuggingface.co/stabilityai/StableBeluga2)\\n[73]\\nY. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar-\\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling\\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399,\\n2022.\\n[74]\\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,\\nY. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-\\nfinetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\\n[75]\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical\\nreport,” arXiv preprint arXiv:2305.10403, 2023.\\n[76]\\nK. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\\nmodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,\\n2022.\\n[77]\\nK. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-\\nlevel medical question answering with large language models,” arXiv\\npreprint arXiv:2305.09617, 2023.\\n[78]\\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\\nA. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot\\nlearners,” arXiv preprint arXiv:2109.01652, 2021.\\n[79]\\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\\nmodels: Methods, analysis & insights from training gopher,” arXiv\\npreprint arXiv:2112.11446, 2021.\\n[80]\\nV. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi-\\ntask prompted training enables zero-shot task generalization,” arXiv\\npreprint arXiv:2110.08207, 2021.\\n[81]\\nY. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-\\ntraining for language understanding and generation,” arXiv preprint\\narXiv:2107.02137, 2021.\\n[82]\\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\\net al., “Improving language models by retrieving from trillions of\\ntokens,” in International conference on machine learning.\\nPMLR,\\n2022, pp. 2206–2240.\\n[83]\\nO. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technical\\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\\n[84]\\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\\nY. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of\\nlanguage models with mixture-of-experts,” in International Conference\\non Machine Learning.\\nPMLR, 2022, pp. 5547–5569.\\n[85]\\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\\nT. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language\\nmodels for dialog applications,” arXiv preprint arXiv:2201.08239,\\n2022.\\n[86]\\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\\nC. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained\\ntransformer language models,” arXiv preprint arXiv:2205.01068, 2022.\\n[87]\\nR. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\\navia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large\\nlanguage model for science,” arXiv preprint arXiv:2211.09085, 2022.\\n[88]\\nE. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\\nS. Savarese, and C. Xiong, “Codegen: An open large language\\nmodel for code with multi-turn program synthesis,” arXiv preprint\\narXiv:2203.13474, 2022.\\n[89]\\nS. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al.,\\n“Alexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model,” arXiv preprint arXiv:2208.01448, 2022.\\n[90]\\nA. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu,\\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al.,\\n“Improving alignment of dialogue agents via targeted human judge-\\nments,” arXiv preprint arXiv:2209.14375, 2022.\\n[91]\\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\\nV. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,\\n“Solving quantitative reasoning problems with language models,”\\nAdvances in Neural Information Processing Systems, vol. 35, pp.\\n3843–3857, 2022.\\n[92]\\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster,\\nH. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning\\nparadigms,” arXiv preprint arXiv:2205.05131, 2022.\\n[93]\\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-\\nparameter open-access multilingual language model,” arXiv preprint\\narXiv:2211.05100, 2022.\\n[94]\\nA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\\nW. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained\\nmodel,” arXiv preprint arXiv:2210.02414, 2022.\\n[95]\\nS. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,\\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\\n“Pythia: A suite for analyzing large language models across train-\\ning and scaling,” in International Conference on Machine Learning.\\nPMLR, 2023, pp. 2397–2430.\\n[96]\\nS. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\\nA. Awadallah, “Orca: Progressive learning from complex explanation\\ntraces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.\\n[97]\\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source\\nbe with you!” arXiv preprint arXiv:2305.06161, 2023.\\n[98]\\nS. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv,\\nL. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you\\nneed: Aligning perception with language models,” arXiv preprint\\narXiv:2302.14045, 2023.\\n[99]\\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly\\ncapable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\\n[100]\\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue:\\nEmbodied reasoning through planning with language models,” arXiv\\npreprint arXiv:2207.05608, 2022.\\n[101]\\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti\\net al., “Using deepspeed and megatron to train megatron-turing\\nnlg 530b, a large-scale generative language model,” arXiv preprint\\narXiv:2201.11990, 2022.\\n[102]\\nI. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\\ndocument transformer,” arXiv preprint arXiv:2004.05150, 2020.\\n[103]\\nS. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language\\nmodel instruction meta learning through the lens of generalization,”\\narXiv preprint arXiv:2212.12017, 2022.\\n[104]\\nY. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\\nand F. Wei, “Language models are general-purpose interfaces,” arXiv\\npreprint arXiv:2206.06336, 2022.\\n[105]\\nZ. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,\\nand C. Gan, “Principle-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,” arXiv preprint\\narXiv:2305.03047, 2023.\\n[106]\\nW. E. team, “Palmyra-base Parameter Autoregressive Language\\nModel,” https://dev.writer.com, 2023.\\n[107]\\n——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\\n[108]\\nYandex.\\nYalm.\\n[Online].\\nAvailable:\\nhttps://github.com/yandex/\\nYaLM-100B\\n[109]\\nM. Team et al., “Introducing mpt-7b: a new standard for open-source,\\ncommercially usable llms,” 2023.\\n[110]\\nA. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2:\\nTeaching small language models how to reason,” 2023.\\n[111]\\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and\\nG. Neubig, “Pal: Program-aided language models,” in International\\nConference on Machine Learning.\\nPMLR, 2023, pp. 10 764–10 799.\\n[112]\\nAnthropic. claude. [Online]. Available: https://www.anthropic.com/\\nnews/introducing-claude\\n[113]\\nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\\n“Codegen2: Lessons for training llms on programming and natural\\nlanguages,” arXiv preprint arXiv:2305.02309, 2023.\\n[114]\\nL. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada,\\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct\\ndistillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.\\n[115]\\nX. team. Grok. [Online]. Available: https://grok.x.ai/\\n[116]\\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\\nand J. Zhou, “Qwen-vl: A frontier large vision-language model with\\nversatile abilities,” arXiv preprint arXiv:2308.12966, 2023.\\n[117]\\nmixtral.\\nmixtral.\\n[Online].\\nAvailable:\\nhttps://mistral.ai/news/\\nmixtral-of-experts/\\n[118]\\nD. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei,\\nA. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative\\nlanguage model for multimodal document understanding,” 2023.\\n[119]\\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\\nY. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:\\nWhen the large language model meets programming – the rise of code\\nintelligence,” 2024.\\n[120]\\nF. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge\\nfusion of large language models,” 2024.\\n[121]\\nP. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source\\nsmall language model,” 2024.\\n[122]\\nC. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan,\\n“Llama pro: Progressive llama with block expansion,” 2024.\\n[123]\\nX. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\\nM. Kazi, “Transformer models: an introduction and catalog,” 2023.\\n[124]\\nG. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-\\nweb dataset for falcon llm: outperforming curated corpora with web\\ndata, and web data only,” arXiv preprint arXiv:2306.01116, 2023.\\n[125]\\nD. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al.,\\n“Scaling laws and interpretability of learning from repeated data,”\\narXiv preprint arXiv:2205.10487, 2022.\\n[126]\\nP. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\\nposition representations,” arXiv preprint arXiv:1803.02155, 2018.\\n[127]\\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En-\\nhanced transformer with rotary position embedding,” arXiv preprint\\narXiv:2104.09864, 2021.\\n[128]\\nO. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention\\nwith linear biases enables input length extrapolation,” arXiv preprint\\narXiv:2108.12409, 2021.\\n[129]\\nG. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in\\nlanguage pre-training,” arXiv preprint arXiv:2006.15595, 2020.\\n[130]\\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\\n[131]\\nW. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\\nto trillion parameter models with simple and efficient sparsity,” The\\nJournal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\\n2022.\\n[132]\\nR. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n“Parameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,” 2021.\\n[133]\\nS. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\\nmodels: A survey,” 2023.\\n[134]\\nS. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task\\ngeneralization via natural language crowdsourcing instructions,” arXiv\\npreprint arXiv:2104.08773, 2021.\\n[135]\\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\\ngenerated instructions,” arXiv preprint arXiv:2212.10560, 2022.\\n[136]\\nK. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\\nreport.pdf\\n[137]\\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\\nD. Amodei, “Deep reinforcement learning from human preferences,”\\nAdvances in neural information processing systems, vol. 30, 2017.\\n[138]\\nH. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car-\\nbune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from\\nhuman feedback with ai feedback,” arXiv preprint arXiv:2309.00267,\\n2023.\\n[139]\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\\nC. Finn, “Direct preference optimization: Your language model is\\nsecretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.\\n[140]\\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory\\noptimizations toward training trillion parameter models,” in SC20: In-\\nternational Conference for High Performance Computing, Networking,\\nStorage and Analysis.\\nIEEE, 2020, pp. 1–16.\\n[141]\\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing\\nrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\\n[142]\\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\\narXiv preprint arXiv:2106.09685, 2021.\\n[143]\\nG. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\\n[144]\\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation:\\nA survey,” International Journal of Computer Vision, vol. 129, pp.\\n1789–1819, 2021.\\n[145]\\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J.\\nBang, A. Madotto, and P. Fung, “Survey of hallucination in natural\\nlanguage generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023.\\n[Online]. Available: https://doi.org/10.1145/3571730\\n[146]\\nN. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\\nM. Steedman, “Sources of hallucination by large language models on\\ninference tasks,” 2023.\\n[147]\\nC.-Y.\\nLin,\\n“ROUGE:\\nA\\npackage\\nfor\\nautomatic\\nevaluation\\nof\\nsummaries,” in Text Summarization Branches Out.\\nBarcelona, Spain:\\nAssociation for Computational Linguistics, Jul. 2004, pp. 74–81.\\n[Online]. Available: https://aclanthology.org/W04-1013\\n[148]\\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\\nautomatic evaluation of machine translation,” in Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics,\\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311–\\n318. [Online]. Available: https://aclanthology.org/P02-1040\\n[149]\\nB. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\\nW. Cohen, “Handling divergent reference texts when evaluating\\ntable-to-text generation,” in Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics, A. Korhonen,\\nD. Traum, and L. M`arquez, Eds.\\nFlorence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online].\\nAvailable: https://aclanthology.org/P19-1483\\n[150]\\nZ. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful\\nneural table-to-text generation with content-matching constraints,”\\nin Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter,\\nand J. Tetreault, Eds.\\nOnline: Association for Computational\\nLinguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https:\\n//aclanthology.org/2020.acl-main.101\\n[151]\\nH. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis-\\ntent dialogues by exploiting natural language inference,” Proceedings\\nof the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp.\\n8878–8885, Apr. 2020.\\n[152]\\nO. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,\\nand O. Abend, “q2: Evaluating factual consistency in knowledge-\\ngrounded dialogues via question generation and question answering,”\\nin Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing, M.-F. Moens, X. Huang, L. Specia,\\nand S. W.-t. Yih, Eds.\\nOnline and Punta Cana, Dominican Republic:\\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856–7870.\\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\\n[153]\\nN. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution\\nin dialogue systems: The BEGIN benchmark,” Transactions of the\\nAssociation for Computational Linguistics, vol. 10, pp. 1066–1083,\\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\\n[154]\\nS. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\\nY. Liu, and D. Z. Hakkani-T¨ur, “Rome was built in 1776: A case study\\non factual correctness in knowledge-grounded response generation,”\\nArXiv, vol. abs/2110.05456, 2021.\\n[155]\\nS. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\\nL. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic\\nevaluation of factual precision in long form text generation,” 2023.\\n[156]\\nD. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\\nV. Chaudhary, and M. Young, “Machine learning: The high interest\\ncredit card of technical debt,” in SE4ML: Software Engineering for\\nMachine Learning (NIPS 2014 Workshop), 2014.\\n[157]\\nZ. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought\\nprompting in large language models,” 2022.\\n[158]\\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\\nlarge language models,” 2023.\\n[159]\\nP. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero-\\nresource black-box hallucination detection for generative large lan-\\nguage models,” 2023.\\n[160]\\nN. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\\nand S. Yao, “Reflexion: Language agents with verbal reinforcement\\nlearning,” 2023.\\n[161]\\nS. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\\nK. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi,\\nA. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and\\neecs curriculum using large language models,” 2023.\\n[162]\\nT. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\\nCai, “Promptchainer: Chaining large language model prompts through\\nvisual programming,” 2022.\\n[163]\\nY. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\\nJ. Ba, “Large language models are human-level prompt engineers,”\\n2023.\\n[164]\\nP. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\\nN. Goyal, H. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and\\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\\nNLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2005.11401\\n[165]\\nY. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\\nH. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997, 2023.\\n[166]\\nA. W. Services. (Year of publication, e.g., 2023) Question answering\\nusing retrieval augmented generation with foundation models in\\namazon\\nsagemaker\\njumpstart.\\nAccessed:\\nDate\\nof\\naccess,\\ne.g.,\\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\\n[167]\\nS. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large\\nlanguage models and knowledge graphs: A roadmap,” arXiv preprint\\narXiv:2306.08302, 2023.\\n[168]\\nZ. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\n2023.\\n[169]\\nT. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” 2023.\\n[170]\\nB. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\\nand M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use\\nfor large language models,” 2023.\\n[171]\\nY. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:\\nSolving ai tasks with chatgpt and its friends in huggingface,” arXiv\\npreprint arXiv:2303.17580, 2023.\\n[172]\\nZ. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou et al., “The rise and potential of large language model\\nbased agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.\\n[173]\\nL. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y. Lin et al., “A survey on large language model\\nbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\\n[174]\\nZ. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\\nR. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-\\nFei, and J. Gao, “Agent ai: Surveying the horizons of multimodal\\ninteraction,” arXiv preprint arXiv:2401.03568, 2024.\\n[175]\\nB. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo:\\nDecoupling reasoning from observations for efficient augmented lan-\\nguage models,” 2023.\\n[176]\\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\\n“React: Synergizing reasoning and acting in language models,” 2023.\\n[177]\\nV. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc-\\ning large language model completions with dialog-enabled resolving\\nagents,” 2023.\\n[178]\\nY. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang,\\nand X. Xie, “A survey on evaluation of large language models,” 2023.\\n[179]\\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\\nQ.\\nLe,\\nand\\nS.\\nPetrov,\\n“Natural\\nquestions:\\nA\\nbenchmark\\nfor\\nquestion answering research,” Transactions of the Association for\\nComputational Linguistics, vol. 7, pp. 452–466, 2019. [Online].\\nAvailable: https://aclanthology.org/Q19-1026\\n[180]\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\n2021.\\n[181]\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\\nlanguage models,” arXiv preprint arXiv:2108.07732, 2021.\\n[182]\\nE. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\\nand L. Zettlemoyer, “QuAC: Question answering in context,” in\\nProceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, E. Riloff, D. Chiang, J. Hockenmaier, and\\nJ. Tsujii, Eds.\\nBrussels, Belgium: Association for Computational\\nLinguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available:\\nhttps://aclanthology.org/D18-1241\\n[183]\\nD. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring\\ncoding challenge competence with apps,” NeurIPS, 2021.\\n[184]\\nV. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured\\nqueries from natural language using reinforcement learning,” arXiv\\npreprint arXiv:1709.00103, 2017.\\n[185]\\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for reading\\ncomprehension,” in Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\nR. Barzilay and M.-Y. Kan, Eds.\\nVancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].\\nAvailable: https://aclanthology.org/P17-1147\\n[186]\\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale\\nReAding comprehension dataset from examinations,” in Proceedings\\nof the 2017 Conference on Empirical Methods in Natural Language\\nProcessing, M. Palmer, R. Hwa, and S. Riedel, Eds.\\nCopenhagen,\\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\\n785–794. [Online]. Available: https://aclanthology.org/D17-1082\\n[187]\\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+\\nquestions for machine comprehension of text,” in Proceedings of\\nthe 2016 Conference on Empirical Methods in Natural Language\\nProcessing, J. Su, K. Duh, and X. Carreras, Eds.\\nAustin, Texas:\\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383–2392.\\n[Online]. Available: https://aclanthology.org/D16-1264\\n[188]\\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural\\nyes/no\\nquestions,”\\nCoRR,\\nvol.\\nabs/1905.10044,\\n2019.\\n[Online].\\nAvailable: http://arxiv.org/abs/1905.10044\\n[189]\\nD. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\\n“Looking beyond the surface:a challenge set for reading compre-\\nhension over multiple sentences,” in Proceedings of North American\\nChapter of the Association for Computational Linguistics (NAACL),\\n2018.\\n[190]\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\\nJ. Schulman, “Training verifiers to solve math word problems,”\\nCoRR,\\nvol.\\nabs/2110.14168,\\n2021.\\n[Online].\\nAvailable:\\nhttps:\\n//arxiv.org/abs/2110.14168\\n[191]\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\\nwith the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online].\\nAvailable: https://arxiv.org/abs/2103.03874\\n[192]\\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:\\nCan a machine really finish your sentence?” 2019.\\n[193]\\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try\\narc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018.\\n[Online]. Available: http://arxiv.org/abs/1803.05457\\n[194]\\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA:\\nreasoning about physical commonsense in natural language,” CoRR,\\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\\n1911.11641\\n[195]\\nM. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa:\\nCommonsense reasoning about social interactions,” CoRR, vol.\\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\\n09728\\n[196]\\nT. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of\\narmor conduct electricity? A new dataset for open book question\\nanswering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1809.02789\\n[197]\\nS. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\\n[198]\\nZ. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov,\\nand C. D. Manning, “Hotpotqa: A dataset for diverse, explainable\\nmulti-hop question answering,” CoRR, vol. abs/1809.09600, 2018.\\n[Online]. Available: http://arxiv.org/abs/1809.09600\\n[199]\\nY. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A\\ndataset for llm question answering with external tools,” arXiv preprint\\narXiv:2306.13304, 2023.\\n[200]\\nD. Chen, J. Bolton, and C. D. Manning, “A thorough examination\\nof the cnn/daily mail reading comprehension task,” in Association for\\nComputational Linguistics (ACL), 2016.\\n[201]\\nR. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text\\nsummarization using sequence-to-sequence rnns and beyond,” arXiv\\npreprint arXiv:1602.06023, 2016.\\n[202]\\nY. Bai and D. Z. Wang, “More than reading comprehension: A survey\\non datasets and metrics of textual question answering,” arXiv preprint\\narXiv:2109.12264, 2021.\\n[203]\\nH.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in\\nhistory for conversational machine comprehension,” arXiv preprint\\narXiv:1810.06683, 2018.\\n[204]\\nS. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A\\nsurvey on evaluation metrics for machine translation,” Mathematics,\\nvol. 11, no. 4, p. 1006, 2023.\\n[205]\\nJ. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:\\nA large-scale hallucination evaluation benchmark for large language\\nmodels,” in Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing, 2023, pp. 6449–6464.\\n[206]\\nSimon\\nMark\\nHughes,\\n“Hughes\\nhallucination\\nevaluation\\nmodel\\n(hhem)\\nleaderboard,”\\n2024,\\nhttps://huggingface.co/spaces/vectara/\\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\\n[207]\\nJ. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\\nR. McHardy, “Challenges and applications of large language models,”\\narXiv preprint arXiv:2307.10169, 2023.\\n[208]\\nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al.,\\n“Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\\n[209]\\nY. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T.\\nLee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv\\npreprint arXiv:2309.05463, 2023.\\n[210]\\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\\nY. Bengio, S. Ermon, and C. R´e, “Hyena hierarchy: Towards larger\\nconvolutional language models,” 2023.\\n[211]\\nM. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\\nA. Thomas, “StripedHyena: Moving Beyond Transformers with\\nHybrid Signal Processing Models,” 12 2023. [Online]. Available:\\nhttps://github.com/togethercomputer/stripedhyena\\n[212]\\nD. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\\nB. Spector, M. Poli, A. Rudra, and C. R´e, “Monarch mixer: A simple\\nsub-quadratic gemm-based architecture,” 2023.\\n[213]\\nG. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture\\nmodels,” Annual review of statistics and its application, vol. 6, pp.\\n355–378, 2019.\\n[214]\\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv\\npreprint arXiv:2304.08485, 2023.\\n[215]\\nS. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus:\\nLearning to use tools for creating multimodal agents,” arXiv preprint\\narXiv:2311.05437, 2023.\\n[216]\\nS. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any\\nmultimodal llm,” arXiv preprint arXiv:2309.05519, 2023.\\n[217]\\nN. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\\nD. Z¨uhlke, “Convgenvismo: Evaluation of conversational generative\\nvision models,” 2023.\\n[218]\\nN. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unit\\ntest improvement using large language models at meta,” arXiv preprint\\narXiv:2402.09171, 2024.\\n[219]\\nL. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang,\\nW. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large\\nlanguage models,” arXiv preprint arXiv:2401.05561, 2024.\\n[220]\\nMicrosoft.\\nDeepspeed.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nmicrosoft/DeepSpeed\\n[221]\\nHuggingFace. Transformers. [Online]. Available: https://github.com/\\nhuggingface/transformers\\n[222]\\nNvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\\nMegatron-LM\\n[223]\\nBMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\\nBMTrain\\n[224]\\nEleutherAI.\\ngpt-neox.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nEleutherAI/gpt-neox\\n[225]\\nmicrosoft. Lora. [Online]. Available: https://github.com/microsoft/\\nLoRA\\n[226]\\nColossalAI.\\nColossalai.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nhpcaitech/ColossalAI\\n[227]\\nFastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\\nFastChat\\n[228]\\nskypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\\nskypilot\\n[229]\\nvllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\\n[230]\\nhuggingface. text-generation-inference. [Online]. Available: https:\\n//github.com/huggingface/text-generation-inference\\n[231]\\nlangchain.\\nlangchain.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nlangchain-ai/langchain\\n[232]\\nbentoml. Openllm. [Online]. Available: https://github.com/bentoml/\\nOpenLLM\\n[233]\\nembedchain. embedchain. [Online]. Available: https://github.com/\\nembedchain/embedchain\\n[234]\\nmicrosoft. autogen. [Online]. Available: https://github.com/microsoft/\\nautogen\\n[235]\\nbabyagi.\\nbabyagi.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nyoheinakajima/babyagi\\n[236]\\nguidance.\\nguidance.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nguidance-ai/guidance\\n[237]\\nprompttools. prompttools. [Online]. Available: https://github.com/\\nhegelai/prompttools\\n[238]\\npromptfoo.\\npromptfoo.\\n[Online].\\nAvailable:\\nhttps://github.com/\\npromptfoo/promptfoo\\n[239]\\nfacebook.\\nfaiss.\\n[Online].\\nAvailable:\\nhttps://github.com/\\nfacebookresearch/faiss\\n[240]\\nmilvus. milvus. [Online]. Available: https://github.com/milvus-io/\\nmilvus\\n[241]\\nqdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\\n[242]\\nweaviate. weaviate. [Online]. Available: https://github.com/weaviate/\\nweaviate\\n[243]\\nllama index. llama-index. [Online]. Available: https://github.com/\\nrun-llama/llama index\\nAPPENDIX\\n1. Open Source Toolkits For LLM Development and\\nDeployment\\nThere are various frameworks and libraries developed for\\nLLM training, evaluation, and deployment, and covering every\\nsingle framework is out of this paper’s scope. But we try to\\nprovide a brief introduction of some of the most popular ones,\\ngrouped into different categories.\\nA. LLM Training/Inference Frameworks\\nSome of the popular frameworks which are useful for LLM\\ntraining includes (note that some of them can be used beyond\\nLLM training too):\\nDeepSpeed [220] is a deep learning optimization library\\nthat makes distributed training and inference easy, efficient,\\nand effective. DeepSpeed enables world’s most powerful lan-\\nguage models like MT-530B and BLOOM. It is an easy-\\nto-use deep learning optimization software suite that powers\\nunprecedented scale and speed for both training and inference.\\nWith DeepSpeed you can:\\nTransformers [221] is library by HuggingFace which\\nprovides thousands of pretrained models to perform tasks on\\ndifferent modalities such as text, vision, and audio. Using\\npretrained models one can reduce compute costs, carbon\\nfootprint, and save the time and resources required to train\\na model from scratch.\\nMegatron-LM [222] is a large, powerful transformer\\ndeveloped by the Applied Deep Learning Research team\\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\\nquence, and pipeline), and multi-node pre-training of trans-\\nformer based models such as GPT, BERT, and T5 using mixed\\nprecision.\\nBMTrain [223] is an efficient large model training toolkit\\nthat can be used to train large models with tens of billions of\\nparameters. It can train models in a distributed manner while\\nkeeping the code as simple as stand-alone training.\\nGPT-NeoX [224] leverages many of the same features and\\ntechnologies as the popular Megatron-DeepSpeed library but\\nwith substantially increased usability and novel optimizations.\\nLoRA [225] library provides the support for Low-Rank\\nAdaptation of Large Language Models. It reduces the number\\nof trainable parameters by learning pairs of rank-decompostion\\nmatrices while freezing the original weights. This vastly\\nreduces the storage requirement for large language models\\nadapted to specific tasks and enables efficient task-switching\\nduring deployment all without introducing inference latency.\\nLoRA also outperforms several other adaptation methods in-\\ncluding adapter, prefix-tuning, and fine-tuning.\\nColossalAI library [226] provides a collection of parallel\\ncomponents. It aims to support developers to write their\\ndistributed deep learning models just like how they write their\\nmodel on their laptop. They provide user-friendly tools to\\nkickstart distributed training and inference in a few lines. In\\nterms of Parallelism strategies, they support: Data Parallelism,\\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\\nOptimizer (ZeRO) [140], and Auto-Parallelism.\\nB. Deployment Tools\\nWe provide an overview of some of the most popular LLM\\ndeployment tools here.\\nFastChat [227] is an open platform for training, serv-\\ning, and evaluating large language model based chatbots.\\nFastChat’s core features include: The training and evaluation\\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\\na distributed multi-model serving system with web UI and\\nOpenAI-compatible RESTful APIs.\\nSkypilot [228] is a framework for running LLMs, AI,\\nand batch jobs on any cloud, offering maximum cost savings,\\nhighest GPU availability, and managed execution.\\nvLLM [229] is a fast and easy-to-use library for LLM in-\\nference and serving. vLLM seamlessly supports many Hugging\\nFace models, including the following architectures: Aquila,\\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\\nYi, and many more.\\ntext-generation-inference [230] is a toolkit for deploying\\nand serving Large Language Models (LLMs). TGI enables\\nhigh-performance text generation for the most popular open-\\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\\nGPT-NeoX, and more.\\nLangChain [231] is a framework for developing applica-\\ntions powered by language models. It enables applications that:\\n•\\nAre context-aware: connect a language model to\\nsources of context (prompt instructions, few shot ex-\\namples, content to ground its response in, etc.)\\n•\\nReason: rely on a language model to reason (about\\nhow to answer based on provided context, what ac-\\ntions to take, etc.)\\nOpenLLM [232] is an open-source platform designed to\\nfacilitate the deployment and operation of large language mod-\\nels (LLMs) in real-world applications. With OpenLLM, you\\ncan run inference on any open-source LLM, deploy them on\\nthe cloud or on-premises, and build powerful AI applications.\\nEmbedchain [233] is an Open Source RAG Framework\\nthat makes it easy to create and deploy AI apps. Embedchain\\nstreamlines the creation of RAG applications, offering a seam-\\nless process for managing various types of unstructured data.\\nIt efficiently segments data into manageable chunks, generates\\nrelevant embeddings, and stores them in a vector database for\\noptimized retrieval.\\nAutogen [234] is a framework that enables the devel-\\nopment of LLM applications using multiple agents that can\\nconverse with each other to solve tasks. AutoGen agents\\nare customizable, conversable, and seamlessly allow human\\nparticipation. They can operate in various modes that employ\\ncombinations of LLMs, human inputs, and tools.\\nBabyAGI [235] is an autonomous Artificial Intelligence\\nagent, that is designed to generate and execute tasks based on\\ngiven objectives. It harnesses cutting-edge technologies from\\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\\nand achieve specific goals. In this blog post, we will dive\\ninto the unique features of BabyAGI and explore how it can\\nstreamline task automation.\\nC. Prompting Libraries\\nGuidance [236] is a programming paradigm that offers\\nsuperior control and efficiency compared to conventional\\nprompting and chaining. It allows users to constrain generation\\n(e.g. with regex and CFGs) as well as to interleave control\\n(conditional, loops) and generation seamlessly.\\nPromptTools [237] offers a set of open-source, self-\\nhostable tools for experimenting with, testing, and evaluating\\nLLMs, vector databases, and prompts. The core idea is to\\nenable developers to evaluate using familiar interfaces like\\ncode, notebooks, and a local playground.\\nPromptBench [?] is a Pytorch-based Python package for\\nEvaluation of Large Language Models (LLMs). It provides\\nuser-friendly APIs for researchers to conduct evaluation on\\nLLMs.\\nPromptfoo [238] is a tool for testing and evaluating LLM\\noutput quality. It systematically test prompts, models, and\\nRAGs with predefined test cases.\\nD. VectorDB\\nFaiss [239] is a library developed by Facebook AI Re-\\nsearch that provides efficient similarity search and clustering\\nof dense vectors. It is designed for use with large-scale,\\nhigh-dimensional data and supports several index types and\\nalgorithms for various use cases.\\nMilvus [240] is an open-source vector database built to\\npower embedding similarity search and AI applications. Mil-\\nvus makes unstructured data search more accessible, and pro-\\nvides a consistent user experience regardless of the deployment\\nenvironment.\\nQdrant [241] is a vector similarity search engine and\\nvector database. It provides a production-ready service with a\\nconvenient API to store, search, and manage points—vectors\\nwith an additional payload Qdrant is tailored to extended\\nfiltering support. environment.\\nWeaviate [242] is an open-source, GraphQL-based vec-\\ntor search engine that enables similarity search on high-\\ndimensional data. While it is open-source, the commercial ver-\\nsion offers additional features, support, and managed services.\\nSome of the other popular options includes LlamaIndex\\n[243] and Pinecone.\\n', '1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various approaches in their respective contexts,\\nand speculate on upcoming trends and innovations.\\nOur contributions are as follows:\\n• In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, delineating\\nits evolution through paradigms including naive RAG,\\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024\\n2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current assessment methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, emphasizing\\npotential enhancements to tackle current challenges.\\nThe paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-\\ntraining data, it initially lacks the capacity to provide up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the\\n3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\\nwidespread adoption of ChatGPT. The Naive RAG follows\\na traditional process that includes indexing, retrieval, and\\ngeneration, which is also characterized as a “Retrieve-Read”\\nframework [7].\\nIndexing starts with the cleaning and extraction of raw data\\nin diverse formats like PDF, HTML, Word, and Markdown,\\nwhich is then converted into a uniform plain text format. To\\naccommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model\\nand stored in vector database. This step is crucial for enabling\\nefficient similarity searches in the subsequent retrieval phase.\\nRetrieval. Upon receipt of a user query, the RAG system\\nemploys the same encoding model utilized during the indexing\\nphase to transform the query into a vector representation.\\nIt then computes the similarity scores between the query\\nvector and the vector of chunks within the indexed corpus.\\nThe system prioritizes and retrieves the top K chunks that\\ndemonstrate the greatest similarity to the query. These chunks\\nare subsequently used as the expanded context in prompt.\\nGeneration. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges. The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrelevance, toxicity, or bias in the outputs,\\ndetracting from the quality and reliability of the responses.\\nAugmentation Hurdles. Integrating retrieved information\\nwith the different task can be challenging, sometimes resulting\\nin disjointed or incoherent outputs. The process may also\\nencounter redundancy when similar information is retrieved\\nfrom multiple sources, leading to repetitive responses. Deter-\\nmining the significance and relevance of various passages and\\nensuring stylistic and tonal consistency add further complexity.\\nFacing complex issues, a single retrieval based on the original\\nquery may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].\\n4\\nFig. 3.\\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common\\nmethods include query rewriting query transformation, query\\nexpansion and other techniques [7], [9]–[11].\\nPost-Retrieval Process. Once relevant context is retrieved,\\nit’s crucial to integrate it effectively with the query. The main\\nmethods in post-retrieval process include rerank chunks and\\ncontext compressing. Re-ranking the retrieved information to\\nrelocate the most relevant content to the edges of the prompt is\\na key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre-ranking to uncover both explicit and transformative knowl-\\nedge [16]. The Memory module leverages the LLM’s memory\\nto guide retrieval, creating an unbounded memory pool that\\n5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes with higher latency and ethical considerations regarding\\ndata retrieval. On the other hand, FT is more static, requiring\\nretraining for updates but enabling deep customization of the\\nmodel’s behavior and style. It demands significant compu-\\ntational resources for dataset preparation and training, and\\nwhile it can reduce hallucinations, it may face challenges with\\nunfamiliar data.\\nIn multiple evaluations of their performance on various\\nknowledge-intensive tasks across different topics, [28] re-\\nvealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new\\nknowledge. Additionally, it was found that LLMs struggle\\nto learn new factual information through unsupervised fine-\\ntuning. The choice between RAG and FT depends on the\\nspecific needs for data dynamics, customization, and com-\\nputational capabilities in the application context. RAG and\\nFT are not mutually exclusive and can complement each\\nother, enhancing a model’s capabilities at different levels.\\nIn some instances, their combined use may lead to optimal\\nperformance. The optimization process involving RAG and FT\\nmay require multiple iterations to achieve satisfactory results.\\nIII. RETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval\\ngranularity, pre-processing of the retrieval, and selection of\\nthe corresponding embedding model.\\nA. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in\\nrecent researches towards utilizing content generated by LLMs\\nthemselves for retrieval and enhancement purposes.\\n6\\nTABLE I\\nSUMMARY OF RAG METHODS\\nMethod\\nRetrieval Source\\nRetrieval\\nData Type\\nRetrieval\\nGranularity\\nAugmentation\\nStage\\nRetrieval\\nprocess\\nCoG [29]\\nWikipedia\\nText\\nPhrase\\nPre-training\\nIterative\\nDenseX [30]\\nFactoidWiki\\nText\\nProposition\\nInference\\nOnce\\nEAR [31]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nUPRISE [20]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nRAST [32]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nSelf-Mem [17]\\nDataset-base\\nText\\nSentence\\nTuning\\nIterative\\nFLARE [24]\\nSearch Engine,Wikipedia\\nText\\nSentence\\nTuning\\nAdaptive\\nPGRA [33]\\nWikipedia\\nText\\nSentence\\nInference\\nOnce\\nFILCO [34]\\nWikipedia\\nText\\nSentence\\nInference\\nOnce\\nRADA [35]\\nDataset-base\\nText\\nSentence\\nInference\\nOnce\\nFilter-rerank [36]\\nSynthesized dataset\\nText\\nSentence\\nInference\\nOnce\\nR-GQA [37]\\nDataset-base\\nText\\nSentence Pair\\nTuning\\nOnce\\nLLM-R [38]\\nDataset-base\\nText\\nSentence Pair\\nInference\\nIterative\\nTIGER [39]\\nDataset-base\\nText\\nItem-base\\nPre-training\\nOnce\\nLM-Indexer [40]\\nDataset-base\\nText\\nItem-base\\nTuning\\nOnce\\nBEQUE [9]\\nDataset-base\\nText\\nItem-base\\nTuning\\nOnce\\nCT-RAG [41]\\nSynthesized dataset\\nText\\nItem-base\\nTuning\\nOnce\\nAtlas [42]\\nWikipedia, Common Crawl\\nText\\nChunk\\nPre-training\\nIterative\\nRAVEN [43]\\nWikipedia\\nText\\nChunk\\nPre-training\\nOnce\\nRETRO++ [44]\\nPre-training Corpus\\nText\\nChunk\\nPre-training\\nIterative\\nINSTRUCTRETRO [45]\\nPre-training corpus\\nText\\nChunk\\nPre-training\\nIterative\\nRRR [7]\\nSearch Engine\\nText\\nChunk\\nTuning\\nOnce\\nRA-e2e [46]\\nDataset-base\\nText\\nChunk\\nTuning\\nOnce\\nPROMPTAGATOR [21]\\nBEIR\\nText\\nChunk\\nTuning\\nOnce\\nAAR [47]\\nMSMARCO,Wikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRA-DIT [27]\\nCommon Crawl,Wikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRAG-Robust [48]\\nWikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRA-Long-Form [49]\\nDataset-base\\nText\\nChunk\\nTuning\\nOnce\\nCoN [50]\\nWikipedia\\nText\\nChunk\\nTuning\\nOnce\\nSelf-RAG [25]\\nWikipedia\\nText\\nChunk\\nTuning\\nAdaptive\\nBGM [26]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nCoQ [51]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nToken-Elimination [52]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nPaperQA [53]\\nArxiv,Online Database,PubMed\\nText\\nChunk\\nInference\\nIterative\\nNoiseRAG [54]\\nFactoidWiki\\nText\\nChunk\\nInference\\nOnce\\nIAG [55]\\nSearch Engine,Wikipedia\\nText\\nChunk\\nInference\\nOnce\\nNoMIRACL [56]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nToC [57]\\nSearch Engine,Wikipedia\\nText\\nChunk\\nInference\\nRecursive\\nSKR [58]\\nDataset-base,Wikipedia\\nText\\nChunk\\nInference\\nAdaptive\\nITRG [59]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nRAG-LongContext [60]\\nDataset-base\\nText\\nChunk\\nInference\\nOnce\\nITER-RETGEN [14]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nIRCoT [61]\\nWikipedia\\nText\\nChunk\\nInference\\nRecursive\\nLLM-Knowledge-Boundary [62]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nRAPTOR [63]\\nDataset-base\\nText\\nChunk\\nInference\\nRecursive\\nRECITE [22]\\nLLMs\\nText\\nChunk\\nInference\\nOnce\\nICRALM [64]\\nPile,Wikipedia\\nText\\nChunk\\nInference\\nIterative\\nRetrieve-and-Sample [65]\\nDataset-base\\nText\\nDoc\\nTuning\\nOnce\\nZemi [66]\\nC4\\nText\\nDoc\\nTuning\\nOnce\\nCRAG [67]\\nArxiv\\nText\\nDoc\\nInference\\nOnce\\n1-PAGER [68]\\nWikipedia\\nText\\nDoc\\nInference\\nIterative\\nPRCA [69]\\nDataset-base\\nText\\nDoc\\nInference\\nOnce\\nQLM-Doc-ranking [70]\\nDataset-base\\nText\\nDoc\\nInference\\nOnce\\nRecomp [71]\\nWikipedia\\nText\\nDoc\\nInference\\nOnce\\nDSP [23]\\nWikipedia\\nText\\nDoc\\nInference\\nIterative\\nRePLUG [72]\\nPile\\nText\\nDoc\\nInference\\nOnce\\nARM-RAG [73]\\nDataset-base\\nText\\nDoc\\nInference\\nIterative\\nGenRead [13]\\nLLMs\\nText\\nDoc\\nInference\\nIterative\\nUniMS-RAG [74]\\nDataset-base\\nText\\nMulti\\nTuning\\nOnce\\nCREA-ICL [19]\\nDataset-base\\nCrosslingual,Text\\nSentence\\nInference\\nOnce\\nPKG [75]\\nLLM\\nTabular,Text\\nChunk\\nInference\\nOnce\\nSANTA [76]\\nDataset-base\\nCode,Text\\nItem\\nPre-training\\nOnce\\nSURGE [77]\\nFreebase\\nKG\\nSub-Graph\\nTuning\\nOnce\\nMK-ToD [78]\\nDataset-base\\nKG\\nEntity\\nTuning\\nOnce\\nDual-Feedback-ToD [79]\\nDataset-base\\nKG\\nEntity Sequence\\nTuning\\nOnce\\nKnowledGPT [15]\\nDataset-base\\nKG\\nTriplet\\nInference\\nMuti-time\\nFABULA [80]\\nDataset-base,Graph\\nKG\\nEntity\\nInference\\nOnce\\nHyKGE [81]\\nCMeKG\\nKG\\nEntity\\nInference\\nOnce\\nKALMV [82]\\nWikipedia\\nKG\\nTriplet\\nInference\\nIterative\\nRoG [83]\\nFreebase\\nKG\\nTriplet\\nInference\\nIterative\\nG-Retriever [84]\\nDataset-base\\nTextGraph\\nSub-Graph\\nInference\\nOnce\\n7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data, such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains [29]).\\nSemi-structured data. typically refers to data that contains a\\ncombination of text and table information, such as PDF. Han-\\ndling semi-structured data poses challenges for conventional\\nRAG systems due to two main reasons. Firstly, text splitting\\nprocesses may inadvertently separate tables, leading to data\\ncorruption during retrieval. Secondly, incorporating tables into\\nthe data can complicate semantic similarity searches. When\\ndealing with semi-structured data, one approach involves lever-\\naging the code capabilities of LLMs to execute Text-2-SQL\\nqueries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both\\nof these methods are not optimal solutions, indicating substan-\\ntial research opportunities in this area.\\nStructured data, such as knowledge graphs (KGs) [86] ,\\nwhich are typically verified and can provide more precise in-\\nformation. KnowledGPT [15] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness. In response to the limitations of\\nLLMs in understanding and answering questions about textual\\ngraphs, G-Retriever [84] integrates Graph Neural Networks\\n4https://hotpotqa.github.io/wiki-readme.html\\n5https://github.com/facebookresearch/DPR\\n(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree\\n(PCST) optimization problem for targeted graph retrieval. On\\nthe contrary, it requires additional effort to build, validate,\\nand maintain structured databases. On the contrary, it requires\\nadditional effort to build, validate, and maintain structured\\ndatabases.\\nLLMs-Generated Content. Addressing the limitations of\\nexternal auxiliary information in RAG, some research has\\nfocused on exploiting LLMs’ internal knowledge. SKR [58]\\nclassifies questions as known or unknown, applying retrieval\\nenhancement selectively. GenRead [13] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts\\noften contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool\\nwith a retrieval-enhanced generator, using a memory selec-\\ntor to choose outputs that serve as dual problems to the\\noriginal question, thus self-enhancing the generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model\\nperformance and task effectiveness.\\n2) Retrieval Granularity: Another important factor besides\\nthe data format of the retrieval source is the granularity of\\nthe retrieved data. Coarse-grained retrieval units theoretically\\ncan provide more relevant information for the problem, but\\nthey may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity\\nincreases the burden of retrieval and does not guarantee seman-\\ntic integrity and meeting the required knowledge. Choosing\\n8\\nthe appropriate retrieval granularity during inference can be\\na simple and effective strategy to improve the retrieval and\\ndownstream task performance of dense retrievers.\\nIn text, retrieval granularity ranges from fine to coarse,\\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\\nument. Among them, DenseX [30]proposed the concept of\\nusing propositions as retrieval units. Propositions are defined\\nas atomic expressions in the text, each encapsulating a unique\\nfactual segment and presented in a concise, self-contained nat-\\nural language format. This approach aims to enhance retrieval\\nprecision and relevance. On the Knowledge Graph (KG),\\nretrieval granularity includes Entity, Triplet, and sub-Graph.\\nThe granularity of retrieval can also be adapted to downstream\\ntasks, such as retrieving Item IDs [40]in recommendation tasks\\nand Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-\\nmented, and transformed into Embeddings to be stored in a\\nvector database. The quality of index construction determines\\nwhether the correct context can be obtained in the retrieval\\nphase.\\n1) Chunking Strategy: The most common method is to split\\nthe document into chunks on a fixed number of tokens (e.g.,\\n100, 256, 512) [88]. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise. How-\\never, chunks leads to truncation within sentences, prompting\\nthe optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-\\ntheless, these approaches still cannot strike a balance between\\nsemantic completeness and context length. Therefore, methods\\nlike Small2Big have been proposed, where sentences (small)\\nare used as the retrieval unit, and the preceding and following\\nsentences are provided as (big) context to LLMs [90].\\n2) Metadata Attachments: Chunks can be enriched with\\nmetadata information such as page number, file name, au-\\nthor,category timestamp. Subsequently, retrieval can be filtered\\nbased on this metadata, limiting the scope of the retrieval.\\nAssigning different weights to document timestamps during\\nretrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-\\numents, metadata can also be artificially constructed. For\\nexample, adding summaries of paragraph, as well as intro-\\nducing hypothetical questions. This method is also known as\\nReverse HyDE. Specifically, using LLM to generate questions\\nthat can be answered by the document, then calculating the\\nsimilarity between the original question and the hypothetical\\nquestion during retrieval to reduce the semantic gap between\\nthe question and the answer.\\n3) Structural Index: One effective method for enhancing\\ninformation retrieval is to establish a hierarchical structure for\\nthe documents. By constructing In structure, RAG system can\\nexpedite the retrieval and processing of pertinent data.\\nHierarchical index structure. File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-\\nmaries are stored at each node, aiding in the swift traversal\\nof data and assisting the RAG system in determining which\\nchunks to extract. This approach can also mitigate the illusion\\ncaused by block extraction issues.\\nKnowledge Graph index. Utilize KG in constructing the\\nhierarchical structure of documents contributes to maintaining\\nconsistency. It delineates the connections between different\\nconcepts and entities, markedly reducing the potential for\\nillusions. Another advantage is the transformation of the\\ninformation retrieval process into instructions that LLM can\\ncomprehend, thereby enhancing the accuracy of knowledge\\nretrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building\\nan index between multiple documents using KG. This KG\\nconsists of nodes (representing paragraphs or structures in the\\ndocuments, such as pages and tables) and edges (indicating\\nsemantic/lexical similarity between paragraphs or relationships\\nwithin the document structure), effectively addressing knowl-\\nedge retrieval and reasoning problems in a multi-document\\nenvironment.\\nC. Query Optimization\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language\\nis not well-organized. Another difficulty lies in language\\ncomplexity ambiguity. Language models often struggle when\\ndealing with specialized vocabulary or ambiguous abbrevi-\\nations with multiple meanings. For instance, they may not\\ndiscern whether “LLM” refers to large language model or a\\nMaster of Laws in a legal context.\\n1) Query Expansion: Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nMulti-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.\\nSub-Query. The process of sub-question planning represents\\nthe generation of the necessary sub-questions to contextualize\\nand fully answer the original question when combined. This\\nprocess of adding relevant context is, in principle, similar\\nto query expansion. Specifically, a complex question can be\\ndecomposed into a series of simpler sub-questions using the\\nleast-to-most prompting method [92].\\nChain-of-Verification(CoVe). The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing halluci-\\nnations. Validated expanded queries typically exhibit higher\\nreliability [93].\\n9\\n2) Query Transformation: The core concept is to retrieve\\nchunks based on a transformed query instead of the user’s\\noriginal query.\\nQuery Rewrite.The original queries are not always optimal\\nfor LLM retrieval, especially in real-world scenarios. There-\\nfore, we can prompt LLM to rewrite the queries. In addition to\\nusing LLM for query rewriting, specialized smaller language\\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\\nmentation of the query rewrite method in the Taobao, known\\nas BEQUE [9] has notably enhanced recall effectiveness for\\nlong-tail queries, resulting in a rise in GMV.\\nAnother query transformation method is to use prompt\\nengineering to let LLM generate a query based on the original\\nquery for subsequent retrieval. HyDE [11] construct hypothet-\\nical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.\\nUsing the Step-back Prompting method [10], the original\\nquery is abstracted to generate a high-level concept question\\n(step-back question). In the RAG system, both the step-back\\nquestion and the original query are used for retrieval, and both\\nthe results are utilized as the basis for language model answer\\ngeneration.\\n3) Query Routing: Based on varying queries, routing to\\ndistinct RAG pipeline,which is suitable for a versatile RAG\\nsystem designed to accommodate diverse scenarios.\\nMetadata Router/ Filter. The first step involves extracting\\nkeywords (entity) from the query, followed by filtering based\\non the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific\\napprach see Semantic Router 6. Certainly, a hybrid routing\\napproach can also be employed, combining both semantic and\\nmetadata-based methods for enhanced query routing.\\nD. Embedding\\nIn RAG, retrieval is achieved by calculating the similarity\\n(e.g. cosine similarity) between the embeddings of the ques-\\ntion and document chunks, where the semantic representation\\ncapability of embedding models plays a key role. This mainly\\nincludes a sparse encoder (BM25) and a dense retriever (BERT\\narchitecture Pre-training language models). Recent research\\nhas introduced prominent embedding models such as AngIE,\\nVoyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-\\nditionally, C-MTEB focuses on Chinese capability, covering\\n6 tasks and 35 datasets. There is no one-size-fits-all answer\\nto “which embedding model to use.” However, some specific\\nmodels are better suited for particular use cases.\\n1) Mix/hybrid Retrieval : Sparse and dense embedding\\napproaches capture different relevance features and can ben-\\nefit from each other by leveraging complementary relevance\\ninformation. For instance, sparse retrieval models can be used\\n6https://github.com/aurelio-labs/semantic-router\\n7https://huggingface.co/spaces/mteb/leaderboard\\nto provide initial search results for training dense retrieval\\nmodels. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval\\nmodels can enhance the zero-shot retrieval capability of dense\\nretrieval models and assist dense retrievers in handling queries\\ncontaining rare entities, thereby improving robustness.\\n2) Fine-tuning Embedding Model: In instances where the\\ncontext significantly deviates from pre-training corpus, partic-\\nularly within highly specialized disciplines such as healthcare,\\nlegal practice, and other sectors replete with proprietary jargon,\\nfine-tuning the embedding model on your own domain dataset\\nbecomes essential to mitigate such discrepancies.\\nIn addition to supplementing domain knowledge, another\\npurpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).\\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\\ngenerator to create task-specific retrievers, addressing chal-\\nlenges in supervised fine-tuning, particularly in data-scarce\\ndomains. Another approach, LLM-Embedder [97], exploits\\nLLMs to generate reward signals across multiple downstream\\ntasks. The retriever is fine-tuned with two types of supervised\\nsignals: hard labels for the dataset and soft rewards from\\nthe LLMs. This dual-signal approach fosters a more effective\\nfine-tuning process, tailoring the embedding model to diverse\\ndownstream applications. REPLUG [72] utilizes a retriever\\nand an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and\\neffective training method enhances the performance of the\\nretrieval model by using an LM as the supervisory signal,\\neliminating the need for specific cross-attention mechanisms.\\nMoreover, inspired by RLHF (Reinforcement Learning from\\nHuman Feedback), utilizing LM-based feedback to reinforce\\nthe retriever through reinforcement learning.\\nE. Adapter\\nFine-tuning models may present challenges, such as in-\\ntegrating functionality through an API or addressing con-\\nstraints arising from limited local computational resources.\\nConsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool\\nthat are suitable for a given zero-shot task input. AAR\\n(Augmentation-Adapted Retriver) [47] introduces a universal\\nadapter designed to accommodate multiple downstream tasks.\\nWhile PRCA [69] add a pluggable reward-driven contextual\\nadapter to enhance performance on specific tasks. BGM [26]\\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\\nmodel in between. The bridge model aims to transform the\\nretrieved information into a format that LLMs can work with\\neffectively, allowing it to not only rerank but also dynami-\\ncally select passages for each query, and potentially employ\\nmore advanced strategies like repetition. Furthermore, PKG\\n10\\nintroduces an innovative method for integrating knowledge\\ninto white-box models via directive fine-tuning [75]. In this\\napproach, the retriever module is directly substituted to gen-\\nerate relevant documents according to a query. This method\\nassists in addressing the difficulties encountered during the\\nfine-tuning process and enhances model performance.\\nIV. GENERATION\\nAfter retrieval, it is not a good practice to directly input all\\nthe retrieved information to the LLM for answering questions.\\nFollowing will introduce adjustments from two perspectives:\\nadjusting the retrieved content and adjusting the LLM.\\nA. Context Curation\\nRedundant information can interfere with the final gener-\\nation of LLM, and overly long contexts can also lead LLM\\nto the “Lost in the middle” problem [98]. Like humans, LLM\\ntends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.\\n1) Reranking: Reranking fundamentally reorders document\\nchunks to highlight the most pertinent results first, effectively\\nreducing the overall document pool, severing a dual purpose\\nin information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many\\nrelevant documents as possible and concatenating them to form\\na lengthy retrieval prompt is beneficial. However, excessive\\ncontext can introduce more noise, diminishing the LLM’s\\nperception of key information .\\n(Long) LLMLingua [100], [101] utilize small language\\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\\ndetect and remove unimportant tokens, transforming it into\\na form that is challenging for humans to comprehend but\\nwell understood by LLMs. This approach presents a direct\\nand practical method for prompt compression, eliminating the\\nneed for additional training of LLMs while balancing language\\nintegrity and compression ratio. PRCA tackled this issue by\\ntraining an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data\\npoint consists of one positive sample and five negative sam-\\nples, and the encoder undergoes training using contrastive loss\\nthroughout this process [102] .\\nIn addition to compressing the context, reducing the num-\\nber of documents aslo helps improve the accuracy of the\\nmodel’s answers. Ma et al. [103] propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and SLMs.\\nIn this paradigm, SLMs serve as filters, while LLMs function\\nas reordering agents. The research shows that instructing\\nLLMs to rearrange challenging samples identified by SLMs\\nleads to significant improvements in various Information\\nExtraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the\\nLLM to filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [104], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nB. LLM Fine-tuning\\nTargeted fine-tuning based on the scenario and data char-\\nacteristics on LLMs can yield better results. This is also one\\nof the greatest advantages of using on-premise LLMs. When\\nLLMs lack data in a specific domain, additional knowledge can\\nbe provided to the LLM through fine-tuning. Huggingface’s\\nfine-tuning data can also be used as an initial step.\\nAnother benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-\\nticular style as instructed [37]. For retrieval tasks that engage\\nwith structured data, the SANTA framework [76] implements\\na tripartite training regimen to effectively encapsulate both\\nstructural and semantic nuances. The initial phase focuses on\\nthe retriever, where contrastive learning is harnessed to refine\\nthe query and document embeddings.\\nAligning LLM outputs with human or retriever preferences\\nthrough reinforcement learning is a potential approach. For\\ninstance, manually annotating the final generated answers\\nand then providing feedback through reinforcement learning.\\nIn addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to\\npowerful proprietary models or larger parameter open-source\\nmodels, a simple and effective method is to distill the more\\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\\nbe coordinated with fine-tuning of the retriever to align pref-\\nerences. A typical approach, such as RA-DIT [27], aligns the\\nscoring functions between Retriever and Generator using KL\\ndivergence.\\nV. AUGMENTATION PROCESS IN RAG\\nIn the domain of RAG, the standard practice often involves\\na singular (once) retrieval step followed by generation, which\\ncan lead to inefficiencies and sometimes is typically insuffi-\\ncient for complex problems demanding multi-step reasoning,\\nas it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.\\nA. Iterative Retrieval\\nIterative retrieval is a process where the knowledge base\\nis repeatedly searched based on the initial query and the text\\ngenerated so far, providing a more comprehensive knowledge\\n11\\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\\nbase for LLMs. This approach has been shown to enhance\\nthe robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-\\ntinuity and the accumulation of irrelevant information. ITER-\\nRETGEN [14] employs a synergistic approach that lever-\\nages “retrieval-enhanced generation” alongside “generation-\\nenhanced retrieval” for tasks that necessitate the reproduction\\nof specific information. The model harnesses the content\\nrequired to address the input task as a contextual basis for\\nretrieving pertinent knowledge, which in turn facilitates the\\ngeneration of improved responses in subsequent iterations.\\nB. Recursive Retrieval\\nRecursive retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.\\nThe process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\\nthe retrieval process and refines the CoT with the obtained\\nretrieval results. ToC [57] creates a clarification tree that\\nsystematically optimizes the ambiguous parts in the Query. It\\ncan be particularly useful in complex search scenarios where\\nthe user’s needs are not entirely clear from the outset or where\\nthe information sought is highly specialized or nuanced. The\\nrecursive nature of the process allows for continuous learning\\nand adaptation to the user’s requirements, often resulting in\\nimproved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as seen\\nin model agents like AutoGPT, Toolformer, and Graph-\\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\\nvides its retrieval process into distinct steps where LLMs\\nproactively use retrievers, apply Self-Ask techniques, and em-\\nploy few-shot prompts to initiate search queries. This proactive\\nstance allows LLMs to decide when to search for necessary\\ninformation, akin to how an agent utilizes tools.\\nWebGPT [110] integrates a reinforcement learning frame-\\nwork to train the GPT-3 model in autonomously using a\\nsearch engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby\\nexpanding GPT-3’s capabilities through the use of external\\nsearch engines. Flare automates timing retrieval by monitoring\\nthe confidence of the generation process, as indicated by the\\n12\\nprobability of generated terms [24]. When the probability falls\\nbelow a certain threshold would activates the retrieval system\\nto collect relevant information, thus optimizing the retrieval\\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\\nthe model to introspect its outputs. These tokens come in\\ntwo varieties: “retrieve” and “critic”. The model autonomously\\ndecides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional\\nclassifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the\\nmodel’s autonomous judgment capabilities in generating ac-\\ncurate responses.\\nVI. TASK AND EVALUATION\\nThe rapid advancement and growing adoption of RAG\\nin the field of NLP have propelled the evaluation of RAG\\nmodels to the forefront of research in the LLMs community.\\nThe primary objective of this evaluation is to comprehend\\nand optimize the performance of RAG models across diverse\\napplication scenarios.This chapter will mainly introduce the\\nmain downstream tasks of RAG, datasets, and how to evaluate\\nRAG systems.\\nA. Downstream Task\\nThe core task of RAG remains Question Answering (QA),\\nincluding\\ntraditional\\nsingle-hop/multi-hop\\nQA,\\nmultiple-\\nchoice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-\\nmation Extraction (IE), dialogue generation, code search, etc.\\nThe main downstream tasks of RAG and their corresponding\\ndatasets are summarized in Table II.\\nB. Evaluation Target\\nHistorically, RAG models assessments have centered on\\ntheir execution in specific downstream tasks. These evaluations\\nemploy established metrics suitable to the tasks at hand. For\\ninstance, question answering evaluations might rely on EM\\nand F1 scores [7], [45], [59], [72], whereas fact-checking\\ntasks often hinge on Accuracy as the primary metric [4],\\n[14], [42]. BLEU and ROUGE metrics are also commonly\\nused to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-\\nspecific metrics [160]. Despite this, there is a notable paucity\\nof research dedicated to evaluating the distinct characteristics\\nof RAG models.The main evaluation objectives include:\\nRetrieval Quality. Evaluating the retrieval quality is crucial\\nfor determining the effectiveness of the context sourced by\\nthe retriever component. Standard metrics from the domains\\nof search engines, recommendation systems, and information\\nretrieval systems are employed to measure the performance of\\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [161], [162].\\nGeneration Quality. The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation\\ncan be categorized based on the content’s objectives: unlabeled\\nand labeled content. For unlabeled content, the evaluation\\nencompasses the faithfulness, relevance, and non-harmfulness\\nof the generated answers. In contrast, for labeled content,\\nthe focus is on the accuracy of the information produced by\\nthe model [161]. Additionally, both retrieval and generation\\nquality assessments can be conducted through manual or\\nautomatic evaluation methods [29], [161], [163].\\nC. Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-\\nvance, answer faithfulness, and answer relevance. These qual-\\nity scores evaluate the efficiency of the RAG model from\\ndifferent perspectives in the process of information retrieval\\nand generation [164]–[166].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers\\nremain true to the retrieved context, maintaining consistency\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing\\nthe core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:\\nnoise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency in\\nsynthesizing information from multiple documents to address\\ncomplex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the\\nquality of generation.\\n13\\nTABLE II\\nDOWNSTREAM TASKS AND DATASETS OF RAG\\nTask\\nSub Task\\nDataset\\nMethod\\nQA\\nSingle-hop\\nNatural Qustion(NQ) [111]\\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\\n[20], [44], [72]\\nTriviaQA(TQA) [113]\\n[13], [30], [34], [45], [50], [64]\\n[4], [27], [59], [62], [112]\\n[22], [25], [43], [44], [71], [72]\\nSQuAD [114]\\n[20], [23], [30], [32], [45], [69], [112]\\nWeb Questions(WebQ) [115]\\n[3], [4], [13], [30], [50], [68]\\nPopQA [116]\\n[7], [25], [67]\\nMS MARCO [117]\\n[4], [40], [52]\\nMulti-hop\\nHotpotQA [118]\\n[23], [26], [31], [34], [47], [51], [61], [82]\\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\\n2WikiMultiHopQA [119]\\n[14], [24], [48], [59], [61], [91]\\nMuSiQue [120]\\n[14], [51], [61], [91]\\nLong-form QA\\nELI5 [121]\\n[27], [34], [43], [49], [51]\\nNarrativeQA(NQA) [122]\\n[45], [60], [63], [123]\\nASQA [124]\\n[24], [57]\\nQMSum(QM) [125]\\n[60], [123]\\nDomain QA\\nQasper [126]\\n[60], [63]\\nCOVID-QA [127]\\n[35], [46]\\nCMB [128],MMCU Medical [129]\\n[81]\\nMulti-Choice QA\\nQuALITY [130]\\n[60], [63]\\nARC [131]\\n[25], [67]\\nCommonsenseQA [132]\\n[58], [66]\\nGraph QA\\nGraphQA [84]\\n[84]\\nDialog\\nDialog Generation\\nWizard of Wikipedia (WoW) [133]\\n[13], [27], [34], [42]\\nPersonal Dialog\\nKBP [134]\\n[74], [135]\\nDuleMon [136]\\n[74]\\nTask-oriented Dialog\\nCamRest [137]\\n[78], [79]\\nRecommendation\\nAmazon(Toys,Sport,Beauty) [138]\\n[39], [40]\\nIE\\nEvent Argument Extraction\\nWikiEvent [139]\\n[13], [27], [37], [42]\\nRAMS [140]\\n[36], [37]\\nRelation Extraction\\nT-REx [141],ZsRE [142]\\n[27], [51]\\nReasoning\\nCommonsense Reasoning\\nHellaSwag [143]\\n[20], [66]\\nCoT Reasoning\\nCoT Reasoning [144]\\n[27]\\nComplex Reasoning\\nCSQA [145]\\n[55]\\nOthers\\nLanguage Understanding\\nMMLU [146]\\n[7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling\\nWikiText-103 [147]\\n[5], [29], [64], [71]\\nStrategyQA [148]\\n[14], [24], [48], [51], [55], [58]\\nFact Checking/Verification\\nFEVER [149]\\n[4], [13], [27], [34], [42], [50]\\nPubHealth [150]\\n[25], [67]\\nText Generation\\nBiography [151]\\n[67]\\nText Summarization\\nWikiASP [152]\\n[24]\\nXSum [153]\\n[17]\\nText Classification\\nVioLens [154]\\n[19]\\nTREC [155]\\n[33]\\nSentiment\\nSST-2 [156]\\n[20], [33], [38]\\nCode Search\\nCodeSearchNet [157]\\n[76]\\nRobustness Evaluation\\nNoMIRACL [56]\\n[56]\\nMath\\nGSM8K [158]\\n[73]\\nMachine Translation\\nJRC-Acquis [159]\\n[17]\\n14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance\\nFaithfulness\\nAnswer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nEM\\n✓\\nRecall\\n✓\\nPrecision\\n✓\\n✓\\nR-Rate\\n✓\\nCosine Similarity\\n✓\\nHit Rate\\n✓\\nMRR\\n✓\\nNDCG\\n✓\\nBLEU\\n✓\\n✓\\n✓\\nROUGE/ROUGE-L\\n✓\\n✓\\n✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these\\nmetrics, derived from related work, are traditional measures\\nand do not yet represent a mature or standardized approach for\\nquantifying RAG evaluation aspects. Custom metrics tailored\\nto the nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\nD. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD\\n[167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. DISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original\\nreferences for LLMs to help users verify the generated an-\\nswers. The entire retrieval and reasoning process is observable,\\nwhile generation solely relying on long context remains a\\nblack box. Conversely, the expansion of context provides new\\nopportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Determining the optimal integration of RAG and\\nfine-tuning whether sequential, alternating, or through end-to-\\nend joint training—and how to harness both parameterized\\n15\\nTABLE IV\\nSUMMARY OF EVALUATION FRAMEWORKS\\nEvaluation Framework\\nEvaluation Targets\\nEvaluation Aspects\\nQuantitative Metrics\\nRGB†\\nRetrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL†\\nGeneration Quality\\nCounterfactual Robustness\\nR-Rate (Reappearance Rate)\\nRAGAS‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\nCRUD†\\nRetrieval Quality\\nGeneration Quality\\nCreative Generation\\nKnowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific\\nfunctionalities into RAG and fine-tuned by the results of RAG\\nsystem. For example, CRAG [67] trains a lightweight retrieval\\nevaluator to assess the overall quality of the retrieved docu-\\nments for a query and triggers different knowledge retrieval\\nactions based on confidence levels.\\nD. Scaling laws of RAG\\nEnd-to-end RAG models and pre-trained models based\\non\\nRAG\\nare\\nstill\\none\\nof\\nthe\\nfocuses\\nof\\ncurrent\\nre-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for\\nLLMs, their applicability to RAG remains uncertain. Initial\\nstudies like RETRO++ [44] have begun to address this, yet the\\nparameter count in RAG models still lags behind that of LLMs.\\nThe possibility of an Inverse Scaling Law 10, where smaller\\nmodels outperform larger ones, is particularly intriguing and\\nmerits further investigation.\\nE. Production-Ready RAG\\nRAG’s practicality and alignment with engineering require-\\nments have facilitated its adoption. However, enhancing re-\\ntrieval efficiency, improving document recall in large knowl-\\nedge bases, and ensuring data security—such as preventing\\n10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra\\n12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/\\n16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video. The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by\\nincorporating external, offline strategies for voice-to-text con-\\nversion [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.\\nVid2Seq augments language models with specialized temporal\\nmarkers, facilitating the prediction of event boundaries and\\ntextual descriptions within a unified output sequence [181].\\nCode. RBPS [182] excels in small-scale learning tasks by\\nretrieving code examples that align with developers’ objectives\\nthrough encoding and frequency analysis. This approach has\\ndemonstrated efficacy in tasks such as test assertion genera-\\ntion and program repair. For structured knowledge, the CoK\\nmethod [106] first extracts facts pertinent to the input query\\nfrom a knowledge graph, then integrates these facts as hints\\nwithin the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.\\n17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep\\npace with its evolution. Ensuring accurate and representative\\nperformance assessments is crucial for fully capturing RAG’s\\ncontributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning.\\nPMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158, 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,”\\nin International conference on machine learning.\\nPMLR, 2022, pp.\\n2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nneural information processing systems, vol. 35, pp. 27 730–27 744,\\n2022.\\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[8] I.\\nILIN,\\n“Advanced\\nrag\\ntechniques:\\nan\\nil-\\nlustrated\\noverview,”\\nhttps://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\\n“Large language model based long-tail query rewriting in taobao\\nsearch,” arXiv preprint arXiv:2311.03758, 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117, 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496, 2022.\\n[12] V. Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\\nsityranker and lostinthemiddleranker,” https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\\n[13] W. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nand M. Jiang, “Generate rather than retrieve: Large language models\\nare strong context generators,” arXiv preprint arXiv:2209.10063, 2022.\\n[14] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294, 2023.\\n[15] X. Wang, Q. Yang, Y. Qiu, J. Liang, Q. He, Z. Gu, Y. Xiao,\\nand W. Wang, “Knowledgpt: Enhancing large language models with\\nretrieval and storage access on knowledge bases,” arXiv preprint\\narXiv:2308.11761, 2023.\\n[16] A.\\nH.\\nRaudaschl,\\n“Forget\\nrag,\\nthe\\nfuture\\nis\\nrag-fusion,”\\nhttps://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437, 2023.\\n[18] S. Wang, Y. Xu, Y. Fang, Y. Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple\\nand effective method by retrieving from training data,” arXiv preprint\\narXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint\\narXiv:2311.06595, 2023.\\n[20] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun,\\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\\nfor improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,\\n2023.\\n[21] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\\nK. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval\\nfrom 8 examples,” arXiv preprint arXiv:2209.11755, 2022.\\n[22] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, “Recitation-augmented\\nlanguage models,” arXiv preprint arXiv:2210.01296, 2022.\\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.\\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\narXiv preprint arXiv:2305.06983, 2023.\\n[25] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag:\\nLearning to retrieve, generate, and critique through self-reflection,”\\narXiv preprint arXiv:2310.11511, 2023.\\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\\n“Bridging the preference gap between retrievers and llms,” arXiv\\npreprint arXiv:2401.06954, 2024.\\n[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-\\naugmented dual instruction tuning,” arXiv preprint arXiv:2310.01352,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\\nretrieval? comparing knowledge injection in llms,” arXiv preprint\\narXiv:2312.05934, 2023.\\n[29] T. Lan, D. Cai, Y. Wang, H. Huang, and X.-L. Mao, “Copy is all\\nyou need,” in The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\\nuse?” arXiv preprint arXiv:2312.06648, 2023.\\n[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware\\nmulti-hop evidence retrieval,” arXiv preprint arXiv:2311.02616, 2023.\\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y. Li, and N. Cam-Tu,\\n“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503, 2023.\\n[33] Z. Guo, S. Cheng, Y. Wang, P. Li, and Y. Liu, “Prompt-guided re-\\ntrieval augmentation for non-knowledge-intensive tasks,” arXiv preprint\\narXiv:2305.17653, 2023.\\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\\nto filter context for retrieval-augmented generation,” arXiv preprint\\narXiv:2311.08377, 2023.\\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\\ndata augmentation for low-resource domain tasks,” arXiv preprint\\narXiv:2402.13482, 2024.\\n[36] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is not\\na good few-shot information extractor, but a good reranker for hard\\nsamples!” arXiv preprint arXiv:2303.08559, 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.\\n[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context\\nexamples for large language models,” arXiv preprint arXiv:2307.07164,\\n2023.\\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\\nL. Hong, Y. Tay, V. Q. Tran, J. Samost et al., “Recommender systems\\nwith generative retrieval,” arXiv preprint arXiv:2305.05065, 2023.\\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\\nY. Li, H. Lu et al., “Language models as semantic indexers,” arXiv\\npreprint arXiv:2310.07815, 2023.\\n[41] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\\nzaro, “Raven: In-context learning with retrieval augmented encoder-\\ndecoder language models,” arXiv preprint arXiv:2308.07922, 2023.\\n18\\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\\nO. Kuchaiev, B. Li, C. Xiao et al., “Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study,” arXiv preprint\\narXiv:2304.06762, 2023.\\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\\nzaro, “Instructretro: Instruction tuning post retrieval-augmented pre-\\ntraining,” arXiv preprint arXiv:2310.07713, 2023.\\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\\naugmented generation (rag) models for open domain question answer-\\ning,” Transactions of the Association for Computational Linguistics,\\nvol. 11, pp. 1–17, 2023.\\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\\ntrieval augmentation for long-form question answering,” arXiv preprint\\narXiv:2310.12150, 2023.\\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\\nEnhancing robustness in retrieval-augmented language models,” arXiv\\npreprint arXiv:2311.09210, 2023.\\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-\\nchain: Towards accurate, credible and traceable large language models\\nfor knowledgeintensive tasks,” CoRR, vol. abs/2304.14732, 2023.\\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682, 2023.\\n[53] J. L´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\\nscientific research,” arXiv preprint arXiv:2312.07559, 2023.\\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and\\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\\ning reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing, 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\\n“Nomiracl: Knowing when you don’t know for robust multilingual\\nretrieval-augmented generation,” arXiv preprint arXiv:2312.11361,\\n2023.\\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696, 2023.\\n[58] Y. Wang, P. Li, M. Sun, and Y. Liu, “Self-knowledge guided\\nretrieval augmentation for large language models,” arXiv preprint\\narXiv:2310.05002, 2023.\\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025,\\n2023.\\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509, 2022.\\n[62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\\nof large language models with retrieval augmentation,” arXiv preprint\\narXiv:2307.11019, 2023.\\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059, 2024.\\n[64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, and Y. Shoham, “In-context retrieval-augmented language\\nmodels,” arXiv preprint arXiv:2302.00083, 2023.\\n[65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\\nsample: Document-level event argument extraction via hybrid retrieval\\naugmentation,” in Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\n2023, pp. 293–306.\\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\\nzero-shot semi-parametric language models from multiple tasks,” arXiv\\npreprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884, 2024.\\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\\ngeneration and evidence retrieval,” arXiv preprint arXiv:2310.16568,\\n2023.\\n[69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:\\nFitting black-box large language models for retrieval question answer-\\ning via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\\nlanguage models are strong zero-shot query likelihood models for\\ndocument ranking,” arXiv preprint arXiv:2310.13243, 2023.\\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\\nlms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652, 2023.\\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\\ntionale memory for retrieval augmented generation,” arXiv preprint\\narXiv:2311.04177, 2023.\\n[74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi,\\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\\nretrieval-augmented generation for personalized dialogue systems,”\\narXiv preprint arXiv:2401.13256, 2024.\\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\\n“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757, 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, “Structure-\\naware language model pretraining improves dense retrieval on struc-\\ntured data,” arXiv preprint arXiv:2305.19912, 2023.\\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\\ngraph-augmented language models for knowledge-grounded dialogue\\ngeneration,” arXiv preprint arXiv:2305.18846, 2023.\\n[78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-\\ngeneration alignment for end-to-end task-oriented dialogue system,”\\narXiv preprint arXiv:2310.08877, 2023.\\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\\nknowledge retrieval for task-oriented dialogue systems,” arXiv preprint\\narXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.\\n[81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang,\\nH. Ding, X. Chu, J. Zhao et al., “Think and retrieval: A hypothesis\\nknowledge graph enhanced medical large language models,” arXiv\\npreprint arXiv:2312.15883, 2023.\\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\\nand interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun,\\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,” arXiv preprint\\narXiv:2402.07630, 2024.\\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\\nX. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language\\nand commands into one gpt,” arXiv preprint arXiv:2307.08674, 2023.\\n[86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, “Iseeq: Information\\nseeking question generation using dynamic meta-information retrieval\\nand knowledge graphs,” in Proceedings of the AAAI Conference on\\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.\\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch¨arli,\\nand D. Zhou, “Large language models can be easily distracted by\\nirrelevant context,” in International Conference on Machine Learning.\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R.\\nTeja,\\n“Evaluating\\nthe\\nideal\\nchunk\\nsize\\nfor\\na\\nrag\\nsystem\\nusing\\nllamaindex,”\\nhttps://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.\\n19\\n[89] Langchain, “Recursively split by character,” https://python.langchain.\\ncom/docs/modules/data connection/document transformers/recursive\\ntext splitter, 2023.\\n[90] S.\\nYang,\\n“Advanced\\nrag\\n01:\\nSmall-to-\\nbig\\nretrieval,”\\nhttps://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n[91] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730, 2023.\\n[92] D. Zhou, N. Sch¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495, 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint\\narXiv:2309.12871, 2023.\\n[95] VoyageAI, “Voyage’s embedding models,” https://docs.voyageai.com/\\nembeddings/, 2023.\\n[96] BAAI,\\n“Flagembedding,”\\nhttps://github.com/FlagOpen/\\nFlagEmbedding, 2023.\\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y. Nie, “Retrieve anything\\nto augment large language models,” arXiv preprint arXiv:2310.07554,\\n2023.\\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\\nand P. Liang, “Lost in the middle: How language models use long\\ncontexts,” arXiv preprint arXiv:2307.03172, 2023.\\n[99] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524, 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track\\nand Government Track), J. Campbell, S. Larocca, J. Marciano,\\nK. Savenkov, and A. Yanishevsky, Eds.\\nOrlando, USA: Association\\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context\\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839,\\n2023.\\n[102] V. Karpukhin, B. O˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question\\nanswering,” arXiv preprint arXiv:2004.04906, 2020.\\n[103] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is\\nnot a good few-shot information extractor, but a good reranker for\\nhard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:257532405\\n[104] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092, 2023.\\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y. K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-\\nels with structured knowledge bases,” arXiv preprint arXiv:2305.13269,\\n2023.\\n[107] H.\\nYang,\\nS.\\nYue,\\nand\\nY.\\nHe,\\n“Auto-gpt\\nfor\\nonline\\ndecision\\nmaking:\\nBenchmarks\\nand\\nadditional\\nopinions,”\\narXiv\\npreprint\\narXiv:2306.02224, 2023.\\n[108] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\\n2023.\\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\\nsoning ability via prompt augmented by chatgpt,” arXiv preprint\\narXiv:2304.11116, 2023.\\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics, vol. 7, pp. 453–466,\\n2019.\\n[112] Y. Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y. Zhou,\\n“Exploring the integration strategies of retriever and large language\\nmodels,” arXiv preprint arXiv:2308.12574, 2023.\\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551, 2017.\\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions\\nfor\\nmachine\\ncomprehension\\nof\\ntext,”\\narXiv\\npreprint\\narXiv:1606.05250, 2016.\\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.\\n[118] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdi-\\nnov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.\\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a\\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,”\\narXiv preprint arXiv:2011.01060, 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics, vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190,\\n2019.\\n[122] T. Koˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y. Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\\nquestions meet long-form answers,” arXiv preprint arXiv:2204.06092,\\n2022.\\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\\nAwadallah, A. Celikyilmaz, Y. Liu, X. Qiu et al., “Qmsum: A new\\nbenchmark for query-based multi-domain meeting summarization,”\\narXiv preprint arXiv:2104.05938, 2021.\\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011, 2021.\\n[127] T. M¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID), 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\\nJ. Li, X. Wan, B. Wang et al., “Cmb: A comprehensive medical\\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833, 2023.\\n[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\\npreprint arXiv:2304.12986, 2023.\\n[130] R. Y. Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V. Pad-\\nmakumar, J. Ma, J. Thompson, H. He et al., “Quality: Question an-\\nswering with long input texts, yes!” arXiv preprint arXiv:2112.08608,\\n2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457, 2018.\\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937, 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241, 2018.\\n[134] H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source\\n20\\nplanner for personalized knowledge-grounded dialogue,” arXiv preprint\\narXiv:2310.08840, 2023.\\n[135] ——, “Large language models as source planner for personal-\\nized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,\\n2023.\\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, and S. Wang,\\n“Long time no see! open-domain conversation with long-term persona\\nmemory,” arXiv preprint arXiv:2203.05797, 2022.\\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\\nSu, S. Ultes, D. Vandyke, and S. Young, “Conditional generation\\nand snapshot learning in neural dialogue systems,” arXiv preprint\\narXiv:1606.03352, 2016.\\n[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution\\nof fashion trends with one-class collaborative filtering,” in proceedings\\nof the 25th international conference on world wide web, 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\\nby conditional generation,” arXiv preprint arXiv:2104.05919, 2021.\\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-\\nsentence argument linking,” arXiv preprint arXiv:1911.03766, 2019.\\n[141] H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\\nand E. Simperl, “T-rex: A large scale alignment of natural language\\nwith knowledge base triples,” in Proceedings of the Eleventh Inter-\\nnational Conference on Language Resources and Evaluation (LREC\\n2018), 2018.\\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\\ntraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\\n2017.\\n[143] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.\\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\\n“The cot collection: Improving zero-shot and few-shot learning of\\nlanguage models via chain-of-thought fine-tuning,” arXiv preprint\\narXiv:2305.14045, 2023.\\n[145] A. Saha, V. Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\\n“Complex sequential question answering: Towards learning to converse\\nover linked question answer pairs with a knowledge graph,” in Proceed-\\nings of the AAAI conference on artificial intelligence, vol. 32, no. 1,\\n2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300, 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843, 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926, 2020.\\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.\\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\\nand G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based\\nsummarization,” Transactions of the Association for Computational\\nLinguistics, vol. 9, pp. 211–225, 2021.\\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary! topic-aware convolutional neural networks for ex-\\ntreme summarization,” arXiv preprint arXiv:1808.08745, 2018.\\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\\nS. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms\\nof communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.\\n[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:\\nThe 19th International Conference on Computational Linguistics, 2002.\\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference on\\nempirical methods in natural language processing, 2013, pp. 1631–\\n1642.\\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436, 2019.\\n[158] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\\nand D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus\\nwith 20+ languages,” arXiv preprint cs/0609058, 2006.\\n[160] Y. Hoshi, D. Miyashita, Y. Ng, K. Tatsuno, Y. Morioka, O. Torii,\\nand J. Deguchi, “Ralle: A framework for developing and eval-\\nuating retrieval-augmented large language models,” arXiv preprint\\narXiv:2308.10633, 2023.\\n[161] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”\\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476, 2023.\\n[166] C.\\nJarvis\\nand\\nJ.\\nAllard,\\n“A\\nsurvey\\nof\\ntechniques\\nfor\\nmaximizing\\nllm\\nperformance,”\\nhttps://community.openai.\\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.\\n[168] Y. Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\\nX. Sun, “Recall: A benchmark for llms robustness against external\\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147, 2023.\\n[169] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025,\\n2023.\\n[171] C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.\\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\\nGonzalez, “Raft: Adapting language model to domain specific rag,”\\narXiv preprint arXiv:2403.10131, 2024.\\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-\\nsymbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning.\\nPMLR, 2022, pp.\\n468–485.\\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\\nmodal language modeling,” arXiv preprint arXiv:2211.12561, 2022.\\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\\nimage pre-training with frozen image encoders and large language\\nmodels,” arXiv preprint arXiv:2301.12597, 2023.\\n[178] W. Zhu, A. Yan, Y. Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y.\\nWang, “Visualize before you write: Imagination-guided open-ended\\ntext generation,” arXiv preprint arXiv:2210.03765, 2022.\\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\\nspokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external\\noff-policy speech-to-text mappings in contextual end-to-end automated\\nspeech recognition,” arXiv preprint arXiv:2301.02736, 2023.\\n21\\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\\nJ. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual\\nlanguage model for dense video captioning,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2023, pp. 10 714–10 726.\\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering (ICSE), 2023, pp.\\n2450–2462.\\n']...\n",
      "Similarity: [29.253265380859375, 29.731069564819336]\n",
      "--------------------------------------------------\n",
      "\n",
      "Relevant Image Results:\n",
      "Image 1:\n",
      "Filename: 2402.06196.pdf_image_47.png\n",
      "Similarity: [164.10067749023438, 166.96603393554688]\n",
      "Image file not found: ../images\\2402.06196.pdf_image_47.png\n",
      "Image 1:\n",
      "Filename: 2305.07622v3.pdf_image_0.png\n",
      "Similarity: [164.10067749023438, 166.96603393554688]\n",
      "Image file not found: ../images\\2305.07622v3.pdf_image_0.png\n"
     ]
    }
   ],
   "source": [
    "# Function to display results\n",
    "def display_results(text_results, image_results):\n",
    "    print(\"Relevant Text Results:\")\n",
    "    for i, (doc, meta_list, distance) in enumerate(zip(text_results['documents'], text_results['metadatas'], text_results['distances'])):\n",
    "        for meta in meta_list:\n",
    "            print(f\"Document {i + 1}:\")\n",
    "            print(f\"Source: {meta.get('source', 'Unknown source')}\")\n",
    "            print(f\"Content: {doc[:200]}...\")  # Display first 200 characters\n",
    "            print(f\"Similarity: {distance}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\nRelevant Image Results:\")\n",
    "    for i, (meta_list, distance) in enumerate(zip(image_results['metadatas'], image_results['distances'])):\n",
    "        for meta in meta_list:\n",
    "            print(f\"Image {i + 1}:\")\n",
    "            print(f\"Filename: {meta.get('filename', 'Unknown filename')}\")\n",
    "            print(f\"Similarity: {distance}\")\n",
    "            \n",
    "            # Load and display the image using the correct folder\n",
    "            image_filename = meta.get('filename')\n",
    "            if image_filename:\n",
    "                # Construct the correct full path to the image\n",
    "                image_path = os.path.join(image_folder, os.path.basename(image_filename))\n",
    "                try:\n",
    "                    image = Image.open(image_path)\n",
    "                    plt.figure()\n",
    "                    plt.imshow(image)\n",
    "                    plt.axis('off')  # Turn off axis\n",
    "                    plt.show()\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Image file not found: {image_path}\")\n",
    "\n",
    "# Example usage\n",
    "query_text = \"AI-related images and text\"\n",
    "text_results, image_results = retrieve_from_chroma(query_text, k=2)\n",
    "display_results(text_results, image_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
